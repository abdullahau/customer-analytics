---
title: BG/NBD Model - Stan Implementation
author: Abdullah Mahmood
date: last-modified
format:
  html:
    theme: cosmo
    highlight-style: atom-one
    mainfont: Palatino
    fontcolor: black
    monobackgroundcolor: white
    monofont: Menlo, Lucida Console, Liberation Mono, DejaVu Sans Mono, Bitstream Vera Sans Mono, Courier New, monospace
    fontsize: 13pt
    linestretch: 1.4
    number-sections: true
    number-depth: 4
    toc: true
    toc-location: right
    toc-depth: 4
    code-fold: false
    code-copy: true
    cap-location: bottom
    format-links: false
    embed-resources: true
    anchor-sections: true
    html-math-method:
      method: mathjax
      url: https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js
editor: source
jupyter:
  jupytext:
    formats: ipynb,qmd
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.7
  kernelspec:
    display_name: main
    language: python
    name: main
---

In this notebook we show how to fit a BG/NBD model in Stan. We compare the results with the [`lifetimes`](https://github.com/CamDavidsonPilon/lifetimes) package. The model is presented in the paper: Fader, P. S., Hardie, B. G., & Lee, K. L. (2005). [“Counting your customers” the easy way: An alternative to the Pareto/NBD model. Marketing science, 24(2), 275-284.](http://www.brucehardie.com/papers/bgnbd_2004-04-20.pdf)

### Imports

#### Packages

```{python}
import polars as pl
import numpy as np
from scipy.optimize import minimize
from utils import CDNOW, Stan, StanQuap
import arviz as az
```

#### Data

```{python}
calib_p = 273 # 39 week calibration period

data = CDNOW(master=False, calib_p=calib_p)

rfm_data = data.rfm_summary().select("P1X", "t_x", "T").collect().to_numpy()
p1x, t_x, T = rfm_data[:, 0], rfm_data[:, 1], rfm_data[:, 2]
```

Recall from the paper the following definitions:

-   `p1x` represents the number of repeat purchases the customer has made. This means that it’s one less than the total number of purchases. This is actually slightly wrong. It’s the count of time periods the customer had a purchase in. So if using days as units, then it’s the count of days the customer had a purchase on.
-   `T` represents the age of the customer in whatever time units chosen (weekly, in the above dataset). This is equal to the duration between a customer’s first purchase and the end of the period under study.
-   `t_x` represents the age of the customer when they made their most recent purchases. This is equal to the duration between a customer’s first purchase and their latest purchase. (Thus if they have made only 1 purchase, the recency is 0.)

## Model Specification

The BG/NBD model is a probabilistic model that describes the buying behavior of a customer in the non-contractual setting. It is based on the following assumptions for each customer:

### Frequency Process

1.  While active, the time between transactions is distributed exponential with transaction rate, i.e.

$$
f(t_{j} \mid t_{j-1}; \lambda) = \lambda \exp(-\lambda (t_{j} - t_{j - 1})), \quad t_{j} \geq t_{j - 1} \geq 0
$$

2.  Heterogeneity in $\lambda$ follows a gamma distribution with pdf

$$
f(\lambda \mid r, \alpha) = \frac{\alpha^{r}\lambda^{r - 1}\exp(-\lambda \alpha)}{\Gamma(r)}, \quad \\lambda  > 0
$$

### Dropout Process

3.  After any transaction, a customer becomes inactive with probability $p$.

4.  Heterogeneity in $p$ follows a beta distribution with pdf

$$
f(p \mid a, b) = \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)} p^{a - 1}(1 - p)^{b - 1}, \quad 0 \leq p \leq 1
$$

5.  The transaction rate $\lambda$ and the dropout probability $p$ vary independently across customers.

Instead of estimating $\lambda$ and $p$ for each specific customer, we do it for a randomly chosen customer, i.e. we work with the expected values of the parameters. Hence, we are interesting in finding the posterior distribution of the parameters $r$, $\alpha$, $a$, and $b$.

## Analytical MLE

### Standard SciPy Implementation

```{python}
import numpy as np
from scipy.special import gammaln, hyp2f1, gamma, factorial

def bgnbd_ll(x, p1x, t_x, T):
    r, alpha, a, b = x

    # Logarithm calculations with numerical stability
    log_alpha = np.log(np.clip(alpha, 1e-10, None))  # Avoid log(0) by clipping to a small value
    log_alpha_t_x = np.log(np.clip(alpha + t_x, 1e-10, None))

    # Components of the log-likelihood
    D_1 = (
        gammaln(r + p1x)
        - gammaln(r)
        + gammaln(a + b)
        + gammaln(b + p1x)
        - gammaln(b)
        - gammaln(a + b + p1x)
    )
    D_2 = r * log_alpha - (r + p1x) * log_alpha_t_x
    C_3 = ((alpha + t_x) / (alpha + T)) ** (r + p1x)
    C_4 = a / (b + p1x - 1)

    # Handle cases where p1x > 0 and apply log to valid values
    log_term = np.log(np.clip(C_3 + C_4, 1e-10, None))
    result = D_1 + D_2 + np.where(p1x > 0, log_term, np.log(np.clip(C_3, 1e-10, None)))

    return -np.sum(result)

def bgnbd_est():
    guess={'r': 0.01, 'alpha': 0.01, 'a': 0.01, 'b': 0.01}
    # Bounds for the optimization
    bnds = [(1e-6, np.inf) for _ in range(4)]

    # Optimization using minimize
    return minimize(
        bgnbd_ll,
        x0=list(guess.values()),
        method="BFGS",
        args=(p1x, t_x, T)
    )

result = bgnbd_est()
r, alpha, a, b = result.x
ll = result.fun

print(
f"""r = {r:0.4f}
α = {alpha:0.4f}
a = {a:0.4f}
b = {b:0.4f}
Log-Likelihood = {-ll:0.4f}"""
)

index = index=["r", "α", "a", "b"]
params = result.x
var_mat = result.hess_inv
se = np.sqrt(np.diag(var_mat))
plo = params - 1.96 * se
phi = params + 1.96 * se
pl.DataFrame(
    {
        "Parameter": index,
        "Coef": params,
        "SE (Coef)": se,
        "5.5%": plo,
        "94.5%": phi,
    }
)
```

### **Cameron Davidson-Pilon's `lifetimes`** Implementation

Source: [BetaGeoFitter](https://github.com/CamDavidsonPilon/lifetimes/blob/master/lifetimes/fitters/beta_geo_fitter.py), [fit function](https://github.com/CamDavidsonPilon/lifetimes/blob/master/lifetimes/fitters/__init__.py)

```{python}
import autograd.numpy as np
from autograd.scipy.special import gammaln, beta, gamma
from autograd import value_and_grad, hessian

def negative_log_likelihood(log_params, freq, rec, T):
    params = np.exp(log_params)
    r, alpha, a, b = params

    A_1 = gammaln(r + freq) - gammaln(r) + r * np.log(alpha)
    A_2 = gammaln(a + b) + gammaln(b + freq) - gammaln(b) - gammaln(a + b + freq)
    A_3 = -(r + freq) * np.log(alpha + T)
    A_4 = np.log(a) - np.log(b + np.maximum(freq, 1) - 1) - (r + freq) * np.log(rec + alpha)

    max_A_3_A_4 = np.maximum(A_3, A_4)

    ll = (A_1 + A_2 + np.log(np.exp(A_3 - max_A_3_A_4) + np.exp(A_4 - max_A_3_A_4) * (freq > 0)) + max_A_3_A_4)

    return -ll.sum()

def BetaGeoFitter(guess={'r': 0.1, 'alpha': 0.1, 'a': 0.0, 'b': 0.1}):
    
    # Bounds for the optimization
    # bnds = [(1e-6, np.inf) for _ in range(4)]

    # Optimization using minimize
    return minimize(
        value_and_grad(negative_log_likelihood),
        jac=True,
        method=None,
        args=(p1x, t_x, T),
        tol=1e-7, 
        x0=list(guess.values()),
        options={'disp': True}
    )

result = BetaGeoFitter()
r, alpha, a, b = np.exp(result.x)
ll = result.fun

print(
f"""r = {r:0.4f}
α = {alpha:0.4f}
a = {a:0.4f}
b = {b:0.4f}
Log-Likelihood = {-ll:0.4f}"""
)

index = index=["r", "α", "a", "b"]
params = np.exp(result.x)
hessian_mat = hessian(negative_log_likelihood)(result.x, p1x, t_x, T)
var_mat = (params ** 2) * np.linalg.inv(hessian_mat) # Variance-Covariance Matrix
se = np.sqrt(np.diag(var_mat))  # Standard Error
plo = params - 1.96 * se
phi = params + 1.96 * se
pl.DataFrame(
    {
        "Parameter": index,
        "Coef": params,
        "SE (Coef)": se,
        "5.5%": plo,
        "94.5%": phi,
    }
)
```

## Stan Model

### Parameters as per Peter Fader & Bruce Hardie

```{python}
stan_code = '''
data {
    int<lower=0> N;               // Number of customers
    array[N] int<lower=0> X;      // Number of transactions per customer
    vector<lower=0>[N] T;         // Total observation time per customer
    vector<lower=0>[N] Tx;        // Time of last transaction (0 if X=0)
}

parameters {
    real<lower=0> r;                   // gamma shape (r)
    real<lower=0> alpha;               // gamma scale (alpha)
    real<lower=0, upper=5> a;          // beta shape 1 (a)
    real<lower=0, upper=5> b;          // beta shape 2 (b)
}

model {
    // Weakly informative priors on log parameters
    r ~ weibull(2, 1);
    alpha ~ weibull(2, 10);
    a ~ uniform(0, 5);
    b ~ uniform(0, 5);

    for (n in 1:N) {
        int x = X[n];
        real tx = Tx[n];
        real t = T[n];
    
        if (x == 0) {
              // Likelihood for X=0: (alpha/(alpha + t))^r
              target += r * (log(alpha) - log(alpha + t));
        } else {
              // Term 1: B(a, b + x)/B(a, b) * Γ(r + x)/Γ(r) * (alpha/(alpha + t))^(r + x)
              real beta_term1 = lbeta(a, b + x) - lbeta(a, b);
              real gamma_term = lgamma(r + x) - lgamma(r);
              real term1 = gamma_term + beta_term1 + r * log(alpha) - (r + x) * log(alpha + t);
            
              // Term 2: B(a + 1, b + x - 1)/B(a, b) * Γ(r + x)/Γ(r) * (alpha/(alpha + tx))^(r + x)
              real beta_term2 = lbeta(a + 1, b + x - 1) - lbeta(a, b);
              real term2 = gamma_term + beta_term2 + r * log(alpha) - (r + x) * log(alpha + tx);
            
              // Log-sum-exp for numerical stability
              target += log_sum_exp(term1, term2);
        }
    }
}
'''

data = {
    'N': len(p1x),
    'X': p1x.flatten().astype(int).tolist(),
    'T': T.flatten().tolist(),
    'Tx': t_x.flatten().tolist()
}

stan_model = StanQuap(stan_file='stan_models/bg-nbd', stan_code=stan_code, data=data, algorithm='LBFGS', jacobian=False, tol_rel_grad=1e-7, iter=5000)

index = index=["r", "α", "a", "b"]
params = np.array(list(stan_model.opt_model.stan_variables().values()))
var_mat = var_mar = stan_model.vcov_matrix() # Variance-Covariance Matrix
se = np.sqrt(np.diag(var_mat))  # Standard Error
plo = params - 1.96 * se
phi = params + 1.96 * se
pl.DataFrame(
    {
        "Parameter": index,
        "Coef": params,
        "SE (Coef)": se,
        "5.5%": plo,
        "94.5%": phi,
    }
)
```

```{python}
x = [stan_model.opt_model.optimized_params_pd['r'][0],
     stan_model.opt_model.optimized_params_pd['alpha'][0],
     stan_model.opt_model.optimized_params_pd['a'][0],
     stan_model.opt_model.optimized_params_pd['b'][0]]

print("Log-Likelihood:")
print(bgnbd_ll(x, p1x, t_x, T))
# print(negative_log_likelihood(np.log(np.array(x)), p1x, t_x, T))
```

### Modified Parameters

```{python}
stan_code = '''
data {
    int<lower=0> N;               // Number of customers
    array[N] int<lower=0> X;      // Number of transactions per customer
    vector<lower=0>[N] T;         // Total observation time per customer
    vector<lower=0>[N] Tx;        // Time of last transaction (0 if X=0)
}

parameters {
    real<lower=0> r;                         // Shape parameter for the Gamma prior on purchase rate
    real<lower=0> alpha;                     // Scale parameter for purchase rate
    real<lower=0, upper=1> phi_dropout;      // Mixture weight for dropout process (Uniform prior)
    real<lower=1> kappa_dropout;             // Scale parameter for dropout (Pareto prior)
}

transformed parameters {
    real a = phi_dropout * kappa_dropout;       // Dropout shape parameter (controls early dropout likelihood)
    real b = (1 - phi_dropout) * kappa_dropout; // Dropout scale parameter (controls later dropout likelihood)
}

model {
    // Priors:
    r ~ weibull(2, 1);                // Prior on r (purchase rate shape parameter)
    alpha ~ weibull(2, 10);           // Prior on alpha (purchase rate scale parameter)
    phi_dropout ~ uniform(0,1);       // Mixture component for dropout process
    kappa_dropout ~ pareto(1,1);      // Scale of dropout process

    for (n in 1:N) {
        int x = X[n];                 // Number of transactions for customer n
        real tx = Tx[n];              // Time of last transaction
        real t = T[n];                // Total observation time

        if (x == 0) {
            // Likelihood for customers with zero transactions:
            // Probability of no purchases during (0, T): (alpha/(alpha + t))^r
            // Likelihood for X=0: (alpha/(alpha + t))^r
            target += r * (log(alpha) - log(alpha + t));
        } else {
            // Term 1: Probability of surviving until T and making x purchases
            // Term 1: B(a, b + x)/B(a, b) * Γ(r + x)/Γ(r) * (alpha/(alpha + t))^(r + x)
            real beta_term1 = lbeta(a, b + x) - lbeta(a, b);  // Beta function term
            real gamma_term = lgamma(r + x) - lgamma(r);       // Gamma function term
            real term1 = gamma_term + beta_term1 + r * log(alpha) - (r + x) * log(alpha + t);
            
            // Term 2: Probability of surviving until Tx, then dropping out
            // Term 2: B(a + 1, b + x - 1)/B(a, b) * Γ(r + x)/Γ(r) * (alpha/(alpha + tx))^(r + x)
            real beta_term2 = lbeta(a + 1, b + x - 1) - lbeta(a, b);
            real term2 = gamma_term + beta_term2 + r * log(alpha) - (r + x) * log(alpha + tx);
            
            // Log-sum-exp for numerical stability
            target += log_sum_exp(term1, term2);
        }
    }
}
'''

data = {
    'N': len(p1x),
    'X': p1x.flatten().astype(int).tolist(),
    'T': T.flatten().tolist(),
    'Tx': t_x.flatten().tolist()
}

stan_model = StanQuap(stan_file='stan_models/bg-nbd-1', stan_code=stan_code, data=data, algorithm='LBFGS', jacobian=False, tol_rel_grad=1e-7, iter=5000, generated_var=['a', 'b'])

index = index=["r", "α", 'phi', 'kappa', "a", "b"]
params = np.array(list(stan_model.opt_model.stan_variables().values()))
var_mat = var_mar = stan_model.vcov_matrix() # Variance-Covariance Matrix
se = np.sqrt(np.diag(var_mat))  # Standard Error
se =  np.concatenate((se, np.array([se[-2] * se[-1] , (1 - se[-2]) * se[-1]])))
plo = params - 1.96 * se
phi = params + 1.96 * se
pl.DataFrame(
    {
        "Parameter": index,
        "Coef": params,
        "SE (Coef)": se,
        "5.5%": plo,
        "94.5%": phi,
    }
)
```

```{python}
x = [stan_model.opt_model.optimized_params_pd['r'][0],
     stan_model.opt_model.optimized_params_pd['alpha'][0],
     stan_model.opt_model.optimized_params_pd['a'][0],
     stan_model.opt_model.optimized_params_pd['b'][0]]

print("Log-Likelihood:")
print(bgnbd_ll(x, p1x, t_x, T))
# print(negative_log_likelihood(np.log(np.array(x)), p1x, t_x, T))
```

### MCMC Model Fitting

```{python}
mcmc = stan_model.stan_model.sample(data=data)
mcmc.summary()
```

```{python}
mcmc.diagnose()
```

```{python}
inf_data = az.from_cmdstanpy(mcmc)
az.plot_trace(inf_data);
```

```{python}
az.plot_posterior(inf_data, var_names=['r', 'alpha', 'a', 'b'], color="C0");
```