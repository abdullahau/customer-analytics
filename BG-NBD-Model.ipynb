{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from scipy.special import gammaln, hyp2f1\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from IPython.display import display_markdown\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline\n",
    "\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:\n",
    "- [\"Counting Your Customers\" the Easy Way: An Alternative to the Pareto/NBD Model](https://www.brucehardie.com/abstracts/abstract-fhl_2004-04.html)\n",
    "- [Implementing the BG/NBD Model for Customer Base Analysis in Excel](https://www.brucehardie.com/notes/004/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CDNOW = (\n",
    "    pl.scan_csv(source='data/CDNOW/CDNOW_sample.csv',\n",
    "                has_header=False,\n",
    "                separator=',',\n",
    "                schema={'CustID': pl.Int32,\n",
    "                        'ID': pl.Int32,\n",
    "                        'Date': pl.String,\n",
    "                        'Quant': pl.Int16,\n",
    "                        'Spend': pl.Float64})\n",
    "    .with_columns(pl.col('Date').str.to_date(\"%Y%m%d\"))\n",
    "    .with_columns((pl.col('Date') - pl.date(1996,12,31)).dt.total_days().cast(pl.UInt16).alias('PurchDay'))\n",
    "    .with_columns((pl.col('Spend')*100).round(0).cast(pl.Int64).alias('Spend Scaled'))\n",
    "    .group_by('ID', 'Date')\n",
    "    .agg(pl.col('*').exclude('PurchDay').sum(), pl.col('PurchDay').max())\n",
    "    .sort('ID', 'Date')\n",
    "    .with_columns((pl.col(\"ID\").cum_count().over(\"ID\") - 1).cast(pl.UInt16).alias(\"DoR\"))      \n",
    "    .drop('CustID')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "calwk = 273 # 39 week calibration period\n",
    "\n",
    "# The number of repeat transactions made by each customer in each period\n",
    "freq_x = (\n",
    "    CDNOW\n",
    "    .group_by('ID', maintain_order=True)\n",
    "    .agg(\n",
    "        pl.col('PurchDay')\n",
    "        .filter((pl.col('PurchDay') <= calwk) & (pl.col('DoR') > 0))\n",
    "        .count()\n",
    "        .alias('P1X'), # Period 1: Calibration Period\n",
    "\n",
    "        pl.col('PurchDay')\n",
    "        .filter((pl.col('PurchDay') > calwk) & (pl.col('DoR') > 0))\n",
    "        .count()\n",
    "        .alias('P2X')  # Period 2: Longitudinal Holdout Period      \n",
    "    )\n",
    ")\n",
    "\n",
    "# The number of CDs purchased and total spend across these repeat transactions\n",
    "pSpendQuant = (\n",
    "    CDNOW\n",
    "    .join(freq_x, on='ID', how='left')\n",
    "    .group_by('ID', maintain_order=True)\n",
    "    .agg(\n",
    "        \n",
    "        pl.col('Spend Scaled')\n",
    "        .filter((pl.col('DoR') > 0) & (pl.col('DoR') <= pl.col('P1X')) & (pl.col('P1X') != 0))\n",
    "        .sum()\n",
    "        .alias('P1X Spend'),\n",
    "        \n",
    "        pl.col('Quant')\n",
    "        .filter((pl.col('DoR') > 0) & (pl.col('DoR') <= pl.col('P1X')) & (pl.col('P1X') != 0))\n",
    "        .sum()\n",
    "        .alias('P1X Quant'),        \n",
    "        \n",
    "        pl.col('Spend Scaled')\n",
    "        .filter((pl.col('DoR') > 0) & (pl.col('DoR') > pl.col('P1X')))\n",
    "        .sum()\n",
    "        .alias('P2X Spend'),\n",
    "        \n",
    "        pl.col('Quant')\n",
    "        .filter((pl.col('DoR') > 0) & (pl.col('DoR') > pl.col('P1X')))\n",
    "        .sum()\n",
    "        .alias('P2X Quant')                \n",
    "    )\n",
    ")\n",
    "\n",
    "# The average spend per repeat transaction\n",
    "m_x = (\n",
    "    pSpendQuant\n",
    "    .join(freq_x, on='ID', how='left')\n",
    "    .with_columns(\n",
    "        (pl.col('P1X Spend') / pl.col('P1X')).alias('m_x_calib'),\n",
    "        (pl.col('P2X Spend') / pl.col('P2X')).alias('m_x_valid')\n",
    "    ).fill_nan(0)\n",
    ")\n",
    "\n",
    "# time of last calibration period repeat purchase (in weeks) - Recency\n",
    "ttlrp = (\n",
    "    CDNOW\n",
    "    .join(freq_x, on='ID', how='left')\n",
    "    .with_columns(\n",
    "        pl.col('PurchDay').filter(pl.col('DoR') == 0)\n",
    "        .first()\n",
    "        .over('ID')\n",
    "        .alias('Trial Day')\n",
    "    )\n",
    "    .group_by('ID', maintain_order=True)\n",
    "    .agg(\n",
    "        pl.col('PurchDay', 'Trial Day')\n",
    "        .filter(pl.col('DoR') <= pl.col('P1X'))\n",
    "        .max()\n",
    "        # .alias('LastPurch')\n",
    "    )\n",
    "    .with_columns(\n",
    "        # effective calibration period (in weeks)\n",
    "        ((pl.col('PurchDay') - pl.col('Trial Day')) / 7).alias('t_x'), # Time to Last Repeat Purchase - Recency\n",
    "        ((calwk - pl.col('Trial Day'))/7).alias('T')\n",
    "    )\n",
    "    .drop('PurchDay', 'Trial Day')\n",
    ")\n",
    "\n",
    "# Time of trial purchase (in weeks)\n",
    "tofp = (\n",
    "    CDNOW\n",
    "    .filter(pl.col('DoR') == 0)\n",
    "    .with_columns((pl.col('PurchDay') / 7).alias('Time of First Purch'))\n",
    "    .group_by('Time of First Purch').agg(pl.len().alias('Count'))\n",
    "    .sort('Time of First Purch')\n",
    ")\n",
    "\n",
    "rfm_data = (\n",
    "    m_x\n",
    "    .join(other=ttlrp, on=\"ID\", how=\"left\")\n",
    "    .rename({'P1X': 'x'})\n",
    "    .select('ID', 'x', 't_x', 'T')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0s/z9xp988n3j78zfjwg3y616x00000gn/T/ipykernel_96669/3747063264.py:10: RuntimeWarning: invalid value encountered in log\n",
      "  np.log(a) - np.log(b + rfm_data[:,0] - 1) - (r + rfm_data[:,0]) * np.log(alpha + rfm_data[:,1]),\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "$r$ = 0.2426\n",
       "\n",
       "$\\alpha$ = 4.4136\n",
       "\n",
       "$a$ = 0.7929\n",
       "\n",
       "$b$ = 2.4258\n",
       "\n",
       "Log-Likelihood = -9582.4292"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def bgnbd_est(rfm_data, guess={'r': 0.01, 'alpha': 0.01, 'a': 0.01, 'b':0.01}):\n",
    "    \n",
    "    def log_likelihood(x):\n",
    "        r, alpha, a, b = x\n",
    "\n",
    "        ln_A_1 = gammaln(rfm_data[:,0] + r) - gammaln(r) + r * np.log(alpha)\n",
    "        ln_A_2 = gammaln(a + b) + gammaln(b + rfm_data[:,0]) - gammaln(b) - gammaln(a + b + rfm_data[:,0])\n",
    "        ln_A_3 = -(r + rfm_data[:,0]) * np.log(alpha + rfm_data[:,2])\n",
    "        ln_A_4 = np.where(rfm_data[:,0] > 0, \n",
    "                          np.log(a) - np.log(b + rfm_data[:,0] - 1) - (r + rfm_data[:,0]) * np.log(alpha + rfm_data[:,1]),\n",
    "                          0)\n",
    "        return -np.sum(ln_A_1 + ln_A_2 + np.log(np.exp(ln_A_3) + (rfm_data[:,0] > 0) * np.exp(ln_A_4)))\n",
    "    \n",
    "    bnds = [(0, np.inf) for _ in range(4)]\n",
    "    return minimize(log_likelihood, x0=list(guess.values()), bounds=bnds)\n",
    "\n",
    "result = bgnbd_est(rfm_data.select('x', 't_x', 'T').collect().to_numpy())\n",
    "r, alpha, a, b = result.x\n",
    "ll = result.fun\n",
    "\n",
    "display_markdown(f'''$r$ = {r:0.4f}\n",
    "\n",
    "$\\\\alpha$ = {alpha:0.4f}\n",
    "\n",
    "$a$ = {a:0.4f}\n",
    "\n",
    "$b$ = {b:0.4f}\n",
    "\n",
    "Log-Likelihood = {-ll:0.4f}''', raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, alpha, a, b = 0.242594150135163, 4.41359212062015, 0.792919156875463, 2.4258950494563"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.45882751, 11.60378582, 20.08495471, 29.14030054, 38.06999252,\n",
       "       48.2700452 , 56.58029375, 64.89178284, 73.11856376, 81.7028405 ,\n",
       "       89.13746435, 95.70266923, 97.04402442, 93.55891092, 90.00293954,\n",
       "       86.94761736, 84.0891682 , 81.53582973, 79.39818795, 77.2646425 ,\n",
       "       75.16239719, 73.30137137, 71.56133426, 69.92932431, 68.26604383,\n",
       "       66.82564548, 65.57001014, 64.40697566, 63.18653243, 61.88322392,\n",
       "       60.76934085, 59.70675802, 58.63398038, 57.66364098, 56.72135071,\n",
       "       55.82049889, 55.09954206, 54.27533588, 53.49443501, 52.73960615,\n",
       "       51.92726728, 51.21761918, 50.5323692 , 49.87015708, 49.22972731,\n",
       "       48.60991888, 48.00965626, 47.42794139, 46.86384657, 46.31650818,\n",
       "       45.78512102, 45.26893331, 44.76724221, 44.27938968, 43.80475896,\n",
       "       43.3427712 , 42.89288251, 42.4545813 , 42.02738584, 41.610842  ,\n",
       "       41.2045213 , 40.808019  , 40.4209525 , 40.04295974, 39.62375933,\n",
       "       39.25877987, 38.91100698, 38.55056258, 38.21138897, 37.85224683,\n",
       "       37.57071871, 37.25474674, 37.00205786, 36.74493979, 36.45134222,\n",
       "       36.15519034, 35.88382094, 35.60522178])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_horizon = (calwk * 2) // 7\n",
    "\n",
    "t = np.arange(1/7, forecast_horizon, 1/7)\n",
    "z = t / (alpha + t)\n",
    "h2f1 = hyp2f1(r, b, (a + b - 1), z)\n",
    "E_X_t = (a + b - 1) / (a - 1) * (1 - (alpha / (alpha + t))**r * h2f1)\n",
    "\n",
    "tofp_array = tofp.collect().to_numpy()\n",
    "\n",
    "num_triers = tofp_array[:, 1]\n",
    "trial_week = tofp_array[:, 0]\n",
    "time_trial_week = np.arange(1/7, np.max(trial_week), 1/7)\n",
    "\n",
    "index = ((t.reshape(-1, 1) - time_trial_week) * 7).astype(np.int16)\n",
    "index = np.clip(index - 1, 0, E_X_t.shape[0] - 1)\n",
    "\n",
    "# Compute cumulative repeat sales\n",
    "cum_rpt_sls = np.sum(np.tril(E_X_t[index]) * num_triers, axis=1)\n",
    "\n",
    "# Compute weekly repeat sales\n",
    "wkly_rpt_sls = np.diff(cum_rpt_sls[6::7], prepend=0)\n",
    "\n",
    "wkly_rpt_sls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing Conditional Expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "$E(Y(t) \\mid X = x, t_{x}, T, r, \\alpha, a, b)$ = 1.2259"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = 2\n",
    "t_x = 30.43\n",
    "T = 38.86\n",
    "t = 39 # the length of the period over which we wish to make the conditional forecast\n",
    "\n",
    "a_cust = r+x\n",
    "b_cust = b+x\n",
    "c_cust = a+b+x-1\n",
    "z_cust = t/(alpha + T + t)\n",
    "\n",
    "h2f1_cust = hyp2f1(a_cust, b_cust, c_cust, z_cust)\n",
    "\n",
    "E_Y_X = (a + b + x - 1) / (a - 1) * (1 - ((alpha + T) / (alpha + T + t))**(r + x) * h2f1_cust) / \\\n",
    "        (1 + (x > 0) * a / (b + x - 1) * ((alpha + T) / (alpha + t_x))**(r + x))\n",
    "\n",
    "display_markdown(f'''$E(Y(t) \\\\mid X = x, t_{{x}}, T, r, \\\\alpha, a, b)$ = {E_Y_X:0.4f}''' ,raw=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
