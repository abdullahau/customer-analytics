{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#956bbf\">Analysing Buyer Behaviour Using Consumer Panel Data</span>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#956bbf\">Introduction</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a python implementation of Bruce G.S. Hardie's [An Excel-based Introduction to Analysing Buyer Behaviour Using Consumer Panel Data](https://www.brucehardie.com/notes/042/) using [Polars](https://pola.rs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several leading market research firms (e.g., Kantar, IRI, Nielsen) collect so called consumer panel data and sell reports and analyses based on these data to interested parties. A large number of households are recruited to join the panel and they record all their grocery/HBA purchases. The idea of a panel is that we have repeated observations of the same people. The ability to track what individual households are purchasing over time can give important insights into what behavioural changes lie beneath observed changes in aggregate sales data. (terms panellist and household are used interchangeably) Nowadays, it is common for panellists to record their purchasing by scanning product barcodes via an app on their smartphones.\n",
    "\n",
    "The objective of this notebook is to provide an introduction to basic analyses we can undertake using panel data. After briefing **describing the data** we will be working with, we present some preliminary **aggregate-level analyses**. Next we introduce some basic **brand performance measures** and consider the simple analyses that describe the **variation we observe in buyer behaviour** in a given time period. This analysis focuses on one brand at a time; next we consider some basic analyses that describe consumers’ **buying of multiple brands in a category**. We finish with some basic analyses that describe **how buyer behaviour evolves over time**, both for established products and new products.\n",
    "\n",
    "Note: Whereas consumer panel data gives us information on the purchasing of a sample of customers for the whole category, the data in a firm’s transaction database gives us complete information on the purchasing of our products (but not those of our competitors). At a fundamental level, the types of reports developed by market research firms over the past 60+ years are a good starting point for the types of reports a firm should create as it seeks to understand the buying behaviour of its customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A traditional consumer panel works in the following manner.\n",
    "1) When an individual first joins the panel, they complete a detailed questionnaire. A section of this questionnaire focuses on the demographics of their household. (This is typically updated once a year.)\n",
    "2) After each shopping trip, each panellist records their purchases, scanning the barcode associated with each product and recording other information such as where the purchase was made (store or channel), the price paid, and the use of promotional deals. Twenty years ago, this would have been done using a custom handheld barcode scanner provided by the market research firm. These days, it is more common to use a smartphone app.\n",
    "3) These data are uploaded to the research firm’s servers and merged with the purchases records of the other panellists. Each barcode is matched with detailed product information (e.g., category, brand, size, flavour) and this information is also stored in the database. The analyst can then create (typically product-category-specific) datasets for further analysis that tell us what each panellist purchased, when and where, and associated transaction- and/or product-specific data that may be of interest.\n",
    "4) Panellists drop out of the panel all the time, and the research firm will be recruiting replacement households on a regular basis. When creating a dataset for further analysis, it is generally desirable to work with a so-called static sample of panellists, which comprises all those panellists active using the time period of interest; new panellists, as well as those that dropped out during the given time period, are excluded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0Nrn2ZX9UOK"
   },
   "source": [
    "### <span style=\"color:#956bbf\">Imports</span>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gxxt_AWN9igL"
   },
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bsw2hr5T8cu3"
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "from great_tables import GT, style, loc, md\n",
    "import gc\n",
    "\n",
    "# https://altair-viz.github.io/user_guide/display_frontends.html#displaying-in-jupyterlab\n",
    "alt.renderers.enable('mimetype')\n",
    "# alt.JupyterChart.enable_offline()\n",
    "# alt.renderers.enable(\"jupyter\", offline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7dq85pW9WSz"
   },
   "source": [
    "#### Import Panel Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make use of two datasets as we explore the basic types of summaries of buyer behaviour that can be created using consumer panel data. The first contains data on the purchasing of established brands in a mature product category, while the second contains data on the purchasing of a new product. Both datasets were created using static samples. While these are small datasets and contain a subset of the information available in the research firm’s databases, they are more than sufficient to convey the logic of creating the key summaries of buyer behaviour. Neither dataset includes data on the demographics of each panellist. As such, we will not consider how to create reports that explore how behaviours differ across demographic groups (e.g., by age or geography). However, anyone comfortable with the analyses undertaken in this note should be able to work out how to create such reports for themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset 1**\n",
    "\n",
    "The file `edible_grocery.csv` contains two years of data from a panel of 5021 households on their purchasing in an unnamed edible grocery product category. (We intentionally do not identity the category and the associated brand names.) There are 119 SKUs in this category. 91 SKUs are associated with the four largest brands in the category, which we have named Alpha, Bravo, Charlie, and Delta. The remaining SKUs belong to very low-share brands and we grouped them under the brand Other.\n",
    "\n",
    "*Note*: SKU - A stock-keeping unit (SKU) is a unique combination of the attributes (e.g., brand, package type, package size, flavour) that define the products in the category.\n",
    "\n",
    "Each record in this file consists of seven fields:\n",
    "- `panel_id` A unique identifier for each household.\n",
    "- `trans_id` A unique identifier for the purchase occasion.\n",
    "- `week` The week number in which the purchase occurred. Week 1 corresponds to the calendar week starting on January 1, 20xy. Week 53 corresponds to the calendar week starting on December 31, 20xy.\n",
    "- `sku_id` The SKU code.\n",
    "- `units` The number of units purchased on the particular purchase occasion.\n",
    "- `price` The price per unit paid at the point of purchase.\n",
    "- `brand` The brand associated with the SKU purchased.\n",
    "\n",
    "The associated file `sku_weight.csv` gives us the weight (in grams) of each SKU. There are two fields: `sku_id` and `weight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byUO8v599TuY"
   },
   "outputs": [],
   "source": [
    "grocery_lf = pl.scan_csv(source=\"data/panel-datasets/edible_grocery.csv\",\n",
    "                         has_header=True,\n",
    "                         separator=\",\",\n",
    "                         schema={'panel_id': pl.UInt32,\n",
    "                                 'trans_id': pl.Int32,\n",
    "                                 'week': pl.UInt16,\n",
    "                                 'sku_id': pl.UInt8,\n",
    "                                 'units': pl.Int16,\n",
    "                                 'price': pl.Float32,\n",
    "                                 'brand': pl.Categorical})\n",
    "grocery_lf.head().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECLFaaIe95pK"
   },
   "outputs": [],
   "source": [
    "sku_lf = pl.scan_csv(source=\"data/panel-datasets/sku_weight.csv\",\n",
    "                         has_header=True,\n",
    "                         separator=\",\",\n",
    "                         schema={'sku_id': pl.UInt8,\n",
    "                                 'weight': pl.Int16})\n",
    "sku_lf.head().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_size = 5021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset 2**\n",
    "\n",
    "“Kiwi Bubbles” is a masked name for a shelf-stable juice drink, aimed primarily at children, which is sold as a multipack with several single-serve containers bundled together. Prior to national launch, it underwent a year-long test conducted in two of IRI’s BehaviorScan markets. The file `kiwibubbles_trans.csv` contains purchasing data for the new product, drawn from 1300 panellists in Market 1 and 1499 panellists in Market 2. (The purchasing of other brands in the category has been excluded from the dataset.)\n",
    "\n",
    "Each record in this file consists of five fields:\n",
    "- `ID` A unique identifier for each household.\n",
    "- `Market` 1 or 2.\n",
    "- `Week` The week in which the purchase occurred.\n",
    "- `Day` The day of the week in which the purchase occurred. (The product was launched on day 1 of week 1.)\n",
    "- `Units` The number of units of the new product\n",
    "purchased on the particular purchase occasion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-r0cpsA97dT"
   },
   "outputs": [],
   "source": [
    "kiwi_lf = pl.scan_csv(source=\"data/panel-datasets/kiwibubbles_trans.csv\",\n",
    "                      has_header=True,\n",
    "                      separator=\",\",\n",
    "                      schema={'ID': pl.UInt16,\n",
    "                              'Market': pl.UInt8,\n",
    "                              'Week': pl.Int16,\n",
    "                              'Day': pl.Int16,\n",
    "                              'Units': pl.Int16})\n",
    "kiwi_lf.head().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reusable Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekly Grocery Sales LazyFrame (Query Plan): Weekly 'spend' by 'Category', 'Brand' or 'All'\n",
    "def weekly_spend_summary(brand, lf):\n",
    "    summary = (\n",
    "        lf\n",
    "        .select(['week', 'units', 'price', 'brand'])\n",
    "        .with_columns(((pl.col('units') * pl.col('price'))).alias('spend'))\n",
    "    )\n",
    "    \n",
    "    if brand == 'Category': # Return LazyFrame of total category\n",
    "        summary = summary.group_by('week')\n",
    "    elif brand == 'All': # Return LazyFrame of all brands\n",
    "        summary = summary.group_by('week', 'brand')\n",
    "    else:  # Return LazyFrame of specified brand\n",
    "        summary = summary.filter(\n",
    "            pl.col('brand') == brand\n",
    "        ).group_by('week', 'brand')\n",
    "        \n",
    "    summary = summary.agg(\n",
    "        pl.col(\"spend\").sum().alias('Weekly Spend') \n",
    "    ).sort('week')\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekly Grocery Volume Sales LazyFrame (Query Plan): Weekly 'volume' by 'Brand' or 'All'\n",
    "def weekly_vol_summary(brand, lf):\n",
    "    with pl.StringCache():\n",
    "        lf = (\n",
    "            lf\n",
    "            .join(\n",
    "                other=sku_lf,\n",
    "                left_on=\"sku_id\",\n",
    "                right_on=\"sku_id\"            \n",
    "            )\n",
    "            .select(['week', 'units', 'brand', 'weight'])\n",
    "        )\n",
    "        \n",
    "        if brand != 'All': \n",
    "            brand = [brand] if type(brand) == str else brand\n",
    "            lf = lf.filter(\n",
    "                pl.col('brand').is_in(*[brand])\n",
    "            )\n",
    "            \n",
    "        summary = lf.with_columns(\n",
    "            (((pl.col('units') * pl.col('weight'))/1000)).alias('volume')\n",
    "        ).group_by('week', 'brand').agg(\n",
    "            pl.col(\"volume\").sum().alias('Weekly Volume')\n",
    "        ).sort('week')\n",
    "        \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Altair Weekly Line Plot\n",
    "def weekly_plot(dataframe, y, year=2, color=None, title=\"\", y_axis_label=\"\", pct=False, legend=False):\n",
    "    \n",
    "    # Configure the color encoding only if color is provided\n",
    "    if color is not None:\n",
    "        color_encoding = alt.Color(\n",
    "            f'{color}:N',  # N = a discrete unordered category\n",
    "            legend=alt.Legend(title=color) if legend else None  # Add legend conditionally\n",
    "        )\n",
    "    else:\n",
    "        color_encoding = alt.Color()  # No color encoding    \n",
    "    \n",
    "    chart = alt.Chart(dataframe).mark_line(strokeWidth=1).encode(\n",
    "        x = alt.X(\n",
    "            'week',\n",
    "            axis=alt.Axis(\n",
    "                values=np.arange(0, (year*52) + 1, 13), # Explicitly specify quarter-end weeks\n",
    "                labelExpr=\"datum.value\", # Show only these labels\n",
    "                title='Week'\n",
    "            )\n",
    "        ),\n",
    "        y = alt.Y(\n",
    "            f'{y}:Q', # Q = a continuous real-valued quantity\n",
    "            title=y_axis_label,\n",
    "            axis=alt.Axis(format=\"$,.0f\") if not pct else alt.Axis(format=\",.0%\")\n",
    "        ),\n",
    "        color = color_encoding\n",
    "    ).properties(\n",
    "        width=650,\n",
    "        height=250,\n",
    "        title=title\n",
    "    ).configure_view(\n",
    "        stroke=None\n",
    "    ).configure_axisY(\n",
    "        # grid=False # turn off y-axis grid if required\n",
    "    )\n",
    "\n",
    "    return chart # alt.JupyterChart(chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annual Sales Summary LazyFrame for All Brands\n",
    "def annual_sales_summary():\n",
    "    summary = (\n",
    "        weekly_spend_summary('All', grocery_lf)\n",
    "        .with_columns((pl.col(\"week\") / 52).ceil().alias('year'))\n",
    "        .group_by(['year', 'brand'])\n",
    "        .agg(pl.col(\"Weekly Spend\").sum().alias('Yearly Sales'))\n",
    "    ).sort('year')\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_dist_plot(\n",
    "    data, \n",
    "    column, \n",
    "    bin_edges, \n",
    "    labels, \n",
    "    x_title, \n",
    "    y_title, \n",
    "    chart_title, \n",
    "    subtitle, \n",
    "    width=650, \n",
    "    height=250, \n",
    "    label_angle=0, \n",
    "    left_closed=True, \n",
    "    compute_rel_freq=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a standardized Altair bar chart for relative frequency distribution plots.\n",
    "\n",
    "    Parameters:\n",
    "    - data (Polars LazyFrame or DataFrame): Input dataset.\n",
    "    - column (str): Column to analyze for distribution.\n",
    "    - bin_edges (array-like): Edges for binning.\n",
    "    - labels (list of str): Labels for the bins.\n",
    "    - x_title (str): Title for the x-axis.\n",
    "    - y_title (str): Title for the y-axis.\n",
    "    - chart_title (str): Main title for the chart.\n",
    "    - subtitle (str): Subtitle for the chart.\n",
    "    - width (int, optional): Width of the chart. Default is 650.\n",
    "    - height (int, optional): Height of the chart. Default is 250.\n",
    "    - label_angle (int, optional): Angle for x-axis labels. Default is 0.\n",
    "    - left_closed (bool, optional): Whether bins are left-closed. Default is True.\n",
    "    - compute_rel_freq (bool, optional): Whether to compute relative frequencies. Default is True.\n",
    "\n",
    "    Returns:\n",
    "    - alt.Chart: The generated Altair chart.\n",
    "    \"\"\"\n",
    "    # Apply binning to the data\n",
    "    binned_data = data.with_columns(\n",
    "        pl.col(column).cut(bin_edges, labels=labels, left_closed=left_closed).alias(\"cut\")\n",
    "    )\n",
    "\n",
    "    # Optionally compute relative frequencies\n",
    "    if compute_rel_freq:\n",
    "        binned_data = (\n",
    "            binned_data\n",
    "            .group_by(\"cut\")\n",
    "            .agg(pl.col(\"cut\").count().alias(\"Frequency\"))\n",
    "            .with_columns(\n",
    "                (pl.col(\"Frequency\") / pl.col(\"Frequency\").sum()).alias(\"% of Total\")\n",
    "            )\n",
    "            .collect()\n",
    "        )\n",
    "\n",
    "    # Create the Altair chart\n",
    "    chart = alt.Chart(binned_data).mark_bar().encode(\n",
    "        x=alt.X(\"cut:O\", axis=alt.Axis(labelAngle=label_angle, title=x_title), sort=labels),\n",
    "        y=alt.Y(\"% of Total:Q\", axis=alt.Axis(format=\".0%\", title=y_title)),\n",
    "    ).properties(\n",
    "        width=width,\n",
    "        height=height,\n",
    "        title={\"text\": chart_title, \"subtitle\": subtitle},\n",
    "    )\n",
    "\n",
    "    return chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAVjw68199_0"
   },
   "source": [
    "### <span style=\"color:#956bbf\">Preliminaries</span> \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start analysing household-level behaviour, let us first get a sense of the general sales patterns observed in this category.\n",
    "\n",
    "Our initial objective is to plot weekly revenue for all the brands and the overall category, respectively, and then plot weekly (volume) market shares for target brands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTV6Gf6T-FDX"
   },
   "source": [
    "#### Weekly Sales Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekly Sales Pivot Table - Polars DataFrame\n",
    "# For visualizing and inspecting only\n",
    "weekly_spend_summary('All', grocery_lf).collect().pivot(\n",
    "    on=\"brand\",\n",
    "    index=\"week\",\n",
    "    values=\"Weekly Spend\",\n",
    "    sort_columns=True,\n",
    ").with_columns(\n",
    "    pl.sum_horizontal(pl.exclude('week')).alias(\"Total\") # Row total\n",
    ").sort(\"week\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_plot(dataframe=weekly_spend_summary('Category', grocery_lf).collect(), \n",
    "            y='Weekly Spend', \n",
    "            title='Category - Weekly Revenue', \n",
    "            y_axis_label='Spend ($)',\n",
    "            pct=False,\n",
    "            legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_plot(dataframe=weekly_spend_summary('Alpha', grocery_lf).collect(), \n",
    "            y='Weekly Spend', \n",
    "            title='Alpha - Weekly Revenue', \n",
    "            y_axis_label='Spend ($)',\n",
    "            pct=False,\n",
    "            legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_plot(dataframe=weekly_spend_summary('Bravo', grocery_lf).collect(), \n",
    "            y='Weekly Spend', \n",
    "            title='Bravo - Weekly Revenue', \n",
    "            y_axis_label='Spend ($)',\n",
    "            pct=False,\n",
    "            legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weekly (Volume) Market Share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekly (volume) market share\n",
    "pct_volume =(\n",
    "    weekly_vol_summary('All', grocery_lf)\n",
    "    .collect()\n",
    "    .pivot(\n",
    "        on='brand',\n",
    "        index='week',\n",
    "        values='Weekly Volume',\n",
    "        sort_columns=True\n",
    "    )\n",
    "    .with_columns(\n",
    "        (pl.col(\"*\").exclude(\"week\")) / pl.sum_horizontal(pl.exclude('week'))\n",
    "    )\n",
    ")\n",
    "pct_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pl.StringCache():\n",
    "    pct_volume_plot = (\n",
    "        weekly_vol_summary('All', grocery_lf)\n",
    "        .group_by('week')\n",
    "        .agg(\n",
    "            pl.col('Weekly Volume').sum().alias('Total Weekly Volume')\n",
    "        )\n",
    "        .join(\n",
    "            other=weekly_vol_summary(['Alpha', 'Bravo'], grocery_lf),\n",
    "            on='week',\n",
    "        )\n",
    "        .filter(\n",
    "            (pl.col('brand').is_in(['Alpha', 'Bravo']))\n",
    "        )\n",
    "        .with_columns(\n",
    "            # compute brand wise % of total volume sale\n",
    "            (pl.col('Weekly Volume') / pl.col('Total Weekly Volume')).alias('pct_volume')\n",
    "        )\n",
    "    ).collect()\n",
    "\n",
    "weekly_plot(dataframe=pct_volume_plot, \n",
    "            y='pct_volume', \n",
    "            color='brand', \n",
    "            title=\"Volume Market Share - Alpha vs. Bravo\", \n",
    "            y_axis_label=\"% of Weekly Total\", \n",
    "            pct=True, \n",
    "            legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be a **high level of competition** between these two brands. What is the correlation in their market shares?\n",
    "\n",
    "We observe that there is a strong negative correlation between the shares of Alpha and Bravo: an increase in one brand’s share is associated with a corresponding decrease in the share of the other brand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = (\n",
    "    pct_volume.select(\n",
    "        pl.col('*').exclude('week')  # Exclude 'week' column\n",
    "    )\n",
    "    .corr()  # Compute the correlation matrix\n",
    "    .with_columns(\n",
    "        pl.Series(pct_volume.columns[1:]).alias(\"index\")\n",
    "    )\n",
    ")\n",
    "\n",
    "(\n",
    "    GT(corr_matrix, rowname_col='index')\n",
    "    .tab_header(title=\"Correlation Matrix of Weekly (Volume) Market Share\")\n",
    "    .fmt_number(columns=['Alpha', 'Bravo', 'Charlie', 'Delta', 'Other'])\n",
    "    .data_color(\n",
    "        domain=[-1, 1],\n",
    "        palette=[\"rebeccapurple\", \"white\", 'orange']\n",
    "    )\n",
    "    .tab_style(\n",
    "        style=[\n",
    "            style.text(color=\"black\"),\n",
    "            style.fill('white'),\n",
    "            \n",
    "        ],\n",
    "        locations=[\n",
    "            loc.body(0,0), \n",
    "            loc.body(1,1),\n",
    "            loc.body(2,2),\n",
    "            loc.body(3,3),\n",
    "            loc.body(4,4)\n",
    "        ]\n",
    "    )    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annual Sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us finish this preliminary analysis of the data by computing the annual sales of each brand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_sales_pivot = annual_sales_summary().collect().pivot(\n",
    "    on='brand',\n",
    "    index='year',\n",
    "    sort_columns=True\n",
    ").with_columns(\n",
    "    pl.sum_horizontal(pl.all().exclude('year')).alias(\"Total\") # add totals column\n",
    ")\n",
    "\n",
    "(\n",
    "    GT(annual_sales_pivot, rowname_col=\"year\")\n",
    "    .tab_header(title=\"Annual Sales ($)\")\n",
    "    .tab_stubhead(label=\"Year\")\n",
    "    .fmt_currency()\n",
    "    .fmt_integer(columns='year')\n",
    "    .data_color(\n",
    "        columns=annual_sales_pivot.columns[:-1],\n",
    "        domain=[100, 36_000],\n",
    "        palette=[\"white\", \"rebeccapurple\"]\n",
    "    )    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_change_sales = annual_sales_pivot.with_columns(\n",
    "    pl.col('*').exclude('year').pct_change()\n",
    ").filter(\n",
    "    pl.col('year') == 2\n",
    ").unpivot(\n",
    "    index=\"year\",  # Keep 'year' as a fixed identifier\n",
    "    value_name='% Change',  \n",
    "    variable_name='brand'\n",
    ")\n",
    "\n",
    "(\n",
    "    GT(annual_change_sales, rowname_col='brand')\n",
    "    .tab_header(title='% Change in Annual Sales')\n",
    "    .tab_stubhead(label=\"Brands\")\n",
    "    .fmt_percent()\n",
    "    .data_color(\n",
    "        domain=[-0.3, 0.3],\n",
    "        palette=['orange', 'white', 'rebeccapurple']\n",
    "    ) \n",
    "    .cols_hide('year')   \n",
    "    .cols_label(brand='Y-o-Y % Change')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = alt.Chart(annual_sales_summary().collect()).mark_bar().encode(\n",
    "    x=alt.X(\"brand:N\", \n",
    "            axis=alt.Axis(title='Brands', labelAngle=0)),\n",
    "    xOffset=\"year:N\",\n",
    "    y=alt.Y(\"Yearly Sales:Q\",\n",
    "            axis=alt.Axis(format=\"$,.0f\"),\n",
    "            title='Yearly Revenue ($)'),\n",
    "    color=alt.Color(\"year:N\", title='Year'),\n",
    ").properties(\n",
    "    width=650,\n",
    "    height=250,\n",
    "    title='Year 1 & Year 2 Revenues'\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = alt.Chart(annual_change_sales).encode(\n",
    "    x=alt.X(\"brand:N\", \n",
    "            title='Brand',\n",
    "            axis=alt.Axis(labelAngle=0)\n",
    "    ),\n",
    "    y=alt.Y(\"% Change:Q\",\n",
    "            axis=alt.Axis(format=\".0%\")\n",
    "    ),\n",
    "    color=alt.condition(\n",
    "        alt.datum[\"% Change\"] > 0, # fixed this \n",
    "        alt.value(\"green\"),  # The positive color\n",
    "        alt.value(\"red\")  # The negative color\n",
    "    ),\n",
    "    text=alt.Text(\n",
    "        '% Change',\n",
    "        format=(\".1%\")\n",
    "    )\n",
    "        \n",
    ").properties(\n",
    "    width=650,\n",
    "    height=250,\n",
    "    title='Y-o-Y % Change in Revenue'\n",
    ")\n",
    "\n",
    "chart = base.mark_bar() + base.mark_text(align='center', \n",
    "                                 baseline=alt.expr(alt.expr.if_(alt.datum['% Change'] >= 0, 'bottom', 'top')),\n",
    "                                 dy=alt.expr(alt.expr.if_(alt.datum['% Change'] >= 0, -2, 2)),\n",
    "                                 dx=0)\n",
    "chart.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annual Market Share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_share = annual_sales_pivot.with_columns(\n",
    "    pl.col('*').exclude('year') / pl.col('Total')\n",
    ")\n",
    "\n",
    "(\n",
    "    GT(market_share, rowname_col=\"year\")\n",
    "    .tab_header(title=\"Annual Market Share (%)\")\n",
    "    .tab_stubhead(label=\"Year\")\n",
    "    .fmt_percent()\n",
    "    .cols_hide('Total')\n",
    "    .data_color(\n",
    "        domain=[0, 0.5],\n",
    "        palette=[\"white\", \"rebeccapurple\"]\n",
    "    )    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_change_share = market_share.drop('Total').with_columns(\n",
    "    pl.col('*').exclude('year').pct_change()\n",
    ").filter(\n",
    "    pl.col('year') == 2\n",
    ").unpivot(\n",
    "    index=\"year\",  # Keep 'year' as a fixed identifier\n",
    "    value_name='% Change',  \n",
    "    variable_name='brand'\n",
    ")\n",
    "\n",
    "(\n",
    "    GT(annual_change_share, rowname_col='brand')\n",
    "    .tab_header(title='% Change in Market Share')\n",
    "    .tab_stubhead(label=\"Brands\")\n",
    "    .fmt_percent()\n",
    "    .data_color(\n",
    "        domain=[-0.3, 0.3],\n",
    "        palette=['orange', 'white', 'rebeccapurple']\n",
    "    ) \n",
    "    .cols_hide('year')   \n",
    "    .cols_label(brand='Y-o-Y % Change')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = alt.Chart(annual_change_share).encode(\n",
    "    x=alt.X(\"brand:N\",\n",
    "            title='Brands', \n",
    "            axis=alt.Axis(labelAngle=0)\n",
    "    ),\n",
    "    y=alt.Y(\"% Change:Q\",\n",
    "            axis=alt.Axis(format=\".0%\")\n",
    "    ),\n",
    "    color=alt.condition(\n",
    "        alt.datum[\"% Change\"] > 0, # fixed this \n",
    "        alt.value(\"green\"),  # The positive color\n",
    "        alt.value(\"red\")  # The negative color\n",
    "    ),\n",
    "    text=alt.Text(\n",
    "        '% Change',\n",
    "        format=(\".1%\")\n",
    "    )\n",
    "        \n",
    ").properties(\n",
    "    width=650,\n",
    "    height=250,\n",
    "    title='% Change in Annual Market Share'\n",
    ")\n",
    "\n",
    "chart = base.mark_bar() + base.mark_text(align='center', \n",
    "                                 baseline=alt.expr(alt.expr.if_(alt.datum['% Change'] >= 0, 'bottom', 'top')),\n",
    "                                 dy=alt.expr(alt.expr.if_(alt.datum['% Change'] >= 0, -3, 3)),\n",
    "                                 dx=0)\n",
    "\n",
    "chart.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average SKU Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sku_price = grocery_lf.select(\n",
    "    ['week', 'sku_id', 'price', 'brand']\n",
    ").filter(\n",
    "    (pl.col('week') <= 52) & # pricing in the first year\n",
    "    (pl.col('brand') == 'Alpha') # Filter by Alpha brand \n",
    ").group_by('brand', 'sku_id').agg(\n",
    "    pl.col('price').mean()\n",
    ").drop('brand').sort(\n",
    "    pl.col('sku_id').cast(pl.Int8)\n",
    ")\n",
    "\n",
    "(\n",
    "    GT(avg_sku_price.collect(), rowname_col='sku_id') \n",
    "    .tab_header(title='Alpha - Average SKU Retail Prices')\n",
    "    .tab_stubhead(label=\"SKU IDs\")\n",
    "    .fmt_currency()\n",
    "    .data_color(\n",
    "        domain=[1, 15],\n",
    "        palette=['white', 'rebeccapurple']\n",
    "    )  \n",
    "    .cols_label(price='Average Price ($)')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sku_price.drop(pl.col('sku_id')).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Garbage Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions = ['grocery_lf', 'sku_lf', 'kiwi_lf', 'In', 'exceptions', 'active_variables']\n",
    "\n",
    "active_variables = [\n",
    "    var for var, value in globals().items()\n",
    "    if not var.startswith('_')   # Exclude variables that start with \"_\"\n",
    "    and var not in exceptions    # Exclude variables in the exceptions list\n",
    "    and isinstance(value, (pl.LazyFrame, pl.DataFrame, pl.Series, alt.Chart, alt.LayerChart, list, int, float, str, np.ndarray, np.int64, np.float32))  # Remove these types only\n",
    "]\n",
    "\n",
    "for var in active_variables:\n",
    "    del globals()[var]\n",
    "del active_variables, exceptions, var\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#956bbf\">Exploring Variation in Buyer Behaviour</span>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective here is to explore how to perform the basic analyses that describe the variation we observe in buyer behaviour in a given time period. We will continue to work with the edible grocery dataset, exploring both purchase frequency and spend.\n",
    "\n",
    "Before we can doing any analysis, we need to create some summary datasets. The first will summarise **how many times each panellist purchased each brand as well as in the category**. The second will summarise **how much each panellist spent on each brand and in the category**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grocery_lf.filter(\n",
    "    (pl.col('panel_id') == 3102016) &\n",
    "    (pl.col('trans_id') == 844)\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this one shopping trip (`trans_id` = 844), panellist `3102021` purchased a total of six items: three packs of SKU 5, two packs of SKU 15, and one pack of SKU 89. They purchased two different brand, Alpha and Delta.\n",
    "\n",
    "Why are three lines for SKU 5 and not one line with units = 3. This is simply a function of how the items were scanned at the checkout. Some checkout operators will scan the three items separately; this would result in three lines in the transaction file, each with units = 1. Others will scan the item and press “3” on their till, resulting in one line in the transaction file with units = 3.\n",
    "\n",
    "By convention, this purchase occasion is recorded as one **category transaction**, one Alpha transaction, and one Delta transaction. The number of units of Alpha purchased is five. When we say that the panellist made one category transaction, we mean they purchased at least one item in the category on that shopping trip. When we say that the panellist made one Alpha transaction, we mean they purchased at least one item associated with the brand on that shopping trip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to analyse buyer behaviour in terms of transactions, we need to know the **number of brand and category transactions for each person**. We cannot work directly with the dataset we have been using; we effectively need to collapse the five rows associated with Alpha into one. This will require some intermediate analysis, which we undertake in the following manner.\n",
    "\n",
    "Result of the intermediate prep should produce a dataset that summarizes transaction 844 by panellist 3102016 as 1 occasion of Alpha brand purchase and 1 occasion of Delta brand purchase:\n",
    "<div>\n",
    "<img src=\"references\\intermediate-data-prep.png\" width=\"1000\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Panellist-Level Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"max-width:600px;margin-left: auto; margin-right: auto;\">\n",
    "<img src=\"references\\Consumer-Panel-Data-Relationship.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One row per transaction**, with a binary indicator of whether or not each brand was purchased.\n",
    "\n",
    "- Filter for year == 1, group by transaction ID, panel ID and brand, then count unique panel_ids per transaction and brand\n",
    "- Each panelist has done some number of transactions. Group unique transactions and obtain the count of transactions occasions in which any listed brand was purchased\n",
    "- Note: this is not a sum of total units (by brand) purchased per panellist.\n",
    "- Note: Because there are multiple entires for the same transaction ID, we are not doing a count of brand instance per panelist (without trans_id grouping) as this would duplicate the occasions in some cases.\n",
    "\n",
    "- Produce a dataset where there is just one row per transaction, with a binary indicator of whether or not each brand was purchased.\n",
    "- Create a new column to track category purchase. Each row is an instance of at least one brand purchase so category column is set to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Panellist-level *transaction* summary**\n",
    "\n",
    "The function below executes a query plan which summarizes each panellist's purchase occasions (transactions) for each brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_summary(brand, lf, year):\n",
    "    \n",
    "    # Primary Step: Filter by Year 1 and Remove Unused Columns\n",
    "    filtered_lf = lf.filter(\n",
    "        (pl.col('week') <= (year * 52)) &\n",
    "        (pl.col('week') > ((year - 1) * 52))\n",
    "    ).drop(\n",
    "        pl.col('week','sku_id')\n",
    "    )\n",
    "\n",
    "    # Intermediate Step: Group by trans_id, panel_id, and brand\n",
    "    group_trans = filtered_lf.drop(\n",
    "        pl.col('price', 'units')\n",
    "    ).group_by(\n",
    "        'trans_id', 'panel_id', 'brand'\n",
    "    ).n_unique()\n",
    "    \n",
    "    if brand == \"Category\":\n",
    "        # Panellist-level category transaction summary\n",
    "        summary = group_trans.group_by(\n",
    "            'panel_id'\n",
    "        ).n_unique()\n",
    "    else:\n",
    "        # Panellist-level brand transaction summary\n",
    "        summary = group_trans.filter(\n",
    "            pl.col('brand') == brand\n",
    "        ).group_by(\n",
    "            'panel_id'\n",
    "        ).n_unique()\n",
    "    \n",
    "    return summary.select(\n",
    "        pl.col('panel_id'),\n",
    "        pl.col('trans_id').alias('# of Purchases'),\n",
    "        pl.col('brand').alias('Brands Purchased')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_pivot(lf, year):\n",
    "    \n",
    "    # Primary Step: Filter by Year 1 and Remove Unused Columns\n",
    "    filtered_lf = lf.filter(\n",
    "        (pl.col('week') <= (year * 52)) &\n",
    "        (pl.col('week') > ((year - 1) * 52))\n",
    "    ).drop(\n",
    "        pl.col('week','sku_id')\n",
    "    )\n",
    "\n",
    "    # Intermediate Step: Group by trans_id, panel_id, and brand\n",
    "    group_trans = filtered_lf.drop(\n",
    "        pl.col('price', 'units')\n",
    "    ).group_by(\n",
    "        'trans_id', 'panel_id', 'brand'\n",
    "    ).n_unique() # count of unique entires\n",
    "    \n",
    "    summary = group_trans.collect().pivot(\n",
    "        on='brand',\n",
    "        index='panel_id',\n",
    "        values='panel_id',\n",
    "        aggregate_function=\"len\"\n",
    "        \n",
    "    ).join(\n",
    "        other=group_trans.group_by('panel_id').n_unique().drop('brand').collect(),\n",
    "        on='panel_id'\n",
    "    ).rename(\n",
    "        {'trans_id': 'Category'}\n",
    "    ).drop(\n",
    "        pl.col('panel_id')\n",
    "    )\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Panellist-level *spend* summary**\n",
    "\n",
    "The function below executes a query plan which summarizes the amount each panellist spent on each brand (and in the category) during year 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spend_summary(brand, lf, year):\n",
    "    \n",
    "    group_spend = lf.filter(\n",
    "        (pl.col('week') <= (year * 52)) &\n",
    "        (pl.col('week') > ((year - 1) * 52))\n",
    "    ).drop(\n",
    "        pl.col('week','sku_id')\n",
    "    ).with_columns(\n",
    "        ((pl.col('units') * pl.col('price'))).alias('spend')\n",
    "    )\n",
    "    \n",
    "    if brand == \"Category\":\n",
    "        # Panellist-level category spend summary\n",
    "        summary = group_spend.drop(\n",
    "            pl.col('units', 'price', 'brand')\n",
    "        ).group_by(\n",
    "            'panel_id'\n",
    "        ).agg(\n",
    "            pl.col('spend').sum()\n",
    "        )\n",
    "    else:\n",
    "        # Panellist-level brand spend summary\n",
    "        summary = group_spend.drop(\n",
    "            pl.col('units', 'price')\n",
    "        ).group_by(\n",
    "            'panel_id', 'brand'\n",
    "        ).agg(\n",
    "            pl.col('spend').sum()\n",
    "        ).filter(\n",
    "            pl.col('brand') == brand\n",
    "        ).drop('brand')\n",
    "        \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spend_pivot(lf, year):\n",
    "    \n",
    "    group_spend = lf.filter(\n",
    "        (pl.col('week') <= (year * 52)) &\n",
    "        (pl.col('week') > ((year - 1) * 52))\n",
    "    ).drop(\n",
    "        pl.col('week','sku_id')\n",
    "    ).with_columns(\n",
    "        ((pl.col('units') * pl.col('price'))).alias('spend')\n",
    "    ).collect().pivot(\n",
    "        on='brand',\n",
    "        index='panel_id',\n",
    "        values='spend',\n",
    "        aggregate_function='sum'\n",
    "    ).select(\n",
    "        'Alpha', 'Bravo', 'Charlie', 'Delta', 'Other'\n",
    "    )\n",
    "\n",
    "    return group_spend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Panellist-level *volume purchasing* summary**\n",
    "\n",
    "The function below executes a query plan which summarises each panellist’s volume purchasing in year 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vol_summary(brand, lf, year):\n",
    "    with pl.StringCache():\n",
    "        group_vol = lf.filter(\n",
    "            (pl.col('week') <= (year * 52)) &\n",
    "            (pl.col('week') > ((year - 1) * 52))\n",
    "        ).join(\n",
    "            other=sku_lf,\n",
    "            left_on='sku_id',\n",
    "            right_on='sku_id'\n",
    "        ).drop(\n",
    "            pl.col('week','sku_id')\n",
    "        ).with_columns(\n",
    "            # volume column that is the product of weight of each SKU and the units of SKU sold\n",
    "            (((pl.col('units') * pl.col('weight'))/1000)).alias('volume') # # weight from grams to kilograms\n",
    "        ).drop(\n",
    "            pl.col('units', 'price', 'weight')\n",
    "        )\n",
    "        \n",
    "        if brand == \"Category\":\n",
    "            # Panellist-level category volume sales summary\n",
    "            summary = group_vol.drop(\n",
    "                pl.col('brand')\n",
    "            ).group_by(\n",
    "                'panel_id'\n",
    "            ).agg(\n",
    "                pl.col('volume').sum()\n",
    "            )\n",
    "        else:\n",
    "            # Panellist-level brand volume sales summary\n",
    "            summary = group_vol.group_by(\n",
    "                'panel_id', 'brand'\n",
    "            ).agg(\n",
    "                pl.col('volume').sum()\n",
    "            ).filter(\n",
    "                pl.col('brand') == brand\n",
    "            ).drop('brand')\n",
    "        \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vol_pivot(lf, year):\n",
    "    with pl.StringCache():\n",
    "        group_vol = lf.filter(\n",
    "            (pl.col('week') <= (year * 52)) &\n",
    "            (pl.col('week') > ((year - 1) * 52))\n",
    "        ).join(\n",
    "            other=sku_lf,\n",
    "            left_on='sku_id',\n",
    "            right_on='sku_id'\n",
    "        ).drop(\n",
    "            pl.col('week','sku_id')\n",
    "        ).with_columns(\n",
    "            # volume column that is the product of weight of each SKU and the units of SKU sold\n",
    "            (((pl.col('units') * pl.col('weight'))/1000)).alias('volume') # # weight from grams to kilograms\n",
    "        ).drop(\n",
    "            pl.col('units', 'price', 'weight')\n",
    "        ).collect().pivot(\n",
    "            on='brand',\n",
    "            index='panel_id',\n",
    "            values='volume',\n",
    "            aggregate_function='sum'\n",
    "        ).select(\n",
    "            'Alpha', 'Bravo', 'Charlie', 'Delta', 'Other'\n",
    "        )\n",
    "        \n",
    "    return group_vol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at panellist-level spend and panellist-level volume purchasing, we note that for each row, `Category` equals the sum of the brand numbers, as we would expect. But this is not always the case in panellist-level transactions. Why is the sum of the brand-specific numbers sometimes greater than the associated category number?\n",
    "\n",
    "Answer: Because a single transactions (once grouped) can have occasions of multiple brand purchases but the transaction only counts as one occasion of a category purchase. Consider the panellist `3102016` and her transaction ID `844`, The 4 entires on the one transaction ID count as a single category transaction, a single Alpha brand transaction and a single Delta transaction. The category transaction count is not a sum of brand purchase transaction count because regardless of the brand(s) and quantities purchased, the panelist purchased the category once in that transaction only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining Purchase Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two standard brand performance metrics that summarize purchasing behaviour are **penetration** and **purchases per buyer (PPB)**. \n",
    "\n",
    "- *Penetration* is the percentage of households buying the product/category at least once in the given time period. \n",
    "  - In order to compute this, we need to know the number of panellists buying the product at least once in the time period of interest and the size of the panel during this period.\n",
    "- *Purchases per buyer (PPB)* is the average number of times (separate shopping trips) the product/category was purchased (in the given time period) by those households that made at least one product/category purchase (in the given time period).\n",
    "  - i.e. purchases per buyer (PPB) is the average number of times the product was purchased (in the given time period) per buyer. \n",
    "  - This is computed as the total number of purchase occasions on which the product was purchased by the panellists (in the time period of interest) divided by the number of panellists that purchased the product at least once (in the time period of interest).\n",
    "\n",
    "Looking at *panellist-level transaction summary*, we see that there are 4574 rows in this table, meaning that we have summary data on the purchasing of 4574 households in year 1. But there are 5021 households in the panel. What has happened to the remaining 447 households? They did not make any category purchase during the year. (But they will have purchased in other categories.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of panellists who purchased each brand at least once in year 1\n",
    "buyers = trans_pivot(grocery_lf, 1).count().select(\n",
    "    'Alpha', 'Bravo', 'Charlie', 'Delta', 'Other', 'Category'\n",
    ").unpivot(\n",
    "    variable_name='brand',\n",
    "    value_name='buyers'\n",
    ")\n",
    "\n",
    "(\n",
    "    GT(buyers, rowname_col='brand')\n",
    "    .tab_header(title='Number of Buyers in Year 1')\n",
    "    .tab_stubhead(label=\"Brands/Category\")\n",
    "    .fmt_integer()\n",
    "    .data_color(\n",
    "        rows=['Alpha', 'Bravo', 'Charlie', 'Delta', 'Other'],\n",
    "        domain=[100, 3_000],\n",
    "        palette=['white', 'rebeccapurple']\n",
    "    )  \n",
    "    .cols_label(buyers='# Of Buyers')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On how many purchase occasions did the brand/category buyers of each brand/category buy that brand/category?\n",
    "# i.e, On how many purchase occasions did the 2624 buyers of Alpha buy Alpha?\n",
    "transactions = trans_pivot(grocery_lf, 1).sum().select(\n",
    "    'Alpha', 'Bravo', 'Charlie', 'Delta', 'Other', 'Category'\n",
    ").unpivot(\n",
    "    variable_name='brand',\n",
    "    value_name='transactions'\n",
    ")\n",
    "\n",
    "(\n",
    "    GT(transactions, rowname_col='brand')\n",
    "    .tab_header(title='Purchase Occasions by Buyers')\n",
    "    .tab_stubhead(label=\"Brands/Category\")\n",
    "    .fmt_integer()\n",
    "    .data_color(\n",
    "        rows=['Alpha', 'Bravo', 'Charlie', 'Delta', 'Other'],\n",
    "        domain=[400, 10_000],\n",
    "        palette=[ 'white', 'rebeccapurple']\n",
    "    )  \n",
    "    .cols_label(transactions='# Of Transactions')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_size = 5021\n",
    "\n",
    "# Penetration is simply the number of brand buyers divided by the number of panellists, expressed as a percentage\n",
    "penetration = buyers.with_columns(\n",
    "    (pl.col('buyers') / panel_size).alias('penetration')\n",
    ")\n",
    "\n",
    "# Purchases per buyer (PPB) is simply to total number of purchases occasions divided by the number of buyers.\n",
    "ppb = transactions.with_columns(\n",
    "    (pl.col('transactions') / penetration.get_column('buyers')).alias('ppb')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    GT(penetration, rowname_col='brand')\n",
    "    .tab_header(title='Penetration by Category & Brand')\n",
    "    .tab_stubhead(label=\"Brands/Category\")\n",
    "    .fmt_percent()\n",
    "    .data_color(\n",
    "        rows=['Alpha', 'Bravo', 'Charlie', 'Delta', 'Other'],\n",
    "        domain=[0, 1],\n",
    "        palette=['white', 'rebeccapurple']\n",
    "    )  \n",
    "    .cols_label(penetration='Penetration (%)')\n",
    "    .cols_hide('buyers')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    GT(ppb, rowname_col='brand')\n",
    "    .tab_header(title='Purchases per Buyer (PPB)')\n",
    "    .tab_stubhead(label=\"Brands/Category\")\n",
    "    .fmt_number()\n",
    "    .data_color(\n",
    "        rows=['Alpha', 'Bravo', 'Charlie', 'Delta', 'Other'],\n",
    "        domain=[0, 5],\n",
    "        palette=['white', 'rebeccapurple']\n",
    "    )  \n",
    "    .cols_label(ppb='PPB')\n",
    "    .cols_hide('transactions')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that 91% of the households in the panel purchased in the category at least once in year 1. (This is a widely purchased product category.) On average, they purchased in the category 4.4 times that year. Looking at Alpha, we see that 52% of the households in the panel purchased the brand at least once, purchasing it on average 3.5 times.\n",
    "\n",
    "Penetration and PPB are in fact summary measures of an important but frequently overlooked summary of buyer behaviour: the **distribution of purchase frequency**. We first explore how to create this summary of category purchasing and then consider how to create such a summary of brand purchasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of Purchase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distribution of Category Purchase Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the *panellist-level transaction summary*, the distribution of category purchasing is determined by counting how many households made one category purchase (panellists 3102011, 3102046, etc.), two category purchases (panellists 3102012, 3102021, etc.), and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category Transaction Distribution\n",
    "category_trans_dist = (\n",
    "    trans_summary('Category', grocery_lf, 1)\n",
    "    .group_by(pl.col('# of Purchases'))\n",
    "    .agg(pl.len().alias(\"Frequency\"))\n",
    "    .collect() \n",
    "    # Add zero purchase row to the top of the data series with a new column name 'Frequency'\n",
    "    .pipe(\n",
    "        lambda df: df.vstack(\n",
    "            pl.DataFrame(\n",
    "                {\n",
    "                    \"# of Purchases\": [0],\n",
    "                    \"Frequency\": [panel_size - df[\"Frequency\"].sum()],\n",
    "                }\n",
    "            ).with_columns(\n",
    "                pl.col(\"# of Purchases\").cast(pl.UInt32),\n",
    "                pl.col('Frequency').cast(pl.UInt32)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    .sort(by='# of Purchases')\n",
    "    # Add percentage of total column\n",
    "    .with_columns(\n",
    "        (pl.col('Frequency') / pl.col('Frequency').sum()).alias(\"% of Total\")\n",
    "    )    \n",
    ")\n",
    "\n",
    "(\n",
    "    GT(category_trans_dist, rowname_col='# of Purchases')\n",
    "    .tab_header(title='Category Purchase Distribution')\n",
    "    .tab_stubhead(label=\"# of Purchases\")\n",
    "    .fmt_integer(columns='Frequency')\n",
    "    .fmt_percent(columns='% of Total')\n",
    "    .data_color(\n",
    "        columns=['% of Total'],\n",
    "        domain=[0, 0.2],\n",
    "        palette=['white', 'rebeccapurple']\n",
    "    )  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wish to create a visual representation of this distribution, it is tempting to simply plot the data in % of total column. However, the resulting plot would be misleading as *some purchase frequencies are missing in the data*. In particular, we see that no one made *21 category purchases; ditto for 23, 24, and 26*. One solution is to insert manually the missing number of purchases levels with 0 frequencies and then plot the data. However, the observed (relative) frequencies in the right tail are so small that they do not show up in a plot. We can therefore create a **right-censored distribution**. Here we have chosen 15 as the (right) censoring point; we see that 51 households (or 1% of the panellists) made 15 or more category purchases in year 1.\n",
    "\n",
    "There is nothing magical about our choice of 15. Generally, the choice of censoring point is a function of how many bins you wish to display and the height of the right-most bar with which you feel comfortable.\n",
    "\n",
    "Note that while the right-censored distribution is useful when creating a summary figure or table, it is of limited value beyond that. For example, it is not possible to compute the **mean purchase frequency** (and therefore PPB) from this summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom bin list (with an overflow bin) to label categories\n",
    "labels = [str(i) if i < 15 else '15+' for i in range(0, 16, 1)]\n",
    "chart = freq_dist_plot(\n",
    "    data=category_trans_dist,\n",
    "    column=\"# of Purchases\",\n",
    "    bin_edges=np.arange(0, 15, 1),\n",
    "    labels=labels,\n",
    "    x_title=\"Number of Category Transactions\",\n",
    "    y_title=\"% of Households\",\n",
    "    chart_title=\"Category Transaction Distribution (%)\",\n",
    "    subtitle=\"Distribution of category purchasing in year 1\",\n",
    "    left_closed=False,\n",
    "    compute_rel_freq=False\n",
    ")\n",
    "chart.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that 9% of the panellists made no category purchases, 13% of the panellists made one category purchase, . . . , and 1% of the panellists made at least 15 category purchases in year 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Relationship Between the Mean and PPB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the average number of times the category was purchased by a household in year 1?\n",
    "\n",
    "- Recall that mean, which we denote by $E(X)$, is given by:\n",
    "  - $$E(X)=\\sum\\frac{xf_{x}}{n}$$\n",
    "  - where $f_{x}$ is the frequency with which $x$ occurs in the dataset, $n$ is the sample size, and the summation is over all possible values of $x$.\n",
    "- Denoting the relative frequency with which $x$ occurs (i.e., $f_{x}/n$) by $P(X = x)$, the mean is given by:\n",
    "  - $$E(X)=\\sum x P(X = x)$$\n",
    "  - More formally, we can say that $P(X = x)$ is the empirical probability that a randomly chosen household made $x$ purchases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of the relative purchase frequency\n",
    "mean = category_trans_dist.select(\n",
    "    (pl.col('# of Purchases') * pl.col('% of Total')).sum()\n",
    ").item(0,0)\n",
    "print(f'{mean = :0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that average number of category purchases is 4.0. Why is this different from the 4.4 purchases per buyer (PPB) computed above? The mean we have just computed *includes those households that made zero purchases*, whereas *PPB is the average among those households that made at least one* (in this case category) purchase.\n",
    "\n",
    "We can derive the relationship between these two quantities (mean of the relative frequency and PPB) in the following manner:\n",
    "$$\n",
    "  \\begin{aligned}\n",
    "    PBB &= \\sum_{x=1}^{\\max x} \\frac{xf_{x}}{n-f_{0}} \\\\\n",
    "                   &= \\sum_{x=0}^{\\max x} \\frac{xf_{x}}{n-f_{0}} \\\\\n",
    "                   &= \\sum_{x=0}^{\\max x} \\left(\\frac{xf_{x}}{n-f_{0}}\\right)\\left(\\frac{n}{n}\\right) \\\\\n",
    "                   &= \\sum_{x=0}^{\\max x} \\left(\\frac{xf_{x}}{n}\\right)\\left(\\frac{n}{n-f_{0}}\\right) \\\\\n",
    "                   &= \\left(\\frac{n}{n-f_{0}}\\right)\\sum_{x=0}^{\\max x} \\left(\\frac{xf_{x}}{n}\\right) \\\\\n",
    "                   &= \\frac{E(X)}{1-P(X=0)}\n",
    "  \\end{aligned}\n",
    "$$\n",
    "\n",
    "In other words, **PPB is the mean divided by penetration**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All category PPB computations here are equivalent\n",
    "ppb_category = mean / penetration.filter(pl.col('brand') == 'Category').item(0,2)\n",
    "print(ppb_category)\n",
    "print(ppb.filter(pl.col('brand') == 'Category').item(0,2))\n",
    "ppb_category = mean / (1 - category_trans_dist['% of Total'][0])\n",
    "print(ppb_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distribution of Brand Purchase Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now create the distribution of purchase frequency for Alpha, which is. With the one exception noted below, the logic follows that associated with creating the distribution of category purchasing.\n",
    "\n",
    "In contrast to the `groupby` aggregation output associated with our summary of category purchasing, this aggregation does contain a zero category. However, we must be careful in our interpretation of the associated frequency. We see that 1950 category buyers did not buy Alpha in year 1. However, in order to have a complete summary of brand purchasing, we should also account for those 447 households that made no category purchases that year. The number of panellists making zero purchases of Alpha is the total number of panellists (5021) minus the number of panellists that made 1, 2, 3, . . . purchases.\n",
    "\n",
    "**Note**: What is Alpha’s penetration of category buyers? (1 − 1950/4574) = 57%.\n",
    "- Category Penetration = (1 - % of category buyers who did not purchase the brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha Brand Transaction Distribution\n",
    "alpha_trans_dist = (\n",
    "    trans_summary('Alpha', grocery_lf, 1)\n",
    "    .group_by(pl.col('# of Purchases'))\n",
    "    .agg(pl.len().alias(\"Frequency\"))\n",
    "    .collect() \n",
    "    .pipe(\n",
    "        lambda df: df.vstack(\n",
    "            pl.DataFrame(\n",
    "                {\n",
    "                    \"# of Purchases\": [0],\n",
    "                    # Calculate zero purchases by subtracting the total sum of all purchases from the full panel size\n",
    "                    \"Frequency\": [panel_size - df[\"Frequency\"].sum()], # This ensures all panelists who made zero purchases are accounted for\n",
    "                }\n",
    "            ).with_columns(\n",
    "                pl.col(\"# of Purchases\").cast(pl.UInt32),\n",
    "                pl.col('Frequency').cast(pl.UInt32)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    .sort(by='# of Purchases')\n",
    "    .with_columns(\n",
    "        # Add a '% of Total' column\n",
    "        (pl.col('Frequency') / pl.col('Frequency').sum()).alias(\"% of Total\")\n",
    "    )    \n",
    ")\n",
    "\n",
    "(\n",
    "    GT(alpha_trans_dist, rowname_col='# of Purchases')\n",
    "    .tab_header(title='Alpha Purchase Distribution')\n",
    "    .tab_stubhead(label=\"# of Purchases\")\n",
    "    .fmt_integer(columns='Frequency')\n",
    "    .fmt_percent(columns='% of Total')\n",
    "    .data_color(\n",
    "        columns=['% of Total'],\n",
    "        domain=[0, 0.5],\n",
    "        palette=['white', 'rebeccapurple']\n",
    "    )  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom bin list (with an overflow bin) to label categories\n",
    "labels = [str(i) if i < 10 else '10+' for i in range(11)]\n",
    "\n",
    "chart = freq_dist_plot(\n",
    "    data=alpha_trans_dist,\n",
    "    column=\"# of Purchases\",\n",
    "    bin_edges=np.arange(0, 10, 1),\n",
    "    labels=labels,\n",
    "    x_title=\"Number of Transactions\",\n",
    "    y_title=\"% of Households\",\n",
    "    chart_title=\"Alpha Transaction Distribution (%)\",\n",
    "    subtitle=\"Distribution of purchase frequency for Alpha in year 1\",\n",
    "    left_closed=False,\n",
    "    compute_rel_freq=False\n",
    ")\n",
    "chart.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of Spend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distribution of Category Spend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now turn our attention to creating summaries of total spend. Our initial goal is to create a histogram of category spend (in dollars) across those panellists that made at least one purchase in the category in year 1. In the following plot, the raw total spend data have been binned in bins with a width of $5. \n",
    "\n",
    "We see that 16% of category buyers spent up to $5 in the category during this one-year period; 23% spent between $5 and $10; . . . and 2% spent more than $50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before deciding on what bin width to use when creating the histogram, let us first compute some basic descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_spend = spend_summary('Category', grocery_lf, 1).drop('panel_id')\n",
    "\n",
    "category_spend.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to visualize the variability in category spend. Given this objective, some would automatically think of creating a (kernel) density plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(category_spend.collect()).transform_density(\n",
    "    'spend',\n",
    "    as_=['spend', 'Density']\n",
    ").mark_area().encode(\n",
    "    x=alt.X(\"spend:Q\").scale(domain=(0,150)),\n",
    "    y='Density:Q',\n",
    ").properties(\n",
    "    width=650,\n",
    "    height=250\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this provides a good visualisation of the shape of the distribution, it can be difficult for most “consumers” of the plot to extract some additional information that may be of interest. For example, it is not easy to answer the question “What percentage of category buyers spent $30 or less in year 1?”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be quite a bit of variability in category spend. To get further insight into the distribution of total category spend across the panellists, we need to determine the total spend quantities associate with the various percentile levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = np.arange(5, 96, 5)\n",
    "# np.percentile returns the k-th percentile of values in a range\n",
    "percentile_values = np.percentile(category_spend.select('spend').collect(), percentiles)\n",
    "\n",
    "percentile_df = pl.DataFrame({'Percentile Level': percentiles,\n",
    "                              'Percentile': percentile_values})\n",
    "\n",
    "(\n",
    "    GT(percentile_df, rowname_col='Percentile Level')\n",
    "    .tab_header(title='Category Spend Percentile')\n",
    "    .tab_stubhead(label=\"Percentile Level\")\n",
    "    .fmt_number()\n",
    "    .data_color(\n",
    "        domain=[0, 40],\n",
    "        palette=['white', 'rebeccapurple']\n",
    "    )  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that 5% of the category buyers spent $2.69 or less in the category during the year, 10% spent $3.39 or less, and so on. The heaviest 5% of buyers each spent more than $39.72 in the category during the year.\n",
    "\n",
    "Looking out this output, we conclude that a bin size of $5 is probably about right.\n",
    "\n",
    "How many bins do we go with? This is an empirical question. Since 5% of the panel spent more than $39.72, we certainly want to go above $40 in order to get a sense of how they are spread between $39.72 and the maximum of $166.70. We will go with $50. If too many panellists have spent more than $50, we can always add more bins. If too few panellists fall into this bin, we can always combine the bins we have created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The distributions of many customer behaviours have a long right tail. Accommodating the range of values can make it difficult to get a clear sense of what is happening on the left side of the distribution. It can therefore be helpful to bin the data (as with a histogram) but to right censor the data, assigning all of the observations with a value of $𝑥$ or higher to an $𝑥 +$ bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that 750 panellists spent $5 or less in the category during year 1, 1073 spent between $5 and $10, . . . , and 109 spent more than $50.\n",
    "\n",
    "We convert the raw counts into percentages and plot these percentages as a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom bin list (with an overflow bin) to label categories\n",
    "labels = [f\"{lb} - {lb + 5 if lb != 50 else '∞'}\" for lb in range(-5, 55, 5)]\n",
    "chart = freq_dist_plot(\n",
    "    data=category_spend,\n",
    "    column=\"spend\",\n",
    "    bin_edges=np.arange(0, 51, 5),\n",
    "    labels=labels,\n",
    "    x_title=\"Category Spend ($)\",\n",
    "    y_title=\"% of Households\",\n",
    "    chart_title=\"Category Spend Distribution (%)\",\n",
    "    subtitle=\"Distribution of category spend across category buyers\"\n",
    ")\n",
    "chart.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general shape of this distribution (i.e., an interior mode, median less than the mean, right-skewed with a long right-tail) is what we typically observe when we look at spend data.\n",
    "\n",
    "This, of course, depends on the bin width we choose when summarising the data. If we had chosen a bin width of $10, the left-most bar would be the highest bar and we would no longer observe an interior mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distribution of Brand Spend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now create the distribution of spend on Alpha. We will follow the same basic process as for the distribution of category spend with a few minor changes.\n",
    "\n",
    "Note that there are a number of 0 or NaN values in the Alpha spend column. While these panellists purchased in the category during the year, they did not purchase any of Alpha’s SKUs. The first thing we need to do is remove these observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_spend = spend_summary('Alpha', grocery_lf, 1).drop('panel_id')\n",
    "\n",
    "alpha_spend.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = np.arange(5, 96, 5)\n",
    "\n",
    "percentile_values = np.percentile(alpha_spend.select('spend').collect(), percentiles)\n",
    "\n",
    "percentile_df = pl.DataFrame({'Percentile Level': percentiles,\n",
    "                              'Percentile': percentile_values})\n",
    "\n",
    "(\n",
    "    GT(percentile_df, rowname_col='Percentile Level')\n",
    "    .tab_header(title='Alpha Spend Percentile')\n",
    "    .tab_stubhead(label=\"Percentile Level\")\n",
    "    .fmt_number()\n",
    "    .data_color(\n",
    "        domain=[0, 40],\n",
    "        palette=['white', 'rebeccapurple']\n",
    "    )  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having looked at some basic descriptive stats (as above), we will summarise the data using bins of width $2, right-censoring at $40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [f\"{lb} - {lb + 2 if lb != 40 else '∞'}\" for lb in range(-2, 41, 2)]\n",
    "chart = freq_dist_plot(\n",
    "    data=alpha_spend,\n",
    "    column=\"spend\",\n",
    "    bin_edges=np.arange(0, 41, 2),\n",
    "    labels=labels,\n",
    "    x_title=\"Brand Spend ($)\",\n",
    "    y_title=\"% of Households\",\n",
    "    chart_title=\"Alpha Spend Distribution (%)\",\n",
    "    subtitle=\"Distribution of spend on Alpha across brand buyers\",\n",
    "    label_angle=-45\n",
    ")\n",
    "chart.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general observation made earlier about category spend distribution about the shape of the distribution of spend holds. It is not so smooth, but this is a function of the smaller bins sizes. There is, however, one obvious aberration—the large spike for \\$2–\\$4. What is going on here? The average price of most Alpha SKUs is between \\$2 and \\$4. (Review average SKU prices of Alpha) We recall from Alpha's distribution of purchase frequency plot that 48% of the panel made zero purchases of Alpha and 15% made one transaction. This implies that 0.15/(1 − 0.48) = 29% of Alpha buyers made just one purchase of the brand. Assuming they only purchased only one unit of one Alpha SKU on that purchase occasion, we would expect a large number of Alpha buyers to spend between \\$2 and \\$4; 24% is not unrealistic.\n",
    "\n",
    "How would you determine the validity of the assumption that the buyers who purchased Alpha just once mostly bought only one unit of Alpha SKU on that purchase occasion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyzing Alpha's One-Time Buyers in Year 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Time Alpha Buyers\n",
    "one_time_alpha_buyers = trans_summary('Alpha',grocery_lf,1).filter(pl.col('# of Purchases') == 1).collect()\n",
    "one_time_alpha_buyers\n",
    "\n",
    "# Alpha buyers who spent between $2 to $4\n",
    "spend_range_alpha_buyers = spend_summary('Alpha', grocery_lf, 1).filter(\n",
    "    (pl.col('spend') >= 2) &\n",
    "    (pl.col('spend') < 4)\n",
    ").collect()\n",
    "\n",
    "res = one_time_alpha_buyers.with_columns(\n",
    "    contains=pl.col('panel_id').is_in(spend_range_alpha_buyers.select('panel_id'))\n",
    ").select('contains').sum().item()\n",
    "\n",
    "print(f'There were {one_time_alpha_buyers.count().item(0,'panel_id')} one-time buyers of Alpha brand in Year 1.')\n",
    "print(f'{res} of those one-time Alpha buyers spent between [$2, $4).')\n",
    "print(f'Given that there are {spend_range_alpha_buyers.count().item(0,'spend')} Alpha buyers in total who spent between $2-$4,')\n",
    "print(f'the {res} one-time buyers represent {res / spend_range_alpha_buyers.count().item(0,'spend'):.0%} of the Alpha buyers in that spend range.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validate**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spend_range_alpha_buyers.filter(\n",
    "    pl.col('panel_id').is_in(one_time_alpha_buyers.select('panel_id'))\n",
    ").select(pl.col('spend').sum()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grocery_lf.filter(\n",
    "    (pl.col('week') <= 52) &\n",
    "    (pl.col('panel_id').is_in(spend_range_alpha_buyers.select('panel_id'))) &  # Assuming panel_id is the relevant column\n",
    "    (pl.col('panel_id').is_in(one_time_alpha_buyers.select('panel_id'))) &  # Same assumption\n",
    "    (pl.col('brand') == 'Alpha')\n",
    ").with_columns(\n",
    "    ((pl.col('units') * pl.col('price'))).alias('spend')\n",
    ").select(pl.col('spend').sum()).collect().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of Volume Purchase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distribution of Category Volume Purchase Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_vol = vol_summary('Category', grocery_lf, 1).drop('panel_id')\n",
    "\n",
    "category_vol.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = np.arange(5, 96, 5)\n",
    "\n",
    "with pl.StringCache(): \n",
    "    percentile_values = np.percentile(category_vol.select('volume').collect(), percentiles)\n",
    "\n",
    "percentile_df = pl.DataFrame({'Percentile Level': percentiles,\n",
    "                              'Percentile': percentile_values})\n",
    "\n",
    "(\n",
    "    GT(percentile_df, rowname_col='Percentile Level')\n",
    "    .tab_header(title='Category Volume Percentile')\n",
    "    .tab_stubhead(label=\"Percentile Level\")\n",
    "    .fmt_number()\n",
    "    .data_color(\n",
    "        domain=[0, 15],\n",
    "        palette=['white', 'rebeccapurple']\n",
    "    )  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [f\"{lb} - {lb + 1 if lb != 12 else '∞'}\" for lb in range(-1, 13, 1)]\n",
    "chart = freq_dist_plot(\n",
    "    data=category_vol,\n",
    "    column=\"volume\",\n",
    "    bin_edges=np.arange(0, 13, 1),\n",
    "    labels=labels,\n",
    "    x_title=\"Category Volume Purchase ($)\",\n",
    "    y_title=\"% of Households\",\n",
    "    chart_title=\"Category Volume Purchase Distribution (%)\",\n",
    "    subtitle=\"Distribution of category volume purchases across category buyers\"\n",
    ")\n",
    "chart.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distribution of Brand Volume Purchase Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are a number of 0 or NaN values in the Alpha volume purchase column. These panaellists (like the brand spend section) purchased in the category during the year, but they did not purchase any of Alpha's products. Let us remove observations of `panel_ids` that have not at least made 1 purchase from Alpha brand during the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_vol = vol_summary('Alpha', grocery_lf, 1).drop('panel_id')\n",
    "\n",
    "alpha_vol.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = np.arange(5, 96, 5)\n",
    "\n",
    "with pl.StringCache(): \n",
    "    percentile_values = np.percentile(alpha_vol.select('volume').collect(), percentiles)\n",
    "\n",
    "percentile_df = pl.DataFrame({'Percentile Level': percentiles,\n",
    "                              'Percentile': percentile_values})\n",
    "\n",
    "(\n",
    "    GT(percentile_df, rowname_col='Percentile Level')\n",
    "    .tab_header(title='Alpha Volume Percentile')\n",
    "    .tab_stubhead(label=\"Percentile Level\")\n",
    "    .fmt_number()\n",
    "    .data_color(\n",
    "        domain=[0, 15],\n",
    "        palette=['white', 'rebeccapurple']\n",
    "    )  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [f\"{lb} - {lb + 1 if lb != 9 else '∞'}\" for lb in range(-1, 10, 1)]\n",
    "\n",
    "chart = freq_dist_plot(\n",
    "    data=alpha_vol,\n",
    "    column=\"volume\",\n",
    "    bin_edges=np.arange(0, 10, 1),\n",
    "    labels=labels,\n",
    "    x_title=\"Brand Volume Purchase ($)\",\n",
    "    y_title=\"% of Households\",\n",
    "    chart_title=\"Alpha Brand Volume Purchase Distribution (%)\",\n",
    "    subtitle=\"Distribution of Alpha volume purchases across brand buyers\"\n",
    ")\n",
    "chart.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Garbage Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions = ['grocery_lf', 'sku_lf', 'kiwi_lf', 'In', 'exceptions', 'active_variables',\n",
    "              'penetration', 'ppb', 'panel_size']\n",
    "\n",
    "active_variables = [\n",
    "    var for var, value in globals().items()\n",
    "    if not var.startswith('_')   # Exclude variables that start with \"_\"\n",
    "    and var not in exceptions    # Exclude variables in the exceptions list\n",
    "    and isinstance(value, (pl.LazyFrame, pl.DataFrame, pl.Series, alt.Chart, alt.LayerChart, list, int, float, str, np.ndarray, np.int64, np.float32))  # Remove these types only\n",
    "]\n",
    "\n",
    "for var in active_variables:\n",
    "    del globals()[var]\n",
    "del active_variables, exceptions, var\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing a Basic Decile Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that there is a lot of variability in purchase frequency and spend, be it at the product or category level. A useful way of expressing the variability is via a decile analysis. As the name suggests, this sees us **dividing the customer base into 10 equally sized groups and then summarising each group’s buying behaviour**. We will consider two versions of this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decile analysis of category buying behaviour - Decile based on category buyers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first focus on creating the table below. The table captures the decile analysis of category buying behaviour (where each decile equals 10% category buyers). Recall that 4574 households made at least one category purchase in year 1. Having **sorted these households by total category spend**, we create **10 equally sized groups**. Decile 1 is the 10% of households that spent the most in the category during year 1, decile 2 is the next largest 10% of spenders, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decile cuts in the group_spend data using the sorted and ranked data\n",
    "\n",
    "# Part 1: Break-tie ranking of total category spend data\n",
    "# Part 2: Decile cuts in ranking\n",
    "buyer_decile = spend_summary(\n",
    "    'Category', grocery_lf, 1\n",
    ").with_columns(\n",
    "    pl.col('spend').rank(method='ordinal', descending=True).alias('rank'),\n",
    ").with_columns(\n",
    "    (np.floor(10 * (pl.col('rank') - 1) / pl.col('rank').max()) + 1).cast(pl.UInt16).alias('decile')\n",
    ").sort('decile', 'rank').collect()\n",
    "\n",
    "buyer_decile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def calculate_percentage(series):\n",
    "    return series / series.sum()\n",
    "\n",
    "def group_aggregate(data, group_col, agg_col, agg_expr):\n",
    "    return data.group_by(group_col).agg(agg_expr(pl.col(agg_col)))\n",
    "\n",
    "# Decile table creation\n",
    "decile_tab = buyer_decile.select(\"decile\").unique().sort(\"decile\").rename({\"decile\": \"Decile\"})\n",
    "\n",
    "# Populate decile statistics\n",
    "# Decile counts and % HH\n",
    "decile_counts = group_aggregate(buyer_decile, 'decile', 'panel_id', pl.Expr.count)\n",
    "decile_tab.insert_column(1, pl.lit(calculate_percentage(decile_counts['panel_id'])).alias('% HH')) \n",
    "\n",
    "# % Spend\n",
    "decile_spend = group_aggregate(buyer_decile, 'decile', 'spend', pl.Expr.sum)\n",
    "decile_tab.insert_column(2, pl.lit(calculate_percentage(decile_spend['spend'])).alias('% Spend')) \n",
    "\n",
    "# % Transactions\n",
    "group_trans = trans_summary('Category', grocery_lf, 1).collect().join(\n",
    "    other=buyer_decile.group_by('panel_id').agg(pl.col('decile').sum()),\n",
    "    on='panel_id'\n",
    ").sort('decile')\n",
    "decile_trans = group_aggregate(group_trans, 'decile', '# of Purchases', pl.Expr.sum)\n",
    "decile_tab.insert_column(3, pl.lit(calculate_percentage(decile_trans['# of Purchases'])).alias('% Trans')) \n",
    "\n",
    "# Spend/HH, Cat Trans/HH, Avg Spend/Trans (AOV)\n",
    "decile_tab.insert_column(4, pl.lit(decile_spend['spend'] / decile_counts['panel_id']).alias('Spend/HH'))\n",
    "decile_tab.insert_column(5, pl.lit(decile_trans['# of Purchases'] / decile_counts['panel_id']).alias('Cat Trans/HH'))\n",
    "decile_tab.insert_column(6, pl.lit(decile_spend['spend'] / decile_trans['# of Purchases']).alias('Avg Spend/Trans'))\n",
    "\n",
    "# Unique Brands Purchased\n",
    "unique_brands = group_aggregate(group_trans, 'decile', 'Brands Purchased', pl.Expr.mean)\n",
    "decile_tab.insert_column(7, pl.lit(unique_brands['Brands Purchased']).alias('# Unique Brands'))\n",
    "\n",
    "(\n",
    "    GT(decile_tab, rowname_col='Decile')\n",
    "    .tab_header(title='Decile Analysis of Category Buying Behavior',\n",
    "                subtitle= 'Each decile equals 10% of category buyers')\n",
    "    .tab_stubhead(label=\"Decile\")\n",
    "    .fmt_percent(columns=['% HH', '% Spend', '% Trans'], decimals=1)\n",
    "    .fmt_number(columns=['Cat Trans/HH', '# Unique Brands'])\n",
    "    .fmt_currency(columns=['Spend/HH','Avg Spend/Trans'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading across the row associated with Decile 1, we see that they accounted for 28% of category spend and 24% of total transactions. On average, they spent $44.65 in the category across an average of 10.6 purchase occasions. This corresponds to an average category spend per category transaction of $4.21. On average, these households purchased 1.8 different\n",
    "brands during the year. Contrast this to the 10% of category buyers that spent the least in the category. They account for 2% of category spend and transactions, making on average one category purchase. And so on.\n",
    "\n",
    "This is a very basic decile table. Some additional information that could be reported includes the *average number of units purchased per transaction* and the *average number of unique SKUs purchased in the year*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decile analysis of category buying behaviour - Decile based on category spend/revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decile analysis we have just completed uses deciles that represent 10% of the category buyers. An alternative approach is to create deciles that represent **10% of category spend**. In other words, the decile analysis took the entire customer base that purchased the category in year 1 and split the base into ten equal group. In this analysis, we will take the total customer spend or total revenue and split that revenue into ten equal groups. The only change to what we have done above is how we create the decile variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the total revenue into 10 equal parts and generate the decile bins\n",
    "spend_total = spend_summary('Category', grocery_lf, 1).select(pl.col('spend')).sum().collect().to_series()[0]\n",
    "breaks = np.arange(0, spend_total, spend_total/10)\n",
    "labels = [f'{i}' for i in range(0, 11, 1)]\n",
    "\n",
    "spend_decile = spend_summary(\n",
    "    'Category', grocery_lf, 1\n",
    ").sort(\n",
    "    'spend', descending=True\n",
    ").with_columns(\n",
    "    pl.col('spend').cum_sum().alias('cum sum') # Create a cumulative sum column for category spend (category revenue)\n",
    ").with_columns(\n",
    "    pl.col('cum sum').cut( # Cut function determines the decile range for each cumulative sum value\n",
    "        breaks=breaks,\n",
    "        labels=labels # Function also adds the decile label from 1 to 10\n",
    "    ).cast(pl.UInt16).alias('decile')\n",
    ").sort('decile').collect()\n",
    "\n",
    "spend_decile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def calculate_percentage(series):\n",
    "    return series / series.sum()\n",
    "\n",
    "def group_aggregate(data, group_col, agg_col, agg_expr):\n",
    "    return data.group_by(group_col).agg(agg_expr(pl.col(agg_col)))\n",
    "\n",
    "# Decile table creation\n",
    "decile_tab_spend = spend_decile.select(\"decile\").unique().sort(\"decile\").rename({\"decile\": \"Decile\"})\n",
    "\n",
    "# Populate decile statistics\n",
    "# Decile counts and % HH\n",
    "decile_counts = group_aggregate(spend_decile, 'decile', 'panel_id', pl.Expr.count)\n",
    "decile_tab_spend.insert_column(1, pl.lit(calculate_percentage(decile_counts['panel_id'])).alias('% HH')) \n",
    "\n",
    "# % Spend\n",
    "decile_spend = group_aggregate(spend_decile, 'decile', 'spend', pl.Expr.sum)\n",
    "decile_tab_spend.insert_column(2, pl.lit(calculate_percentage(decile_spend['spend'])).alias('% Spend')) \n",
    "\n",
    "# % Transactions\n",
    "group_trans = trans_summary('Category', grocery_lf, 1).collect().join(\n",
    "    other=spend_decile.group_by('panel_id').agg(pl.col('decile').sum()),\n",
    "    on='panel_id'\n",
    ").sort('decile')\n",
    "decile_trans = group_aggregate(group_trans, 'decile', '# of Purchases', pl.Expr.sum)\n",
    "decile_tab_spend.insert_column(3, pl.lit(calculate_percentage(decile_trans['# of Purchases'])).alias('% Trans')) \n",
    "\n",
    "# Spend/HH, Cat Trans/HH, Avg Spend/Trans (AOV)\n",
    "decile_tab_spend.insert_column(4, pl.lit(decile_spend['spend'] / decile_counts['panel_id']).alias('Spend/HH'))\n",
    "decile_tab_spend.insert_column(5, pl.lit(decile_trans['# of Purchases'] / decile_counts['panel_id']).alias('Cat Trans/HH'))\n",
    "decile_tab_spend.insert_column(6, pl.lit(decile_spend['spend'] / decile_trans['# of Purchases']).alias('Avg Spend/Trans'))\n",
    "\n",
    "# Unique Brands Purchased\n",
    "unique_brands = group_aggregate(group_trans, 'decile', 'Brands Purchased', pl.Expr.mean)\n",
    "decile_tab_spend.insert_column(7, pl.lit(unique_brands['Brands Purchased']).alias('# Unique Brands'))\n",
    "\n",
    "(\n",
    "    GT(decile_tab_spend, rowname_col='Decile')\n",
    "    .tab_header(title='Decile Analysis of Category Buying Behavior',\n",
    "                subtitle= 'Each decile equals 10% of category spend/revenue')\n",
    "    .tab_stubhead(label=\"Decile\")\n",
    "    .fmt_percent(columns=['% HH', '% Spend', '% Trans'], decimals=1)\n",
    "    .fmt_number(columns=['Cat Trans/HH', '# Unique Brands'])\n",
    "    .fmt_currency(columns=['Spend/HH','Avg Spend/Trans'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading across the row associated with Decile 1, we see that the top 2% of households accounted for 10% of category spend. On average, they spent \\$63.77 in the category, across an average of 13.4 purchase occasions. This corresponds to an average category spend per category transaction of \\$4.74. On average, these households purchased 1.9 different brands during the year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lorenz Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lorenz Curves - Transactions/Purchasing - Discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Lorenz curve is a common graphical tool for visualising “concentration” or “inequality” in the distribution of a quantity of interest (e.g., income, buying behaviour). It show the proportion of the overall quantity (e.g., number of transactions, spend) associated with the bottom x% of the unit of observation associated with the distribution (e.g., households). When analysing buyer behaviour, this sees us lining up all customers in ascending order of their level of purchasing and computing the share of total purchasing\n",
    "accounted by each person. The Lorenz curve is created by plotting the cumulative percentage of customers (x-axis) against the cumulative percentage of total purchasing (y-axis).\n",
    "\n",
    "The Lorenz curve for transactions associated with Alpha is computed and plotted below and interpreted in the following manner. We see that that x = 80% roughly corresponds to y = 54%, which means the 80% of the buyers of Alpha (when sorted from least to most frequent buyers) account for 54% of all the buying of Alpha in year 1. This implies the top 20% of buyers (in terms of purchase frequency) account for 46% of total purchases. The “rule” of 80/20 does not hold here; rather, it is 46/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are only interested in those panellists that made at least one purchase of Alpha in year 1.\n",
    "# Alpha Brand Transaction Distribution of Transactions Greater Than 0\n",
    "alpha_trans_dist = (\n",
    "    trans_summary('Alpha', grocery_lf, 1)\n",
    "    .filter(pl.col('# of Purchases') > 0)\n",
    "    .group_by('# of Purchases')\n",
    "    # Frequency of the Grouped Transaction Count\n",
    "    .agg(pl.col('# of Purchases').count().alias('Frequency'))\n",
    "    .sort('# of Purchases')\n",
    "    # Add a '% of Buyers' column\n",
    "    .with_columns((pl.col('Frequency')/pl.col('Frequency').sum()).alias('% of Buyers'))\n",
    "    # Cumulative % of Buyers\n",
    "    .with_columns((pl.col('% of Buyers').cum_sum()).alias('Cum % of Buyers'))\n",
    "    # Total number of transactions made by those customers who made one purchase of Alpha\n",
    "    # Total Transactions = frequency x # of transactions\n",
    "    .with_columns((pl.col('Frequency')*pl.col('# of Purchases')).alias('Total Transactions'))\n",
    "    # Add a '% of Transactions' column\n",
    "    .with_columns((pl.col('Total Transactions')/pl.col('Total Transactions').sum()).alias('% Transactions'))\n",
    "    # Cumulative % of Transactions\n",
    "    .with_columns((pl.col('% Transactions').cum_sum()).alias('Cum % Transactions'))\n",
    ")\n",
    "\n",
    "(\n",
    "    GT(alpha_trans_dist.collect(), rowname_col='# of Purchases')\n",
    "    .tab_header(title='Alpha Buyers & Transaction Distribution',\n",
    "                subtitle= 'Lorenz Curve Table')\n",
    "    .tab_stubhead(label=\"# of Purchases\")\n",
    "    .fmt_percent(columns=['% of Buyers', 'Cum % of Buyers', '% Transactions', 'Cum % Transactions'], decimals=1)\n",
    "    .fmt_integer(columns=['Frequency', 'Total Transactions'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are only interested in those panellists that made at least one purchase of Alpha in year 1.\n",
    "- We compute the total number of transactions made by those customers who made one purchase of Alpha: this is obviously the number of transactions times the frequency of the transactions. We see that the 517 households that purchased Alpha twice made a total of 1034 transactions.\n",
    "- What percentage of the total number of buyers of Alpha are the 733 households that made one purchase? The 517 households that made two purchases? What percentage of total purchasing is the 733 purchases made by those households that made one purchase? The 1034 purchases by made by those households that made two purchases? We first compute the total number of Alpha buyers and the total amount of purchasing by this group, and then compute the associated percentages. We see that **28% of Alpha buyers made just one purchase and their purchasing accounted for 8% of all Alpha purchasing**. Similarly, we see that **20% of buyers made two purchases and their purchasing accounted for 11% of all Alpha purchasing**.\n",
    "- We compute the cumulative percentage of both `% of buyers` and `% of transactions` quantities We see, for example, that 48% of all buyers made two or fewer purchases and accounted for 20% of all the purchases of Alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_row = pl.DataFrame({\n",
    "    'Cum % of Buyers': [0.0],\n",
    "    'Cum % Transactions': [0.0]\n",
    "})\n",
    "\n",
    "alpha_lorenz_plot = alpha_trans_dist.select('Cum % of Buyers' ,'Cum % Transactions').collect()\n",
    "alpha_lorenz_plot_zero = pl.concat([zero_row, alpha_lorenz_plot])\n",
    "\n",
    "chart = alt.Chart(alpha_lorenz_plot_zero).mark_line().encode(\n",
    "    x=alt.X('Cum % of Buyers:Q', title='Cumulative % Alpha Buyers', axis=alt.Axis(format='.0%')),\n",
    "    y=alt.Y('Cum % Transactions:Q', \n",
    "            title='Cumulative % Alpha Transactions', \n",
    "            axis=alt.Axis(format='.0%'),\n",
    "            scale=alt.Scale(domain=[0, 1])),\n",
    ").properties(\n",
    "    title=\"Lorenz Curve for Alpha Transactions\",\n",
    "    width=550,\n",
    "    height=450\n",
    ")\n",
    "\n",
    "chart.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted above, we see that x = 80% roughly corresponds to y = 54%. We can compute the exact number by interpolation (linear) in the following manner:\n",
    "$$\\frac{y-y_{0}}{x-x_{0}}=\\frac{y_{1}-y_{0}}{x_{1}-x_{0}}$$\n",
    "$$(y-y_{0})=\\frac{(y_{1}-y_{0})}{(x_{1}-x_{0})}(x-x_{0})$$\n",
    "\n",
    "<div style=\"max-width:200px;margin-left: auto; margin-right: auto;\">\n",
    "<img src=\"references/linear_interp1.png\" width=\"200\"/>\n",
    "</div>\n",
    "\n",
    "<div style=\"max-width:200px;margin-left: auto; margin-right: auto;\">\n",
    "<img src=\"references/linear_interp.png\" width=\"200\"/>\n",
    "</div>\n",
    "\n",
    "- We see that 73.4% ($x_{0}$) of buyers accounted for 45.0% ($y_{0}$) of purchases, and that 82.1% ($x_{1}$) of buyers accounted for 57.5% ($y_{1}$) of purchases. Therefore, when $x$ = 80%, $y$ can be solved in the following way:\n",
    "- $$0.45 + (0.8 - 0.734)\\frac{(0.575 - 0.45)}{(0.821 - 0.734)} = 0.5448$$\n",
    "- we see that 80% falls 76% of the way between 73.4% and 82.1%. Therefore, the associated percentage of total purchases will lie 76% of the way between 45.0% and 57.5%.\n",
    "- As the “bottom” 80% account for 54% of total purchasing, it follows that the “top” 20% account for 46% of total purchasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy linear interpolation function simplifies the computation, return the interpolated y value\n",
    "np.interp(0.8, xp=alpha_lorenz_plot['Cum % of Buyers'], fp=alpha_lorenz_plot['Cum % Transactions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A related quantity of interest is the percentage of buyers that account for half of total purchasing. This can be read off the Lorenz curve in the following manner.\n",
    "\n",
    "- We see that 73.4% of purchasers account for 45.0% of total purchases, and that 82.1% of purchasers account for 57.5% of total purchases.\n",
    "- We can apply the same linear interpolation function but instead of solving for the $y$, we solve for $x$.\n",
    "- $$(x-x_{0})=\\frac{(x_{1}-x_{0})}{(y_{1}-y_{0})}(y-y_{0})$$\n",
    "- $$\\frac{(0.821-0.734)}{(0.575-0.45)}(0.5-0.45) + 0.734 = 0.769$$\n",
    "- We see that 50% lies 40% of the way between 44.0% and 57.5%. Therefore, the associated percentage of purchasers lies 40% of the way between 73.4% and 82.1%, we see this is 76.9%.\n",
    "- As the “bottom” 77% of buyers account for half of total purchasing, it follows that the “top” 23% also account for half of total purchasing, which we can write as 50/23. This quantity is easy for most people grasp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reversing the axis returns the interpolated x value\n",
    "np.interp(0.5, xp=alpha_lorenz_plot['Cum % Transactions'], fp=alpha_lorenz_plot['Cum % of Buyers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lorenz Curves - Spend - Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purchase frequency is a discrete quantity and we created the Lorenz curve off the distribution of transactions. We now consider how to create a Lorenz curve when the quantity of interest is continuous. We will focus on creating the Lorenz curve for the spend associated with Alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_spend_dist = (\n",
    "    spend_summary('Alpha', grocery_lf, 1)\n",
    "    .filter(pl.col('spend') > 0)\n",
    "    # Sort Alpha customer from lowest to highest\n",
    "    .sort(pl.col('spend'), descending=False)\n",
    "    # Compute spend as a % of total Alpha brand spend in year 1\n",
    "    # percentage of total Alpha purchasing accounted for by each panellist\n",
    "    .with_columns((pl.col('spend')/pl.col('spend').sum()).alias('% Spend'))\n",
    "    # Compute the cumulative percentage of spend numbers\n",
    "    .with_columns((pl.col('% Spend').cum_sum()).cast(pl.Float64).alias('Cum % Spend'))\n",
    ").collect()\n",
    "\n",
    "# Compute the cumulative percentage of buyers number\n",
    "spend_count = alpha_spend_dist.height\n",
    "alpha_spend_dist = alpha_spend_dist.with_columns(\n",
    "    pl.lit(np.linspace(1 / spend_count, 1, spend_count)).alias('Cum % Cust')\n",
    ")\n",
    "\n",
    "alpha_spend_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_row = pl.DataFrame({\n",
    "    'Cum % Spend': [0.0],\n",
    "    'Cum % Cust': [0.0]\n",
    "})\n",
    "\n",
    "alpha_lorenz_plot = alpha_spend_dist.select('Cum % Spend' ,'Cum % Cust')\n",
    "alpha_lorenz_plot_zero = pl.concat([zero_row, alpha_lorenz_plot])\n",
    "\n",
    "chart = alt.Chart(alpha_lorenz_plot_zero).mark_line().encode(\n",
    "    x=alt.X('Cum % Cust:Q', title='Cumulative % Alpha Buyers', axis=alt.Axis(format='.0%')),\n",
    "    y=alt.Y('Cum % Spend:Q', \n",
    "            title='Cumulative % Alpha Revenue', \n",
    "            axis=alt.Axis(format='.0%'),\n",
    "            scale=alt.Scale(domain=[0, 1])),\n",
    ").properties(\n",
    "    title=\"Lorenz Curve for Alpha Spend\",\n",
    "    width=550,\n",
    "    height=450\n",
    ")\n",
    "\n",
    "chart.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at `alpha_spend_dist`, we see that the \"bottom\" 80% of Alpha buyers account for 50.8% of its revenue in year 1, which is equivalent to 49/20. This is higher than the 46/20 we observed for transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.interp(0.8, xp=alpha_lorenz_plot['Cum % Cust'], fp=alpha_lorenz_plot['Cum % Spend'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Garbage Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions = ['grocery_lf', 'sku_lf', 'kiwi_lf', 'In', 'exceptions', 'active_variables',\n",
    "              'penetration', 'ppb', 'panel_size']\n",
    "\n",
    "active_variables = [\n",
    "    var for var, value in globals().items()\n",
    "    if not var.startswith('_')   # Exclude variables that start with \"_\"\n",
    "    and var not in exceptions    # Exclude variables in the exceptions list\n",
    "    and isinstance(value, (pl.LazyFrame, pl.DataFrame, pl.Series, alt.Chart, alt.LayerChart, list, int, float, str, np.ndarray, np.int64, np.float32))  # Remove these types only\n",
    "]\n",
    "\n",
    "for var in active_variables:\n",
    "    del globals()[var]\n",
    "del active_variables, exceptions, var\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#956bbf\">Exploring Multibrand Buying Behaviour</span>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have explored purchasing at the level of the individual brand or overall category. We now step back and consider panellists’ purchasing of multiple brands (in a given time period).\n",
    "\n",
    "To set the scene, let us first determine the percentage of category buyers that bought 1, 2, 3, 4, or 5 different brands in the course of year 1. (Note that we only have five brands in the dataset, with Other being an aggregation of several very small share brands)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_purchases = (\n",
    "    trans_summary('Category', grocery_lf, 1)\n",
    "    .group_by('Brands Purchased')\n",
    "    .agg(pl.col('panel_id').count().alias('Category Buyer'))\n",
    "    .with_columns((pl.col('Category Buyer') / pl.col('Category Buyer').sum()).alias('% Category Buyers'))\n",
    "    .sort('Brands Purchased')\n",
    " ).collect()\n",
    "\n",
    "(\n",
    "    GT(brand_purchases, rowname_col='Brands Purchased')\n",
    "    .tab_header(title='Category Buyers That Bought Different Brands',\n",
    "                subtitle='Distribution of the number of separate brands purchased by category buyers in year 1')\n",
    "    .tab_stubhead(label=\"Brands Purchased\")\n",
    "    .fmt_integer(columns=['Category Buyer'])\n",
    "    .fmt_percent(columns=['% Category Buyers'], decimals=0)\n",
    "    .data_color(\n",
    "        columns=['% Category Buyers'],\n",
    "        domain=[0, 0.7],\n",
    "        palette=['white', 'rebeccapurple']\n",
    "    ) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that over two-thirds of the category buyers only ever bought one brand. This is despite the fact that 86% of category buyers made two or more category purchases in the course of the year. No household purchased all five brands that year.\n",
    "\n",
    "Note: We know from analyzing the distribution of category purchase frequency that 8.9% of the panel made zero category purchases and 13.2% made one category purchase. 1 − 0.132/(1 − 0.089) = 0.855, which means 86% of year 1 category buyers made more than one category purchase that year.\n",
    "\n",
    "Can also be computed as: (1 - 0.132 - 0.089)/(1 - 0.089) [% of panellists that more than 1 purchase] / [% of category buyers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How does the number of different brands purchased in the year vary as a function of the number of category purchases made during the year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of category purchases made by each household and number of brands purchased by each household\n",
    "brand_loyalty = (\n",
    "    trans_summary('Category', grocery_lf, 1)\n",
    "    .group_by('# of Purchases', 'Brands Purchased')\n",
    "    .agg(pl.col('panel_id').count().alias('Count'))\n",
    "    .collect()\n",
    "    .pivot(\n",
    "        on='Brands Purchased',\n",
    "        index='# of Purchases',\n",
    "        values='Count'\n",
    "    ).sort(pl.col('# of Purchases'))\n",
    "    .with_columns(pl.col('*'))\n",
    "    .select('# of Purchases', '1', '2', '3', '4')\n",
    ")\n",
    "\n",
    "(\n",
    "    GT(brand_loyalty, rowname_col='# of Purchases')\n",
    "    .tab_header(title='# of Purchase Occasions Vs. Brands Purchased ',\n",
    "                subtitle='Brands purchased as function of the number of category purchases made in year 1')\n",
    "    .tab_stubhead(label=\"# of Purchases\")\n",
    "    .sub_missing(missing_text=\"\")\n",
    "    .fmt_integer()\n",
    "    .data_color(\n",
    "        domain=[0, 700],\n",
    "        palette=['white', 'rebeccapurple'],\n",
    "        na_color='white'\n",
    "    ) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the resulting table output, we see that there is quite a high level of sole-brand loyalty (i.e., only buying one brand), even as the number of category purchases increases. We have two panellists that made 20 category purchases, all with the same brand. That’s some level of\n",
    "loyalty!\n",
    "\n",
    "We can compute the (weighted) average number of different brands purchased for each level of category purchasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa_brands_purchased = (\n",
    "    trans_summary('Category', grocery_lf, 1)\n",
    "    .group_by('# of Purchases', 'Brands Purchased')\n",
    "    .agg(pl.col('panel_id').count().alias('Count'))\n",
    "    .with_columns((pl.col('Brands Purchased') * pl.col('Count')).alias('Product'))\n",
    "    .group_by('# of Purchases')\n",
    "    .agg(\n",
    "        (pl.col('Product').sum() / pl.col('Count').sum()) .alias('Weighted Average')\n",
    "    ).sort(by='# of Purchases')\n",
    ").collect()\n",
    "\n",
    "(\n",
    "    GT(wa_brands_purchased, rowname_col='# of Purchases')\n",
    "    .tab_header(title='W.A. # of Different Brands Purchased')\n",
    "    .tab_stubhead(label=\"# of Purchases\")\n",
    "    .fmt_number()\n",
    "    .data_color(\n",
    "        domain=[1, 4],\n",
    "        palette=['white', 'rebeccapurple']\n",
    "    )\n",
    "    .cols_label({'Weighted Average': 'Weighted Average Brands'}) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the average number of brands purchased does increase as a function of category purchasing. This should not come as a surprise, as more category purchases equals more opportunities to buy different brands. We saw in earlier that two brands (Alpha and Bravo) had a combined value market share of 86%. As such, the relatively low number of different brands purchased in the year is not too surprising.\n",
    "\n",
    "We now consider three common analyses designed to give insight into the nature of multibrand buying behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplication of Purchase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know from our analysis of penetration and PPB earlier that 52% of households purchased Alpha and 51% of households purchased Bravo. What percentage of Alpha buyers also purchased Bravo during the year (and vice-versa)? The answer to these (and similar) questions is provided by a duplication of purchase analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before performing this analysis on our dataset, let us consider a toy example. The following table summarizes the purchasing of four brands by six households.\n",
    "\n",
    "|      | A | B | C | D |\n",
    "|------|---|---|---|---|\n",
    "| HH01 | **1** | 0 | **2** | 0 |\n",
    "| HH02 | 0 | 1 | 0 | 0 |\n",
    "| HH03 | **1** | **3** | 0 | 0 |\n",
    "| HH04 | 0 | 0 | 1 | 4 |\n",
    "| HH05 | **1** | **1** | 0 | **1** |\n",
    "| HH06 | 0 | 0 | 0 | 1 |\n",
    "\n",
    "We see that three households made at least one purchase of brand A, three households made at least one purchase of brand B, and so on. How many brand A buyers also purchased brand B? Two (HH03 and HH05). How many brand A buyers also purchased brand C? One (HH01). Repeating this for all brands gives us the following table, which we will call a **duplication count table**.\n",
    "\n",
    "|   | A | B | C | D |\n",
    "|---|---|---|---|---|\n",
    "| **A** | 3 | 2 | 1 | 1 |\n",
    "| **B** | 2 | 3 | 0 | 1 |\n",
    "| **C** | 1 | 0 | 2 | 1 |\n",
    "| **D** | 1 | 1 | 1 | 3 |\n",
    "\n",
    "For any given row, the number in each cell is the number of buyers of the brand associated with that row that also purchased the brand associated with that column. (The diagonal is obviously the number of buyers of each brand.) Looking at the row for brand A, we see that three households\n",
    "purchased that brand. Two of these households (67%) also made at least one purchase of brand B, and one of these three households (33%) also made at least one purchase of brand C. These row percentages are reported in the following table, which we call the **duplication of purchase table**. (By convention, we leave the diagonal blank.)\n",
    "\n",
    "|   | A   | B   | C   | D   |\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "| **A** |     | 67% | 33% | 33% |\n",
    "| **B** | 67% |     | 0%  | 33% |\n",
    "| **C** | 50% | 0%  |     | 50% |\n",
    "| **D** | 33% | 33% | 33% |     |\n",
    "\n",
    "How can we create this table efficiently when we have a large number of panellists? One approach, which makes use of matrix multiplication, is as follows:\n",
    "- We create that we will call an “**ever buyers**” matrix, which is a matrix of size (number of panellists) × (number of brands), where each cell takes on a value of 1 if the panellist (row) ever purchased the brand (column) in the period of interest; 0 otherwise.\n",
    "\n",
    "|      | A | B | C | D |\n",
    "|------|---|---|---|---|\n",
    "| HH01 | 1 | 0 | 1 | 0 |\n",
    "| HH02 | 0 | 1 | 0 | 0 |\n",
    "| HH03 | 1 | 1 | 0 | 0 |\n",
    "| HH04 | 0 | 0 | 1 | 1 |\n",
    "| HH05 | 1 | 1 | 0 | 1 |\n",
    "| HH06 | 0 | 0 | 0 | 1 |\n",
    "\n",
    "- Pre-multiplying the “ever buyers” matrix by its own transpose gives us the **duplication count table** we created above $A^{T}\\cdot A=B$. (Evaluate the example below.)\n",
    "  \n",
    "Transposed \"ever buyer\" matrix:\n",
    "\n",
    "|   | HH01 | HH02 | HH03 | HH04 | HH05 | HH06 |\n",
    "|:---:|:----:|:----:|:----:|:----:|:----:|:----:|\n",
    "| A | 1    | 0    | 1    | 0    | 1    | 0    |\n",
    "| B | 0    | 1    | 1    | 0    | 1    | 0    |\n",
    "| C | 1    | 0    | 0    | 1    | 0    | 0    |\n",
    "| D | 0    | 0    | 0    | 1    | 1    | 1    |\n",
    "\n",
    "Matrix Multiply By:\n",
    "|      | A | B | C | D |\n",
    "|------|---|---|---|---|\n",
    "| HH01 | 1 | 0 | 1 | 0 |\n",
    "| HH02 | 0 | 1 | 0 | 0 |\n",
    "| HH03 | 1 | 1 | 0 | 0 |\n",
    "| HH04 | 0 | 0 | 1 | 1 |\n",
    "| HH05 | 1 | 1 | 0 | 1 |\n",
    "| HH06 | 0 | 0 | 0 | 1 |\n",
    "\n",
    "- The 'ever buyer' matrix effectively creates a binary of 1 (Yes) or 0 (No) to factor in the required inclusions or exclusions of intersection between buyers of different brands. In this case, as we multiply the binary matrix and its transposed form together, we are computing a matrix that records the count of existing intersection of buyers of a brand who also purchased other brands. \n",
    "- Dividing each cell by the number of buyers of the brand associated with each row gives us the **duplication of purchase table**.\n",
    "\n",
    "[Matrix Multiplication Visualizer](http://matrixmultiplication.xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m x n matrix where m = # of panellists, n = # of brands and value is number of purchases\n",
    "brand_purch = np.array([\n",
    "    [1, 0, 2, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [1, 3, 0, 0],\n",
    "    [0, 0, 1, 4],\n",
    "    [1, 1, 0, 1],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "# m x n matrix where value is 1 if buyer ever purchased, 0 if never purchased\n",
    "ever_buyers = np.where(brand_purch != 0, 1, 0)\n",
    "\n",
    "# n x n square matrix representing count of row/column brand buyers who also purchased column/row brands\n",
    "dup_count = ever_buyers.T @ ever_buyers\n",
    "print('Duplication Count Matrix:')\n",
    "print(dup_count, dup_count.shape)\n",
    "\n",
    "print()\n",
    "# Diagonal vector of dup_count matrix representing the number of buyers of each ROW brand\n",
    "brand_buyers = np.diag(dup_count).reshape(4, 1)\n",
    "print('Number of Buyers of Each Brand:')\n",
    "print(brand_buyers, brand_buyers.shape)\n",
    "\n",
    "# n x n square matrix representing the proportion of brand buyers for each ROW brand that also purchased other COLUMN brands\n",
    "dup_purchase = dup_count / brand_buyers\n",
    "print('Duplication of Purchase Matrix:')\n",
    "print(dup_purchase, dup_purchase.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_purchases = trans_pivot(grocery_lf, 1).select('Alpha', 'Bravo', 'Charlie', 'Delta', 'Other')\n",
    "\n",
    "# Step 1: Create ever buyers matrix\n",
    "ever_buyers = brand_purchases.select(\n",
    "    [pl.when(pl.col(col) > 0).then(1).otherwise(0).alias(col) for col in brand_purchases.columns]\n",
    ").to_numpy()\n",
    "\n",
    "# Step 2: Compute duplication count matrix\n",
    "dup_count = ever_buyers.T @ ever_buyers\n",
    "brand_buyers = np.diag(dup_count).reshape(5, 1)\n",
    "\n",
    "# Step 3: Compute duplication of purchase matrix\n",
    "dup_purchase = dup_count / brand_buyers\n",
    "\n",
    "# DataFrame conversion for tabular view\n",
    "dup_count_df = (\n",
    "    pl.from_numpy(dup_count, schema=brand_purchases.columns)\n",
    "    .with_columns(pl.Series('Brands', brand_purchases.columns))\n",
    ")\n",
    "\n",
    "dup_purchase_df = (\n",
    "    pl.from_numpy(dup_purchase, schema=brand_purchases.columns)\n",
    "    .with_columns(pl.Series('Brands', brand_purchases.columns))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    GT(dup_count_df, rowname_col='Brands')\n",
    "    .tab_header(title='Duplication Count Matrix',\n",
    "                subtitle='Count of buyers who purchased row AND column brands')\n",
    "    .tab_stubhead(label=\"Brands\")\n",
    "    .sub_missing(missing_text=\"\")\n",
    "    .fmt_integer()\n",
    "    .data_color(\n",
    "        domain=[0, 900],\n",
    "        palette=['white', 'rebeccapurple'],\n",
    "    ) \n",
    "    .tab_style(\n",
    "        style=[\n",
    "            style.text(color=\"white\"),\n",
    "            style.fill('lightgrey'),\n",
    "            \n",
    "        ],\n",
    "        locations=[\n",
    "            loc.body(['Alpha'],['Alpha']), \n",
    "            loc.body(['Bravo'],['Bravo']),\n",
    "            loc.body(['Charlie'],['Charlie']),\n",
    "            loc.body(['Delta'],['Delta']),\n",
    "            loc.body(['Other'],['Other'])\n",
    "        ]\n",
    "    )\n",
    "    .tab_source_note(md(\"\"\"**Read Row/Column as**: _# of People who Purchased [Brand] / Who also Purchased [Brand]_\"\"\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    GT(dup_purchase_df, rowname_col='Brands')\n",
    "    .tab_header(title='Duplication of Purchase Matrix',\n",
    "                subtitle='% of brand buyers that also purchased other brands')\n",
    "    .tab_stubhead(label=\"Brands\")\n",
    "    .sub_missing(missing_text=\"\")\n",
    "    .fmt_percent(decimals=0)\n",
    "    .data_color(\n",
    "        domain=[0, 0.7],\n",
    "        palette=['white', 'rebeccapurple'],\n",
    "    ) \n",
    "    .tab_style(\n",
    "        style=[\n",
    "            style.text(color=\"white\"),\n",
    "            style.fill('white'),\n",
    "            \n",
    "        ],\n",
    "        locations=[\n",
    "            loc.body(0,0),\n",
    "            loc.body(1,1),\n",
    "            loc.body(2,2),\n",
    "            loc.body(3,3),\n",
    "            loc.body(4,4)\n",
    "        ]\n",
    "    )\n",
    "    .tab_source_note(md(\"\"\"**Read Row/Column as**: _% of People who Purchased [Brand] / Who also Purchased [Brand]_\"\"\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The duplication of purchase matrix for year 1 is computed above. We see that 34% of those panellists that purchased Alpha in year 1 also made at least one purchase of Bravo that year. We see that 15% of Alpha buyers also bought Charlie, whereas 50% of Charlie buyers also bought Alpha. This asymmetry is not surprising given the relative size of the two brands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Share of Category Requirements (SCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The duplication of purchase table tells use that 34% of Alpha buyers also purchased Bravo, 15% also purchased Charlie, and so on. This “polygamous purchasing” leads to an obvious question: How “loyal” are they to Alpha? The answer to this question obviously depends on what we mean by “loyal”. One commonly used measure of loyalty is **share of category requirements (SCR)**, which is the percentage of category volume that the brand represents among its buyers.\n",
    "\n",
    "The SCR numbers for each brand are reported below. We see that Alpha has a share of category requirements of 69%. This means that 69% of the *total category volume purchased by buyers of Alpha goes to Alpha*. Contrast this to Delta, which has an SCR of 40%. This means that 40% of the volume purchased in the category by buyers of Delta goes to that brand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of brands\n",
    "brands = ['Alpha', 'Bravo', 'Charlie', 'Delta', 'Other']\n",
    "\n",
    "# Initialize results list\n",
    "scr_results = []\n",
    "\n",
    "with pl.StringCache():\n",
    "    # Compute total category volume\n",
    "    total_vol = vol_summary('Category', grocery_lf, 1)\n",
    "\n",
    "    for brand in brands:\n",
    "        # Join brand-specific volume to total category volume to get total category volume for panelists who purchased the brand\n",
    "        scr_lf = (\n",
    "            total_vol.join(\n",
    "                other=vol_summary(brand, grocery_lf, 1),\n",
    "                on='panel_id',\n",
    "                how='inner'\n",
    "            )\n",
    "            .with_columns(\n",
    "                # Compute total brand and category volumes\n",
    "                # Total purchasing of each brand\n",
    "                (pl.col('volume_right').sum()).alias('Brand Volume'),\n",
    "                # The total amount of category purchasing conditioned on the fact that at least one purchase of the brand of interest was made.\n",
    "                (pl.col('volume').sum()).alias('Category Volume'),          \n",
    "            )\n",
    "            .with_columns(\n",
    "                # Compute SCR - ratio of brand purchasing to category purchasing\n",
    "                (pl.col('Brand Volume').sum() / pl.col('Category Volume').sum()).alias('SCR')          \n",
    "            )\n",
    "            .select('Brand Volume', 'Category Volume', 'SCR')\n",
    "            .collect()\n",
    "        )\n",
    "        \n",
    "        # Append results\n",
    "        scr_results.append({\n",
    "            \"Brands\": brand,\n",
    "            \"Brand Purchasing\": scr_lf.item(1, \"Brand Volume\"),\n",
    "            \"Category Purchasing\": scr_lf.item(1, \"Category Volume\"),\n",
    "            \"SCR\": scr_lf.item(1, \"SCR\")\n",
    "        })\n",
    "\n",
    "# Create final DataFrame\n",
    "scr = pl.DataFrame(scr_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    GT(scr, rowname_col='Brands')\n",
    "    .tab_header(title='Share of Category Requirements (SCR)',\n",
    "                subtitle='% of category volume that the brand represents among its buyers')\n",
    "    .tab_stubhead(label=\"Brands\")\n",
    "    .fmt_integer(columns=['Brand Purchasing', 'Category Purchasing'])\n",
    "    .fmt_percent(columns='SCR', decimals=0)\n",
    "    .data_color(\n",
    "        domain=[0, 0.7],\n",
    "        palette=['white', 'rebeccapurple'],\n",
    "        columns='SCR'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2\n",
    "\n",
    "brand_vol = vol_pivot(grocery_lf, 1).fill_null(0)\n",
    "\n",
    "ever_buyers = brand_vol.select(\n",
    "    [pl.when(pl.col(col) > 0).then(1).otherwise(0).alias(col) for col in brand_vol.columns]\n",
    ").to_numpy()\n",
    "\n",
    "brand_vol = brand_vol.to_numpy()\n",
    "\n",
    "cross_purchasing = ever_buyers.T @ brand_vol\n",
    "brand_purchasing = np.diag(cross_purchasing)\n",
    "category_purchasing = np.sum(cross_purchasing, axis=1)\n",
    "scr = brand_purchasing / category_purchasing\n",
    "scr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Purchasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross Purchasing - *By Volume Purchase*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just seen that 69% of total category volume purchasing by the buyers of Alpha went to that brand. We know from the duplication of purchase analysis that 15% of Alpha buyers also purchased Charlie. How much of their category *volume* purchasing went to Charlie? This is answered via **cross purchase analysis** (sometimes called a **combination purchase analysis**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the logic of the associated calculations, let us revisit the toy problem introduced earlier. We have the following summary of the purchasing of four brands by six households and the associated “ever buyers” matrix. We will assume the \"ever buyers\" matrix reports volume purchasing in kilograms.\n",
    "\n",
    "**Brand Purchasing**\n",
    "|      | A | B | C | D | Total |\n",
    "|:----:|:---:|:---:|:---:|:---:|:-----:|\n",
    "| HH01 | **1** | 0 | **2** | 0 | **3**     |\n",
    "| HH02 | 0 | 1 | 0 | 0 | 1     |\n",
    "| HH03 | **1** | **3** | 0 | 0 | **4**     |\n",
    "| HH04 | 0 | 0 | 1 | 4 | 5     |\n",
    "| HH05 | **1** | **1** | 0 | **1** | **3**     |\n",
    "| HH06 | 0 | 0 | 0 | 1 | 1     |\n",
    "\n",
    "**Ever Buyers Matrix**\n",
    "|      | A | B | C | D |\n",
    "|------|---|---|---|---|\n",
    "| HH01 | 1 | 0 | 1 | 0 |\n",
    "| HH02 | 0 | 1 | 0 | 0 |\n",
    "| HH03 | 1 | 1 | 0 | 0 |\n",
    "| HH04 | 0 | 0 | 1 | 1 |\n",
    "| HH05 | 1 | 1 | 0 | 1 |\n",
    "| HH06 | 0 | 0 | 0 | 1 |\n",
    "\n",
    "We see that buyers of brand A purchased 3 kg (1 + 1 + 1, from A column) of brand A and a total of 10 kg (3 + 4 + 3, from total column) in the category (i.e., SCR = 30%). We see that they also purchased 4 kg (3 + 1, from B column) of brand B, 2 kg (2, from C column) of brand C and 1 kg (1, from D column) of brand D. The associated numbers for all brands are given in the following table:\n",
    "\n",
    "|   | A | B | C | D |\n",
    "|---|---|---|---|---|\n",
    "| **A** | 3 | 4 | 2 | 1 |\n",
    "| **B** | 2 | 5 | 0 | 1 |\n",
    "| **C** | 1 | 0 | 3 | 4 |\n",
    "| **D** | 1 | 1 | 1 | 6 |\n",
    "\n",
    "The sum of the elements of each row gives us the total amount of category purchasing by buyers of the brand of that row. Dividing each row entry by the sum of that row’s elements gives us the following **cross purchasing table**, the *diagonal of which is obviously SCR*.\n",
    "\n",
    "|   | A   | B   | C   | D    |\n",
    "|:---:|:---:|:---:|:---:|:----:|\n",
    "| **A** | 30% | 40% | 20% | 10%  |\n",
    "| **B** | 25% | 63% | 0%  | 13%  |\n",
    "| **C** | 13% | 0%  | 38% | 50%  |\n",
    "| **D** | 11% | 11% | 11% | 67%  |\n",
    "\n",
    "How do we create the table efficiently when we have a large number of panellists? One approach is to pre-multiply the panellist × brand volume purchasing summary table by the transpose of the “ever buyers” matrix $A^{T}\\cdot B$:\n",
    "\n",
    "Transposed \"ever buyer\" matrix:\n",
    "\n",
    "|   | HH01 | HH02 | HH03 | HH04 | HH05 | HH06 |\n",
    "|:---:|:----:|:----:|:----:|:----:|:----:|:----:|\n",
    "| A | 1    | 0    | 1    | 0    | 1    | 0    |\n",
    "| B | 0    | 1    | 1    | 0    | 1    | 0    |\n",
    "| C | 1    | 0    | 0    | 1    | 0    | 0    |\n",
    "| D | 0    | 0    | 0    | 1    | 1    | 1    |\n",
    "\n",
    "Multiplied by Brand Volume Purchasing:\n",
    "\n",
    "|      | A | B | C | D |\n",
    "|:----:|:---:|:---:|:---:|:---:|\n",
    "| HH01 | 1 | 0 | 2 | 0 |\n",
    "| HH02 | 0 | 1 | 0 | 0 |\n",
    "| HH03 | 1 | 3 | 0 | 0 |\n",
    "| HH04 | 0 | 0 | 1 | 4 |\n",
    "| HH05 | 1 | 1 | 0 | 1 |\n",
    "| HH06 | 0 | 0 | 0 | 1 |\n",
    "\n",
    "The 'ever buyer' matrix creates a binary of 1 (buyer) or 0 (non-buyer) to help factor in the inclusions/exclusions and overlaps between brands purchased by all buyers. In this case, as we multiply the transposed form of the binary matrix with the quantity/volume matrix representing the quantity of purchase of different brands by each buyer, we are solving for a matrix that represents the total volume of purchasing for a set of brands by a set of buyers and their purchases of other brands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m x n matrix where m = # of panellists, n = # of brands and value is volume of purchases \n",
    "brand_purch = np.array([\n",
    "    [1, 0, 2, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [1, 3, 0, 0],\n",
    "    [0, 0, 1, 4],\n",
    "    [1, 1, 0, 1],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "# m x n matrix where value is 1 if buyer ever purchased, 0 if never purchased\n",
    "ever_buyers = np.where(brand_purch != 0, 1, 0)\n",
    "\n",
    "# n x n square matrix representing row brand buyers and their volume/unit purchases from row brand and column brands\n",
    "cross_purch_matrix = ever_buyers.T @ brand_purch\n",
    "print('Cross Purchasing Matrix:')\n",
    "print(cross_purch_matrix, cross_purch_matrix.shape)\n",
    "\n",
    "print()\n",
    "# The sum of the elements of each row gives us the total amount of category purchasing (volume) by buyers of the brand of that row\n",
    "category_purch = cross_purch_matrix.sum(axis=1).reshape(4, 1)\n",
    "print('Total Amount of Category Purchasing by Buyers of the Row Brand:')\n",
    "print(brand_buyers, brand_buyers.shape)\n",
    "\n",
    "print()\n",
    "# n x n square matrix representing the proportion of volume purchased by row brand buyers for each COLUMN brands relative to total purchased of the row brand buyer\n",
    "cross_purch = cross_purch_matrix / category_purch\n",
    "print('Cross Purchasing Matrix as a % of Total Volume Purchased of Each Brand:')\n",
    "print(cross_purch, cross_purch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross purchasing analysis for year 1 is computed and reported below. We see that for those panellists that purchased Alpha at least once in year 1, 69% of their category volume purchased went to Alpha, 18% went to Bravo, 8% to Charlie, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_vol_purchases = vol_pivot(grocery_lf, 1).fill_null(0)\n",
    "\n",
    "# Step 1: Create ever buyers matrix\n",
    "ever_buyers = brand_vol_purchases.select(\n",
    "    [pl.when(pl.col(col) > 0).then(1).otherwise(0).alias(col) for col in brand_purchases.columns]\n",
    ").to_numpy()\n",
    "\n",
    "# Step 2: Compute cross purchase matrix\n",
    "cross_purch_matrix = ever_buyers.T @ brand_vol_purchases\n",
    "total_volume_purchases = np.sum(cross_purch_matrix, axis=1).reshape(5, 1)\n",
    "\n",
    "# Step 3: Compute cross purchase matrix as a % of total volume purchased of each brand\n",
    "cross_purch = cross_purch_matrix / total_volume_purchases\n",
    "\n",
    "# DataFrame conversion for tabular view\n",
    "cross_purch_matrix_df = (\n",
    "    pl.from_numpy(cross_purch_matrix, schema=brand_vol_purchases.columns)\n",
    "    .with_columns(pl.Series('Brands', brand_vol_purchases.columns))\n",
    ")\n",
    "\n",
    "cross_purch_df = (\n",
    "    pl.from_numpy(cross_purch, schema=brand_vol_purchases.columns)\n",
    "    .with_columns(pl.Series('Brands', brand_vol_purchases.columns))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    GT(cross_purch_matrix_df, rowname_col='Brands')\n",
    "    .tab_header(title='Cross Purchasing Analysis',\n",
    "                subtitle='Total Volume of Category Purchasing by Buyers of the Brand (by row)')\n",
    "    .tab_stubhead(label=\"Brands\")\n",
    "    .sub_missing(missing_text=\"\")\n",
    "    .fmt_integer()\n",
    "    .data_color(\n",
    "        domain=[0, 3000],\n",
    "        palette=['white', 'rebeccapurple'],\n",
    "    ) \n",
    "    .tab_style(\n",
    "        style=[\n",
    "            style.text(color=\"white\"),\n",
    "            style.fill('lightgrey'),\n",
    "            \n",
    "        ],\n",
    "        locations=[\n",
    "            loc.body(0,0),\n",
    "            loc.body(1,1),\n",
    "            loc.body(2,2),\n",
    "            loc.body(3,3),\n",
    "            loc.body(4,4)\n",
    "        ]\n",
    "    )\n",
    "    .tab_source_note(md(\"\"\"**Read Row/Column as**: _Purchasers of [Brands] / Total Volume Purchased of [Brands]_\"\"\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    GT(cross_purch_df, rowname_col='Brands')\n",
    "    .tab_header(title='Cross Purchasing Analysis',\n",
    "                subtitle='% of Total Volume of Category Purchasing by Buyers of the Brand (by row)')\n",
    "    .tab_stubhead(label=\"Brands\")\n",
    "    .sub_missing(missing_text=\"\")\n",
    "    .fmt_percent(decimals=0)\n",
    "    .data_color(\n",
    "        domain=[0, 0.7],\n",
    "        palette=['white', 'rebeccapurple'],\n",
    "    ) \n",
    "    .tab_source_note(md(\"\"\"**Read Row/Column as**: _Purchasers of [Brands] / % Total Volume Purchased of [Brands]_\"\"\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importance of Competition Plot - *By Volume Purchase*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given brand, we can plot the associated row entries as a pie chart, for example, see the *Importance of competition plot*, which plots the percentage of total volume of category purchasing by buyers of Alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_purch_plot = (\n",
    "    pl.from_numpy(cross_purch.T, schema=brand_vol_purchases.columns, orient='row')\n",
    "    .with_columns(pl.Series('Brands', brand_vol_purchases.columns))\n",
    ")\n",
    "\n",
    "base = alt.Chart(cross_purch_plot).encode(\n",
    "    alt.Theta(\"Alpha:Q\").stack(True),\n",
    "    alt.Color(\"Brands:N\")\n",
    ").properties(\n",
    "    width=600,\n",
    "    title={'text': 'Importance of Competition to Buyers of Alpha',\n",
    "           'subtitle': '% of Total Volume of Category Purchasing by Buyers of Alpha'},\n",
    ")\n",
    "\n",
    "pie = base.mark_arc(outerRadius=145)\n",
    "text = base.mark_text(radius=165, size=15).encode(\n",
    "    text=alt.Text(\"Alpha:N\", format=\".0%\"))\n",
    "\n",
    "pie + text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importance Against Expectation - *By Volume Purchase*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously noted when we computed SCR, we see that Alpha accounts for 69% of category purchasing by the buyers of Alpha. We see from the cross purchase analysis see that Bravo accounts for 18% of their category purchasing. Is this large or small? One way of answering this question is to compare actual purchasing against expectation given general purchasing patterns in the category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Share of Category Purchasing - Alpha Brand Selected\n",
    "share_of_cat_purch = cross_purch[0]\n",
    "\n",
    "# Market Share - Volume\n",
    "market_share = (\n",
    "    brand_vol_purchases.sum() / # sum all columns -> returns a single row dataframe\n",
    "    brand_vol_purchases.sum_horizontal().sum() # sum of total category volume sales -> retruns a scalar value\n",
    ").to_numpy().reshape(5)\n",
    "\n",
    "# Share of residual purchasing -- Alpha Buyers\n",
    "# The percentage of the category purchasing not accounted for by Alpha that goes to each of the other brands\n",
    "residual_purch_brand = share_of_cat_purch / (1 - share_of_cat_purch[0])\n",
    "\n",
    "# Share of residual purchasing -- Category Buyers\n",
    "# Residual share of category purchasing (across all category buyers) once Alpha is removed\n",
    "# when we exclude Alpha, what percentage of (the remaining) category purchasing goes to each of the other brands)\n",
    "residual_purch_category = market_share / (1 - market_share[0])\n",
    "\n",
    "# Index against expectation\n",
    "index_against_expect = 100 * residual_purch_brand/residual_purch_category\n",
    "\n",
    "# Importance Against Expectations - Create DataFrame\n",
    "stack = np.vstack((share_of_cat_purch, \n",
    "                   market_share, \n",
    "                   residual_purch_brand, \n",
    "                   residual_purch_category, \n",
    "                   index_against_expect))\n",
    "\n",
    "cols = pl.DataFrame({'Col': ['Share of Category Purchasing', \n",
    "        'Volume Market Share', \n",
    "        'Share of Residual Purchasing - Alpha Buyers', \n",
    "        'Share of Residual Purchasing - Category Buyers',\n",
    "        'Index Against Expectation']})\n",
    "\n",
    "rows = cross_purch_df.columns[:-1]\n",
    "\n",
    "importance_against_expect = pl.from_numpy(stack, schema=rows).hstack(columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    GT(importance_against_expect, rowname_col='Col')\n",
    "    .tab_header(title='Importance Against Expectation',\n",
    "                subtitle='Compare Actual Purchasing against Expectation Given General Purchasing Patterns')    \n",
    "    .fmt_percent(rows=list(range(4)), decimals=0)\n",
    "    .fmt_integer(rows=4)\n",
    "    .data_color(\n",
    "        rows=4,\n",
    "        domain=[80, 170],\n",
    "        palette=['white', 'rebeccapurple'],\n",
    "    )\n",
    "    .tab_style(\n",
    "        style=[\n",
    "            style.text(color=\"white\"),\n",
    "            style.fill('white'), \n",
    "        ],\n",
    "        locations=[loc.body(0,[2, 3, 4])]\n",
    "    )    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_ae = pl.DataFrame({'Index Against Expectation - By Volume Purchase': index_against_expect[1:],\n",
    "                         'Brands': cross_purch_df.columns[1:-1]})\n",
    "\n",
    "# Create the Altair chart\n",
    "alt.Chart(index_ae).mark_bar().encode(\n",
    "    x=alt.X(\"Index Against Expectation - By Volume Purchase:Q\"),\n",
    "    y=alt.Y(\"Brands:N\"),\n",
    ").properties(\n",
    "    width=650,\n",
    "    height=250,\n",
    "    title={\"text\": 'Importance Against Expectation'},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider *Charlie*. With reference to Charlie's residual share of category purchasing not accounted for by Alpha, we see that it accounted for 26% of the category purchasing by Alpha buyers that did not go to Alpha. If the purchasing of Alpha buyers was consistent with overall market patterns (as reflected in the volume market shares), we would expect Charlie to account for 19% of their purchasing. We can therefore say that Charlie’s share of purchasing amongst the\n",
    "buyers of Alpha is above expectation (when expectation is based on overall patterns of buying behaviour).\n",
    "\n",
    "The **Index Against Expectation** essentially captures whether or not and to what extent volume purchases of Alpha's buyers went to its competitors above or below expectations / patterns observed in the overall market.\n",
    "\n",
    "We see that, relative to market share, Bravo is less of threat to Alpha than we would expect (index = 84). Relative to market share, Charlie and Delta are purchased more by buyers of Alpha than we would expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross Purchasing - *By Spend*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat these analyses using spend rather than volume purchasing. \n",
    "\n",
    "We see, for example, that buyers of Alpha spent 71% of their category spend on Alpha. This is in contrast to the 69% of their category volume requirements satisfied by Alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_spend_purchases = spend_pivot(grocery_lf, 1).fill_null(0)\n",
    "\n",
    "# Step 1: Create ever buyers matrix\n",
    "ever_buyers = brand_spend_purchases.select(\n",
    "    [pl.when(pl.col(col) > 0).then(1).otherwise(0).alias(col) for col in brand_purchases.columns]\n",
    ").to_numpy()\n",
    "\n",
    "# Step 2: Compute cross purchase matrix\n",
    "cross_purch_matrix = ever_buyers.T @ brand_spend_purchases\n",
    "total_spend_purchases = np.sum(cross_purch_matrix, axis=1).reshape(5, 1)\n",
    "\n",
    "# Step 3: Compute cross purchase matrix as a % of total spend of each brand\n",
    "cross_purch = cross_purch_matrix / total_spend_purchases\n",
    "\n",
    "# DataFrame conversion for tabular view\n",
    "cross_purch_matrix_df = (\n",
    "    pl.from_numpy(cross_purch_matrix, schema=brand_spend_purchases.columns)\n",
    "    .with_columns(pl.Series('Brands', brand_spend_purchases.columns))\n",
    ")\n",
    "\n",
    "cross_purch_df = (\n",
    "    pl.from_numpy(cross_purch, schema=brand_spend_purchases.columns)\n",
    "    .with_columns(pl.Series('Brands', brand_spend_purchases.columns))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    GT(cross_purch_matrix_df, rowname_col='Brands')\n",
    "    .tab_header(title='Cross Purchasing Analysis',\n",
    "                subtitle='Total Category Spend by Buyers of the Brand (by row)')\n",
    "    .tab_stubhead(label=\"Brands\")\n",
    "    .sub_missing(missing_text=\"\")\n",
    "    .fmt_integer()\n",
    "    .data_color(\n",
    "        domain=[0, 10_000],\n",
    "        palette=['white', 'rebeccapurple'],\n",
    "    ) \n",
    "    .tab_style(\n",
    "        style=[\n",
    "            style.text(color=\"white\"),\n",
    "            style.fill('lightgrey'),\n",
    "            \n",
    "        ],\n",
    "        locations=[\n",
    "            loc.body(0,0),\n",
    "            loc.body(1,1),\n",
    "            loc.body(2,2),\n",
    "            loc.body(3,3),\n",
    "            loc.body(4,4)\n",
    "        ]\n",
    "    )\n",
    "    .tab_source_note(md(\"\"\"**Read Row/Column as**: _Purchasers of [Brands] / Total Spend on [Brands]_\"\"\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    GT(cross_purch_df, rowname_col='Brands')\n",
    "    .tab_header(title='Cross Purchasing Analysis',\n",
    "                subtitle='% of Total Category Spend by Buyers of the Brand (by row)')\n",
    "    .tab_stubhead(label=\"Brands\")\n",
    "    .sub_missing(missing_text=\"\")\n",
    "    .fmt_percent(decimals=0)\n",
    "    .data_color(\n",
    "        domain=[0, 0.8],\n",
    "        palette=['white', 'rebeccapurple'],\n",
    "    ) \n",
    "    .tab_source_note(md(\"\"\"**Read Row/Column as**: _Purchasers of [Brands] / % Total Spend on [Brands]_\"\"\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importance Against Expectation - *By Spend*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a spend-based importance against expectation plot, using value market share as the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Share of Category Purchasing - Alpha Brand Selected\n",
    "share_of_cat_purch = cross_purch[0]\n",
    "\n",
    "# Market Share - Volume\n",
    "market_share = (\n",
    "    brand_spend_purchases.sum() / # sum all columns -> returns a single row dataframe\n",
    "    brand_spend_purchases.sum_horizontal().sum() # sum of total category volume sales -> retruns a scalar value\n",
    ").to_numpy().reshape(5)\n",
    "\n",
    "# Share of residual purchasing -- Alpha Buyers\n",
    "# The percentage of the category purchasing not accounted for by Alpha that goes to each of the other brands\n",
    "residual_purch_brand = share_of_cat_purch / (1 - share_of_cat_purch[0])\n",
    "\n",
    "# Share of residual purchasing -- Category Buyers\n",
    "# Residual share of category purchasing (across all category buyers) once Alpha is removed\n",
    "# when we exclude Alpha, what percentage of (the remaining) category purchasing goes to each of the other brands)\n",
    "residual_purch_category = market_share / (1 - market_share[0])\n",
    "\n",
    "# Index against expectation\n",
    "index_against_expect = 100 * residual_purch_brand/residual_purch_category\n",
    "\n",
    "# Importance Against Expectations - Create DataFrame\n",
    "stack = np.vstack((share_of_cat_purch, \n",
    "                   market_share, \n",
    "                   residual_purch_brand, \n",
    "                   residual_purch_category, \n",
    "                   index_against_expect))\n",
    "\n",
    "cols = pl.DataFrame({'Col': ['Share of Category Purchasing', \n",
    "        'Volume Market Share', \n",
    "        'Share of Residual Purchasing - Alpha Buyers', \n",
    "        'Share of Residual Purchasing - Category Buyers',\n",
    "        'Index Against Expectation']})\n",
    "\n",
    "rows = cross_purch_df.columns[:-1]\n",
    "\n",
    "importance_against_expect = pl.from_numpy(stack, schema=rows).hstack(columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    GT(importance_against_expect, rowname_col='Col')\n",
    "    .tab_header(title='Importance Against Expectation',\n",
    "                subtitle='Compare Actual Purchasing against Expectation Given General Purchasing Patterns')    \n",
    "    .fmt_percent(rows=list(range(4)), decimals=0)\n",
    "    .fmt_integer(rows=4)\n",
    "    .data_color(\n",
    "        rows=4,\n",
    "        domain=[80, 170],\n",
    "        palette=['white', 'rebeccapurple'],\n",
    "    )\n",
    "    .tab_style(\n",
    "        style=[\n",
    "            style.text(color=\"white\"),\n",
    "            style.fill('white'), \n",
    "        ],\n",
    "        locations=[loc.body(0,[2, 3, 4])]\n",
    "    )    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_ae = pl.DataFrame({'Index Against Expectation - By Spend': index_against_expect[1:],\n",
    "                         'Brands': cross_purch_df.columns[1:-1]})\n",
    "\n",
    "# Create the Altair chart\n",
    "alt.Chart(index_ae).mark_bar().encode(\n",
    "    x=alt.X(\"Index Against Expectation - By Spend:Q\"),\n",
    "    y=alt.Y(\"Brands:N\"),\n",
    ").properties(\n",
    "    width=650,\n",
    "    height=250,\n",
    "    title={\"text\": 'Importance Against Expectation'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Garbage Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions = ['grocery_lf', 'sku_lf', 'kiwi_lf', 'In', 'exceptions', 'active_variables',\n",
    "              'penetration', 'ppb', 'panel_size']\n",
    "\n",
    "active_variables = [\n",
    "    var for var, value in globals().items()\n",
    "    if not var.startswith('_')   # Exclude variables that start with \"_\"\n",
    "    and var not in exceptions    # Exclude variables in the exceptions list\n",
    "    and isinstance(value, (pl.LazyFrame, pl.DataFrame, pl.Series, alt.Chart, alt.LayerChart, list, int, float, str, np.ndarray, np.int64, np.float32))  # Remove these types only\n",
    "]\n",
    "\n",
    "for var in active_variables:\n",
    "    del globals()[var]\n",
    "del active_variables, exceptions, var\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#956bbf\">Exploring Dynamics in Buyer Behaviour</span>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, we have been characterizing buyer behavior in a given time period (i.e., one year), be it focusing on one brand or multiple brands. We now consider some standard analyses that give insight into the dynamics of buyer behavior from period to period. We first consider the case of established products and then turn our attention to the analysis of new product buying behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Established Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in summarizing how buyer behavior varies across consecutive periods. We first consider how **temporal variations in total sales** can be understood by **decomposing total sales**. Next we explore **temporal variations in customer-level purchasing** by examining how the distribution of purchasing in one period varies as a function of the level of purchasing in the previous period. Finally, we consider a summary measure of period-to-period purchasing called the **repeat rate**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Understanding Temporal Variations in Sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most firms have systems that will report sales over time. As we try to make sense of any observed changes, it is helpful to note a **fundamental (multiplicative) sales decomposition**. For any time period,\n",
    "\n",
    "$$\n",
    "  \\begin{aligned}\n",
    "    \\text{Sales}   &= \\text{\\# households (HHs) in the country} \\\\\n",
    "                   &= \\times \\text{proportion of HHs buying the brand (penetration)} \\\\\n",
    "                   &= \\times \\text{\\# purchase occasions per buyer (PPB)} \\\\\n",
    "                   &= \\times \\text{\\# packs per purchase} \\\\\n",
    "                   &= \\times \\text{weight or price per pack} \\\\\n",
    "  \\end{aligned}\n",
    "$$\n",
    "\n",
    "There is nothing magical about this specific decomposition. We can create variations on a theme that are more relevant for the specific analysis setting at hand. For example, suppose we are doing an analysis at the brand level, where the SKUs associated with the brand come in different sizes. Furthermore, suppose the time period is sufficiently small that households make only one purchase per period, if at all. (In other words, PPB = 1). A more relevant decomposition would be\n",
    "\n",
    "$$\n",
    "  \\begin{aligned}\n",
    "    \\text{Sales (\\$)}   &= \\text{\\# households (HHs) in the country} \\\\\n",
    "                   &= \\times \\text{proportion of HHs buying the brand (penetration)} \\\\\n",
    "                   &= \\times \\text{average volume per purchase} \\\\\n",
    "                   &= \\times \\text{average price per unit of volume} \\\\\n",
    "  \\end{aligned}\n",
    "$$\n",
    "\n",
    "The product of the last two quantities is often called **average order value (AOV)**.\n",
    "\n",
    "$$AOV = \\text{average volume per purchase} \\times \\text{average price per unit of volume}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the plot of Alpha's revenue we created earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_plot(dataframe=weekly_spend_summary('Alpha', grocery_lf).collect(), \n",
    "            y='Weekly Spend', \n",
    "            title='Plot of weekly revenue for Alpha', \n",
    "            y_axis_label='Spend ($)',\n",
    "            pct=False,\n",
    "            legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe some weeks where this is a massive increase in revenue. How much of this is due to an increase in penetration versus, say, buyers simply buying more product on a given purchase occasion? Let us explore this using the second decomposition given above.\n",
    "\n",
    "1) We start by computing Alpha’s **weekly penetration numbers**:\n",
    "   - Check whether purchase purchase occasions per buyer (PPB) = 1.\n",
    "   - We do this by grouping the panel data by '`week`' and aggregating to get the number of unique '`trans_id`' and '`panel_id`' per week. If the '`num_trans`' and '`num_buyers`' is the same for each week, then no panellist made more than one purchase in any given week, which means PPB = 1.\n",
    "   - We produce a new dataframe that is grouped by 104 weeks and aggregates the count of unique number of panellists (*weekly penetration*) and number of transaction IDs each week.\n",
    "   - Output: The number of panellists that made at least one purchase of Alpha; the total number of category purchase occasions on which Alpha was purchased, for each week.\n",
    "2) The next step is to add the **weekly volume and revenue** numbers: \n",
    "   - Join SKU weights data with panel data, group panel data by `week`, compute `volume` and `spend`.\n",
    "   - Output: Alpha’s (dollar and volume) sales.\n",
    "3) Next we compute the numbers associated with the **revenue decomposition**:\n",
    "   - **Weekly penetration** is simply the number of panellists active in any given week divided by the size of the panel.\n",
    "   - **Average order value** is simply total revenue for any given week divided by the number of panellists that made at least one purchase of Alpha in that week.\n",
    "     - As noted above, the AOV quantity can be decomposed into *average order volume* and *average price per unit volume* (in this case, kg).\n",
    "   - **Average order volume** is simply (total) volume sold in any given week divided by the number of panellists that made at least one purchase of Alpha in that week.\n",
    "   - **Average price per unit volume** is simply total revenue for any given week divided by (total) volume sold in that week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    grocery_lf\n",
    "    .filter(pl.col('brand') == 'Alpha')\n",
    "    .group_by(\"week\")\n",
    "    .agg(\n",
    "        pl.col(\"trans_id\").n_unique().alias(\"num_trans\"),\n",
    "        pl.col(\"panel_id\").n_unique().alias(\"num_buyers\")\n",
    "    ).with_columns(\n",
    "        (pl.col('num_trans') / pl.col('num_buyers')).alias('ppb')\n",
    "    )\n",
    "    .sort('week')\n",
    "    .filter(pl.col('ppb') == 1)\n",
    "    \n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the number of transactions (`num_trans`) associated with each week equals the number of panellists (`num_buyers`) associated with each week. In other words, no panellist made more than one purchase in any given week, which means PPB = 1 (`num_trans` / `num_buyers`), as assumed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for rows where the brand is \"Alpha\"\n",
    "alpha_lf = grocery_lf.filter(pl.col(\"brand\") == \"Alpha\")\n",
    "\n",
    "# Aggregating to get the number of unique trans_id and panel_id per week\n",
    "alpha_weekly_trans = (\n",
    "    alpha_lf\n",
    "    .group_by(\"week\")\n",
    "    .agg(\n",
    "        pl.col(\"trans_id\").n_unique().alias(\"num_trans\"),\n",
    "        pl.col(\"panel_id\").n_unique().alias(\"num_buyers\")\n",
    "    )    \n",
    ")\n",
    "\n",
    "# Aggregating to get the sum of spend and volume per week\n",
    "alpha_weekly_spend_vol = (\n",
    "    alpha_lf\n",
    "    .with_columns(((pl.col('units') * pl.col('price'))).alias('spend'))\n",
    "    .join(other=sku_lf, on='sku_id')\n",
    "    .with_columns((((pl.col('units') * pl.col('weight'))/1000)).alias('volume'))\n",
    "    .group_by('week').agg(\n",
    "        pl.col(\"spend\").sum().alias(\"spend\"),\n",
    "        pl.col(\"volume\").sum().alias(\"volume\")        \n",
    "    )\n",
    ")\n",
    "\n",
    "# The alpha_weekly_trans & alpha_weekly_spend_vol are joined.\n",
    "# alpha_weekly_summary now contains week, num_trans, num_buyers, spend, and volume\n",
    "alpha_weekly_summary = alpha_weekly_trans.join(alpha_weekly_spend_vol, on=\"week\").sort('week')\n",
    "\n",
    "alpha_weekly = (\n",
    "    alpha_weekly_summary\n",
    "    .with_columns(\n",
    "        # Weekly penetration = num_buyer / panel_sizes\n",
    "        (pl.col('num_buyers') / panel_size).alias('penet'),\n",
    "        # PPB = num_trans / num_buyer\n",
    "        (pl.col('num_trans') / pl.col('num_buyers')).alias('ppb'),\n",
    "        # Average Order Value = spend / num_trans\n",
    "        (pl.col('spend') / pl.col('num_trans')).alias('aov'),\n",
    "        # Average Order Value = spend / num_trans\n",
    "        (pl.col('volume') / pl.col('num_trans')).alias('aovol'),\n",
    "        # Average price per unit volume = spend / volume\n",
    "        (pl.col('spend') / pl.col('volume')).alias('avg_price_kg')\n",
    "    ).select(\n",
    "        'week', 'spend', 'penet', 'aov', 'aovol', 'avg_price_kg'\n",
    "        \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the correlations between weekly revenue and the components of its (multiplicative) decomposition across the two years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlations between Weekly Revenue & its Components - Over 2 Years**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = alpha_weekly.select(pl.all().exclude('week')).collect().corr()\n",
    "\n",
    "relabel = pl.Series(['Revenue', 'Penetration', 'AOV', 'AOVOL', 'Price/Unit'])\n",
    "\n",
    "corr_matrix = corr_matrix.with_columns(pl.Series(relabel).alias(\"Decomposition\"))\n",
    "\n",
    "(\n",
    "    GT(corr_matrix, rowname_col='Decomposition')\n",
    "    .tab_header(title='Correlation Matrix of Revenue and the Components of its Decomposition',\n",
    "                subtitle='Correlations between weekly revenue and the components of its (multiplicative) decomposition')\n",
    "    .cols_label(\n",
    "        spend=relabel[0],\n",
    "        penet=relabel[1],\n",
    "        aov=relabel[2],\n",
    "        aovol=relabel[3],\n",
    "        avg_price_kg=relabel[4]\n",
    "    )\n",
    "    .cols_align(align='center')\n",
    "    .fmt_number(decimals=3)\n",
    "    .data_color(\n",
    "        domain=[-1, 1],\n",
    "        palette=['orange', 'white', 'rebeccapurple'],\n",
    "    ) \n",
    "    .tab_style(\n",
    "        style=[\n",
    "            style.text(color=\"white\"),\n",
    "            style.fill('lightgrey'),\n",
    "            \n",
    "        ],\n",
    "        locations=[\n",
    "            loc.body(0,0),\n",
    "            loc.body(1,1),\n",
    "            loc.body(2,2),\n",
    "            loc.body(3,3),\n",
    "            loc.body(4,4)\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a plot of the penetration numbers. Comparing this with Alpha's weekly revenue plot over the two years, we see that the fluctuations in revenue go hand-in-hand with fluctuations in penetration. To make this comparison clearer, we overlay the two series of numbers as well. The correlation between these to quantities is 0.98. It would appear that the key driver of revenue increases is simply more people buying the brand that week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_plot(dataframe=alpha_weekly.select('week', 'penet').collect(), \n",
    "            y='penet', \n",
    "            title='Alpha - Weekly Penetration', \n",
    "            y_axis_label='Penetration',\n",
    "            pct=True,\n",
    "            legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Fold Transformation- convert wide-form data to long-form data directly without any preprocessing\n",
    "\n",
    "source = alpha_weekly.select('week', 'spend', 'penet').collect()\n",
    "\n",
    "base = alt.Chart(source).mark_line().transform_fold(\n",
    "    ['Penetration', 'Revenue'],\n",
    "    as_=['Legend', 'Value']\n",
    ").encode(\n",
    "    alt.Color('Legend:N'),\n",
    "    alt.X('week', axis=alt.Axis(\n",
    "        values=np.arange(0, 104 + 1, 13), # Explicitly specify quarter-end weeks\n",
    "        labelExpr=\"datum.value\", # Show only these labels\n",
    "        title='Week'))\n",
    ")\n",
    "\n",
    "spend_line = base.transform_filter(\n",
    "    alt.datum.Legend == 'Revenue'\n",
    "    ).encode(\n",
    "        y = alt.Y(\n",
    "            'spend:Q', # Q = a continuous real-valued quantity\n",
    "            axis=alt.Axis(format=\"$,.0f\")).title('Revenue ($)')\n",
    ")\n",
    "\n",
    "penet_line = base.transform_filter(\n",
    "    alt.datum.Legend == 'Penetration'\n",
    ").encode(\n",
    "    y = alt.Y(\n",
    "        'penet:Q', # Q = a continuous real-valued quantity\n",
    "        axis=alt.Axis(format=\",.0%\")).title('Penetration')\n",
    ")\n",
    "\n",
    "alt.layer(spend_line, penet_line).resolve_scale(\n",
    "    y='independent'\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=250,\n",
    "    title='Weekly Revenue & Penetration for Alpha'    \n",
    ").configure_view(stroke=None).configure_axisY(grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Pre-processed - Wide-form to long-form conversion\n",
    "\n",
    "source = alpha_weekly.select('week', 'spend', 'penet').unpivot(\n",
    "    index='week',\n",
    "    on=['spend', 'penet']\n",
    ").collect()\n",
    "\n",
    "base = alt.Chart(source).mark_line().encode(\n",
    "    alt.Color('variable:N'),\n",
    "    alt.X('week', axis=alt.Axis(\n",
    "        values=np.arange(0, 104 + 1, 13), # Explicitly specify quarter-end weeks\n",
    "        labelExpr=\"datum.value\", # Show only these labels\n",
    "        title='Week'))\n",
    ")\n",
    "\n",
    "spend_line = base.transform_filter(\n",
    "    alt.datum.variable == 'spend'\n",
    "    ).encode(\n",
    "        y = alt.Y(\n",
    "            'value:Q', # Q = a continuous real-valued quantity\n",
    "            axis=alt.Axis(format=\"$,.0f\")).title('Revenue ($)')\n",
    ")\n",
    "\n",
    "penet_line = base.transform_filter(\n",
    "    alt.datum.variable == 'penet'\n",
    ").encode(\n",
    "    y = alt.Y(\n",
    "        'value:Q', # Q = a continuous real-valued quantity\n",
    "        axis=alt.Axis(format=\",.0%\")).title('Penetration')\n",
    ")\n",
    "\n",
    "alt.layer(spend_line, penet_line).resolve_scale(\n",
    "    y='independent'\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=250,\n",
    "    title='Weekly Revenue & Penetration for Alpha'    \n",
    ").configure_view(stroke=None).configure_axisY(grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the lack of a perfect correlation means there is some variability in average order value that is not highly correlated with penetration. We plot this quantity below. (The correlation between penetration and average order value is 0.39.) In order to get a sense of what lies behind the variability in average order value, we also plot weekly average order volume and average price per kg, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekly_plot(dataframe, y, year=2, title=\"\", y_axis_label=\"\", fmt='currency'):\n",
    "    \n",
    "    if fmt == 'currency':\n",
    "        format = \"$,.1f\"\n",
    "    elif fmt == 'percent':\n",
    "        format = \",.0%\"\n",
    "    else:\n",
    "        format = \",.1f\"\n",
    "    \n",
    "    chart = alt.Chart(dataframe).mark_line(strokeWidth=1).encode(\n",
    "        x = alt.X(\n",
    "            'week',\n",
    "            axis=alt.Axis(\n",
    "                values=np.arange(0, (year*52) + 1, 13), # Explicitly specify quarter-end weeks\n",
    "                labelExpr=\"datum.value\", # Show only these labels\n",
    "                title='Week'\n",
    "            )\n",
    "        ),\n",
    "        y = alt.Y(\n",
    "            f'{y}:Q', # Q = a continuous real-valued quantity\n",
    "            title=y_axis_label,\n",
    "            axis=alt.Axis(format=format)\n",
    "        )\n",
    "    ).properties(\n",
    "        width=650,\n",
    "        height=250,\n",
    "        title=title\n",
    "    ).configure_view(\n",
    "        stroke=None\n",
    "    )\n",
    "\n",
    "    return chart "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_plot(dataframe=alpha_weekly.select('week', 'aov').collect(), \n",
    "            y='aov', \n",
    "            title='Alpha - Weekly Average Order Value', \n",
    "            y_axis_label='Average Order Value ($)',\n",
    "            fmt='currency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_plot(dataframe=alpha_weekly.select('week', 'aovol').collect(), \n",
    "            y='aovol', \n",
    "            title='Alpha - Weekly Average Order Volume (kg)', \n",
    "            y_axis_label='Average Order Volume (kg)',\n",
    "            fmt='number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_plot(dataframe=alpha_weekly.select('week', 'avg_price_kg').collect(), \n",
    "            y='avg_price_kg', \n",
    "            title='Alpha - Weekly Average Price/kg', \n",
    "            y_axis_label='Average $/kg ',\n",
    "            fmt='currency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the weekly average price per kg plot, there was much less variability in price/kg in weeks 1–52 compared to weeks 53–104. It would appear that there was some change in promotion policy between years 1 and 2. We do not have the data to explore this further, however. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlations between Weekly Revenue & its Components - Year 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = alpha_weekly.filter(pl.col('week') <= 52).select(pl.all().exclude('week')).collect().corr()\n",
    "\n",
    "relabel = pl.Series(['Revenue', 'Penetration', 'AOV', 'AOVOL', 'Price/Unit'])\n",
    "\n",
    "corr_matrix = corr_matrix.with_columns(pl.Series(relabel).alias(\"Decomposition\"))\n",
    "\n",
    "(\n",
    "    GT(corr_matrix, rowname_col='Decomposition')\n",
    "    .tab_header(title='Correlation Matrix of Revenue and the Components of its Decomposition',\n",
    "                subtitle='Correlations between weekly revenue and the components of its (multiplicative) decomposition')\n",
    "    .cols_label(\n",
    "        spend=relabel[0],\n",
    "        penet=relabel[1],\n",
    "        aov=relabel[2],\n",
    "        aovol=relabel[3],\n",
    "        avg_price_kg=relabel[4]\n",
    "    )\n",
    "    .cols_align(align='center')\n",
    "    .fmt_number(decimals=3)\n",
    "    .data_color(\n",
    "        domain=[-1, 1],\n",
    "        palette=['orange', 'white', 'rebeccapurple'],\n",
    "    ) \n",
    "    .tab_style(\n",
    "        style=[\n",
    "            style.text(color=\"white\"),\n",
    "            style.fill('lightgrey'),\n",
    "            \n",
    "        ],\n",
    "        locations=[\n",
    "            loc.body(0,0),\n",
    "            loc.body(1,1),\n",
    "            loc.body(2,2),\n",
    "            loc.body(3,3),\n",
    "            loc.body(4,4)\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlations between Weekly Revenue & its Components - Year 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = alpha_weekly.filter(pl.col('week') >= 53).select(pl.all().exclude('week')).collect().corr()\n",
    "\n",
    "relabel = pl.Series(['Revenue', 'Penetration', 'AOV', 'AOVOL', 'Price/Unit'])\n",
    "\n",
    "corr_matrix = corr_matrix.with_columns(pl.Series(relabel).alias(\"Decomposition\"))\n",
    "\n",
    "(\n",
    "    GT(corr_matrix, rowname_col='Decomposition')\n",
    "    .tab_header(title='Correlation Matrix of Revenue and the Components of its Decomposition',\n",
    "                subtitle='Correlations between weekly revenue and the components of its (multiplicative) decomposition')\n",
    "    .cols_label(\n",
    "        spend=relabel[0],\n",
    "        penet=relabel[1],\n",
    "        aov=relabel[2],\n",
    "        aovol=relabel[3],\n",
    "        avg_price_kg=relabel[4]\n",
    "    )\n",
    "    .cols_align(align='center')\n",
    "    .fmt_number(decimals=3)\n",
    "    .data_color(\n",
    "        domain=[-1, 1],\n",
    "        palette=['orange', 'white', 'rebeccapurple'],\n",
    "    ) \n",
    "    .tab_style(\n",
    "        style=[\n",
    "            style.text(color=\"white\"),\n",
    "            style.fill('lightgrey'),\n",
    "            \n",
    "        ],\n",
    "        locations=[\n",
    "            loc.body(0,0),\n",
    "            loc.body(1,1),\n",
    "            loc.body(2,2),\n",
    "            loc.body(3,3),\n",
    "            loc.body(4,4)\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Annual Revenue Decomposition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This same logic can be used to analyse change across longer time periods. For example, we note from our analysis earlier that Alpha’s revenue increased from \\$33,571 in year 1 to \\$35,251 in year 2, a 5% increase. As we are looking at a **static sample** (i.e., the same group of panellists over the two years), the **increase cannot be due to population growth**. Are we observing this growth because more households are buying the product (increase in buyers), and/or because those that are buying it are buying it more often (increase in frequency), and/or because the average spend per transaction has increased (increase in AOV)?\n",
    "\n",
    "In order to explore this, we will use the following (multiplicative) sales decomposition:\n",
    "$$\n",
    "  \\begin{aligned}\n",
    "    \\text{Annual Revenue}   &= \\text{\\# households (HHs) in the panel} \\\\\n",
    "                   &= \\times \\text{proportion of HHs buying the brand (penetration)} \\\\\n",
    "                   &= \\times \\text{\\# purchase occasions per buyer (PPB)} \\\\\n",
    "                   &= \\times \\text{average order volume} \\\\\n",
    "                   &= \\times \\text{average price per unit of volume} \\\\\n",
    "  \\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform a similar decomposition of annual revenue, we first need to create a dataset that summarises, for each year, (1) the number of panellists that made at least one purchase of Alpha, (2) the total number of category purchase occasions on which Alpha was purchased, and (3) Alpha’s (dollar and volume) sales. We use the same logic as above, aggregating by year as opposed to week.\n",
    "\n",
    "1) compute the number of panellists that made at least one transaction\n",
    "2) compute total number of category purchase occasions (number of transactions) on which Alpha was purchased \n",
    "3) compute total revenue and volume for Alpha by year\n",
    "4) compute the components of the revenue decomposition\n",
    "   - **Penetration** is the number of households that made at least purchase of Alpha in the year divided by the size of the panel.\n",
    "   - **PPB** is the total number of transactions associated with Alpha divided by the number of households that made at least purchase of Alpha in the year.\n",
    "   - **Average Order Value** is annual revenue divided by the total number of transactions associated with Alpha.\n",
    "   - **Average Order Volume** is annual total volume divided by the total number of transactions associated with Alpha.\n",
    "   - **Average price/kg** is revenue divided by total volume. \n",
    "5) Computing the percentage changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_yearly_summary = (\n",
    "    grocery_lf\n",
    "    .filter(pl.col('brand') == 'Alpha')\n",
    "    .join(other=sku_lf, on='sku_id')\n",
    "    .with_columns(\n",
    "        (pl.col('units') * pl.col('price')).cast(pl.Float64).alias('spend'),\n",
    "        (pl.col('units') * pl.col('weight')/1000).alias('volume'),\n",
    "        (pl.col('week') / 52).ceil().cast(pl.UInt16).alias('year')\n",
    "    ).group_by('year')\n",
    "    .agg(\n",
    "        # Total number of category purchase occasions each year\n",
    "        pl.col('trans_id').n_unique().alias('num_trans'),\n",
    "        # The number of panellists that made at least one transaction each year\n",
    "        pl.col('panel_id').n_unique().alias('num_buyers'),\n",
    "        # Total revenue generated each year\n",
    "        pl.col('spend').sum(),\n",
    "        # Total volume purchased each year\n",
    "        pl.col('volume').sum()\n",
    "    )\n",
    ")\n",
    "\n",
    "alpha_yearly = (\n",
    "    alpha_yearly_summary\n",
    "    .with_columns(\n",
    "        # Yearly penetration = num_buyer / panel_sizes\n",
    "        (pl.col('num_buyers') / panel_size).alias('penet'),\n",
    "        # PPB = num_trans / num_buyer\n",
    "        (pl.col('num_trans') / pl.col('num_buyers')).alias('ppb'),\n",
    "        # Average Order Value = spend / num_trans\n",
    "        (pl.col('spend') / pl.col('num_trans')).alias('aov'),\n",
    "        # Average Order Value = spend / num_trans\n",
    "        (pl.col('volume') / pl.col('num_trans')).alias('aovol'),\n",
    "        # Average price per unit volume = spend / volume\n",
    "        (pl.col('spend') / pl.col('volume')).alias('avg_price_kg')\n",
    "    ).with_columns(pl.col('year').cast(pl.String))\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "# Transpose to cross-tabulate summary for presentation\n",
    "alpha_yearly_T = alpha_yearly.transpose(\n",
    "    include_header=True, header_name='Summary', column_names='year'\n",
    ")\n",
    "\n",
    "# Compute percentage change for each variable from year 1 to year 2\n",
    "alpha_yearly_pct_change = (\n",
    "    alpha_yearly\n",
    "    .with_columns(pl.col('*').exclude('year').pct_change())\n",
    "    .transpose(include_header=True, header_name='Summary', column_names='year')  \n",
    "    .select('Summary' , pl.col('2').alias('pct_change'))  \n",
    ")\n",
    "# Join summary variables and percentage change coss-tabular data for presentation\n",
    "alpha_yearly_T = alpha_yearly_T.join(alpha_yearly_pct_change, on='Summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relabel = pl.Series(['# of Trans.', \n",
    "                     '# of Buyers',\n",
    "                     'Revenue',\n",
    "                     'Volume',\n",
    "                     'Penetration', \n",
    "                     'PPB',\n",
    "                     'AOV', \n",
    "                     'AOVOL', \n",
    "                     'Avg. Price/Unit'])\n",
    "\n",
    "group = pl.Series(['Sales Summary']*4 + ['Revenue Decomposition']*5)\n",
    "\n",
    "alpha_yearly_T = alpha_yearly_T.with_columns(\n",
    "    pl.Series(relabel).alias('Summary'), \n",
    "    pl.Series(group).alias('Group')\n",
    ")\n",
    "\n",
    "(\n",
    "    GT(alpha_yearly_T, rowname_col='Summary', groupname_col='Group')\n",
    "    .tab_header(title='Alpha - Annual Summary',\n",
    "                subtitle='Transactions, Buyers, Volume, Revenue & Revenue Decomposition')\n",
    "    .cols_label({'1': 'Year 1', \n",
    "                 '2': 'Year 2',\n",
    "                 'pct_change': '% Change'})\n",
    "    .fmt_integer(rows=[0,1])\n",
    "    .fmt_number(rows=[2,3,5,7])\n",
    "    .fmt_currency(rows=[6,8])\n",
    "    .fmt_percent(rows=4)\n",
    "    .fmt_percent(columns=[3])\n",
    "    .data_color(\n",
    "        columns=3,\n",
    "        domain=[-1,1],\n",
    "        palette=['orange', 'white', 'rebeccapurple']\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the *5% increase in revenue* from year 1 to year 2 is associated with a *5% increase in the number of households making at least one purchase* of Alpha in the year. While PPB drops, average order value increases, with these two changes effectively cancelling out each other. (The product of these two quantities changes by one cent between the two years. i.e, $(PPB_{2} \\times AOV_{2})-(PPB_{1} \\times AOV_{1})$.) While the average price/kg drops from year 1 to year 2, this is more than compensated by the increase in average order volume, resulting in a 3% increase in average order value between the two years (AOR = Average Order Volume x Avearge Price/Unit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Temporal Variation in Customer-level Purchasing - by Brand Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that the number of buyers has increased, yet the average number of transactions (PPB) has dropped. Does this mean the “new” buyers are light buyers? Or are the existing buyers buying less?\n",
    "\n",
    "In order to dig deeper, we need to examine **temporal variation in customer-level purchasing**. A natural starting point is to examine the **joint distribution of purchasing** for two consecutive periods.\n",
    "\n",
    "1) We first need to process the panel data so it summarizes *the number of times Alpha was purchased in years 1 and 2 by each panellist* (`num_trans`).\n",
    "2) Then we prepare the **joint distribution** of purchasing table that summarizes customer-level purchasing for all combinations of transaction levels in year 1 and year 2. The values essentially is a count of panellists that made $x_{1}$ transactions in year 1 and $x_{2}$ transactions in year 2.\n",
    "3) Next, we compute the **conditional distribution** using the joint distribution matrix computed earlier. We divide each element of the matrix by their respective row total (row percentages). The conditional distribution is essentially the empirical probability of making $𝑥_{2}$ transactions in year 2 given the panellist made $𝑥_{1}$ transactions in year 1.\n",
    "   - Given that we have computed the joint distribution of panellist count for all transaction-levels for years 1 and 2 (or simply: joint distribution of transaction counts), it is a simple exercise to compute the row percentages, giving us the conditional distributions of transaction counts.\n",
    "4) Finally, we compute and plot the **marginal distribution** of all transaction levels each year. Marginal distribution is the proportion of panellists who made $x_{t}$ transactions in year 1 and year 2 relative to the count of total panellists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizes the number of times Alpha was purchased in years 1 and 2 by each panellist\n",
    "alpha_yearly_trans = (\n",
    "    grocery_lf\n",
    "    .filter(pl.col('brand') == 'Alpha')\n",
    "    .with_columns((pl.col('week') / 52).ceil().cast(pl.UInt16).alias('year'))\n",
    "    # Aggregate to count unique 'trans_id' by 'panel_id' and 'year'\n",
    "    .group_by('panel_id','year')\n",
    "    .agg(pl.col('trans_id').n_unique().alias('num_trans'))\n",
    ")\n",
    "\n",
    "alpha_yearly_trans = (\n",
    "    # Reshape from long format to wide format\n",
    "    # Pivoting the dataframe based on 'year' to create a wide format.\n",
    "    alpha_yearly_trans.collect().pivot(\n",
    "        on='year',          # Each unique year will create a new column\n",
    "        values='num_trans', # The column to aggregate\n",
    "        index='panel_id'    # Rows will be indexed by 'panel_id'\n",
    "    )\n",
    "    # Rename the columns to 'Year 1' and 'Year 2'\n",
    "    .rename({'1': 'Year 1', '2': 'Year 2'})   \n",
    "    # Replace any null values with 0\n",
    "    .fill_null(0) \n",
    ")\n",
    "\n",
    "# Create a basic joint distribution\n",
    "joint_dist_trans = (\n",
    "    alpha_yearly_trans\n",
    "    .group_by('Year 1', 'Year 2')\n",
    "    .agg(pl.col('panel_id').count().alias('count'))\n",
    ")\n",
    "\n",
    "# Add in the number of panellists that made no purchase of Alpha in either year\n",
    "# The processed dataframe so far is a subset of the main panel data containing the purchasing of those that bought Alpha at least once in the two years, there are 3142 such households.\n",
    "# The panel contains 5021 panellists. Therefore the correct number of households that made zero purchases of Alpha in years 1 and 2 should be included in the processed dataframe.\n",
    "tmp = panel_size - joint_dist_trans.select((pl.col('count')).sum()).item(0,0)\n",
    "zero_purch = pl.DataFrame({'Year 1': 0, 'Year 2': 0, 'count': tmp})\n",
    "\n",
    "# Right censor the distribution at 10+\n",
    "joint_dist_trans = (\n",
    "    joint_dist_trans.with_columns(\n",
    "        pl.when(pl.col('Year 1') > 9)\n",
    "        .then(10)\n",
    "        .otherwise(pl.col('Year 1'))\n",
    "        .alias('Year 1')\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.when(pl.col('Year 2') > 9)\n",
    "        .then(10)    \n",
    "        .otherwise(pl.col('Year 2'))\n",
    "        .alias('Year 2')   \n",
    "    )\n",
    "    .vstack(zero_purch.cast(pl.UInt32))\n",
    ")\n",
    "\n",
    "# Unpivot the right censor data to create a year 1 x year 2 distribution matrix of the count of panellists who made x_{1} in year 1 & x_{2} in year 2\n",
    "joint_dist_trans_pivot = (\n",
    "    joint_dist_trans\n",
    "    .sort('Year 2')\n",
    "    .pivot(\n",
    "        index='Year 1',\n",
    "        on='Year 2',\n",
    "        values='count',\n",
    "        aggregate_function='sum'  \n",
    "    )\n",
    "    .rename({'Year 1': 'Year'}) \n",
    "    .sort('Year')   \n",
    "    .fill_null(0)    \n",
    ")\n",
    "\n",
    "# Compute the row percentages, giving us the conditional distributions of transaction counts (the % of panellists from year 1 and their associated transaction counts in year 2)\n",
    "# The conditional probability here is he empirical probability of making 𝑥_{2} transactions in year 2 given the panellist made 𝑥_{1} transactions in year 1\n",
    "year1_trans_total = joint_dist_trans_pivot.select(\n",
    "    pl.col('*').exclude('Year')\n",
    ").sum_horizontal()\n",
    "\n",
    "pct_year1_total = joint_dist_trans_pivot.with_columns(\n",
    "    (pl.col('*').exclude('Year') / year1_trans_total)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizes Alpha transactions by year and panelist\n",
    "alpha_yearly_trans = (\n",
    "    grocery_lf\n",
    "    .filter(pl.col('brand') == 'Alpha')\n",
    "    .with_columns((pl.col('week') / 52).ceil().cast(pl.UInt16).alias('year'))\n",
    "    .group_by(['panel_id', 'year'])\n",
    "    .agg(pl.col('trans_id').n_unique().alias('num_trans'))\n",
    "    .collect()\n",
    "    .pivot(\n",
    "        values='num_trans',\n",
    "        index='panel_id',\n",
    "        on='year'\n",
    "    )\n",
    "    .rename({'1': 'Year 1', '2': 'Year 2'})\n",
    "    .fill_null(0)\n",
    ")\n",
    "\n",
    "# Create joint distribution\n",
    "joint_dist_trans = (\n",
    "    alpha_yearly_trans\n",
    "    .group_by(['Year 1', 'Year 2'])\n",
    "    .agg(pl.len().alias('count'))\n",
    ")\n",
    "\n",
    "# Add zero-purchase panellists\n",
    "zero_purchasers = panel_size - joint_dist_trans['count'].sum()\n",
    "joint_dist_trans = joint_dist_trans.vstack(\n",
    "    pl.DataFrame({'Year 1': [0], 'Year 2': [0], 'count': [zero_purchasers]}).cast(pl.UInt32)\n",
    ")\n",
    "\n",
    "# Right-censor transactions at 10+\n",
    "joint_dist_trans = joint_dist_trans.with_columns([\n",
    "    pl.when(pl.col('Year 1') > 9).then(10).otherwise(pl.col('Year 1')).alias('Year 1'),\n",
    "    pl.when(pl.col('Year 2') > 9).then(10).otherwise(pl.col('Year 2')).alias('Year 2')\n",
    "])\n",
    "\n",
    "# Create the year 1 x year 2 distribution matrix\n",
    "joint_dist_trans_pivot = (\n",
    "    joint_dist_trans\n",
    "    .sort('Year 2')\n",
    "    .pivot(\n",
    "        values='count',\n",
    "        index='Year 1',\n",
    "        on='Year 2',\n",
    "        aggregate_function='sum'\n",
    "    )\n",
    "    .rename({'Year 1': 'Year'})\n",
    "    .sort('Year')\n",
    "    .fill_null(0)\n",
    ")\n",
    "\n",
    "# Compute conditional distribution (row percentages)\n",
    "row_totals = joint_dist_trans_pivot.select(pl.col('*').exclude('Year')).sum_horizontal()\n",
    "pct_joint_dist = joint_dist_trans_pivot.with_columns(\n",
    "    (pl.col('*').exclude('Year') / row_totals)\n",
    ")\n",
    "\n",
    "# Display final results\n",
    "print(\"Alpha Yearly Transactions:\")\n",
    "display(alpha_yearly_trans)\n",
    "print(\"Joint Distribution:\")\n",
    "display(joint_dist_trans)\n",
    "print(\"Joint Distribution - Year 1 x Year 2 Distribution Matrix:\", )\n",
    "display(joint_dist_trans_pivot)\n",
    "print(\"Conditional Distribution (% of Year 1 Transactions):\")\n",
    "display(pct_joint_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relabel = pl.Series([f'{i}' if i < 10 else '10+' for i in range(11)])\n",
    "joint_dist_trans_pivot = joint_dist_trans_pivot.select(pl.col('*').exclude('Year')).with_columns(relabel.alias('Year'))\n",
    "(\n",
    "    GT(joint_dist_trans_pivot, rowname_col='Year')\n",
    "    .tab_header(title='Joint Distribution of the Purchasing of Alpha in Year 1 & 2')\n",
    "    .tab_spanner(label='# of Transactions in Year 2', columns=joint_dist_trans_pivot.columns[:-1])\n",
    "    .tab_stubhead(label='Year')\n",
    "    .cols_label({'10': '10+'})\n",
    "    .cols_align(align='center')\n",
    "    .fmt_integer()\n",
    "    .data_color(\n",
    "        domain=[0, 1900],\n",
    "        palette=['white', 'rebeccapurple'],\n",
    "    )\n",
    "    .tab_source_note(md(\"\"\"**Read Row/Column as**: _# of Transactions in Year 1 / # of Transactions in Year 2_\"\"\")) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we read this table? *Year 1*, *Row 2* tell us how many people who bought Alpha once in year 1 bought Alpha 0, 1, 2, . . . times in year 2. For example, 259 households didn’t buy Alpha in year 2, 201 bought Alpha once in year 2, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relabel = pl.Series([f'{i}' if i < 10 else '10+' for i in range(11)])\n",
    "pct_year1_total = pct_year1_total.select(pl.col('*').exclude('Year')).with_columns(relabel.alias('Year'))\n",
    "(\n",
    "    GT(pct_year1_total, rowname_col='Year')\n",
    "    .tab_header(title='Conditional Distribution of Transaction Counts',\n",
    "                subtitle=md('Empirical probability of making year 2 transaction given year 1 transaction'))\n",
    "    .tab_spanner(label='# of Transactions in Year 2', columns=pct_year1_total.columns[:-1])\n",
    "    .tab_stubhead(label='Year')\n",
    "    .cols_label({'10': '10+'})\n",
    "    .cols_align(align='center')\n",
    "    .fmt_percent(decimals=0)\n",
    "    .data_color(\n",
    "        domain=[0, 1],\n",
    "        palette=['white', 'rebeccapurple'],\n",
    "    )\n",
    "    .tab_source_note(md(\"\"\"**Read Row/Column as**: _# of Transactions in Year 1 / # of Transactions in Year 2_\"\"\")) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret this table? Looking at *Year 1*, *Row 2*, we see that 35.3% of the panellists that made one purchase of Alpha in year 1 made no purchases of Alpha in year 2, 27.4% purchased Alpha once, 17.5% purchased Alpha twice, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute marginal transaction distribution for each year: count of # of panellists who made x_{t} transactions as a % of total panellists\n",
    "def marginal_dist():\n",
    "    years = ['Year 1', 'Year 2']\n",
    "    dfs = []\n",
    "    for year in years:\n",
    "        y_dist = (\n",
    "            joint_dist_trans\n",
    "            .group_by(year)\n",
    "            .agg((pl.col('count').sum() / panel_size).alias(f'% of Total - {year}'))\n",
    "            .with_columns(pl.lit(year).alias('Year'))\n",
    "            .rename({year: 'num_trans'})\n",
    "            .sort('num_trans')        \n",
    "        )\n",
    "        dfs.append(y_dist)\n",
    "    \n",
    "    marginal_dist = dfs[0].drop('Year').hstack(dfs[1].drop('Year', 'num_trans'))\n",
    "    \n",
    "    return marginal_dist\n",
    "\n",
    "marginal_dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# marginal distribution for each year and the associated clustered bar chart\n",
    "\n",
    "# Step 1a: Compute marginal transaction distribution for each year: count of # of panellists who made x_{t} transactions as a % of total panellists\n",
    "# Step 1b: Convert Wide-form data to long-form data\n",
    "def marginal_dist():\n",
    "    years = ['Year 1', 'Year 2']\n",
    "    dfs = []\n",
    "    for year in years:\n",
    "        y_dist = (\n",
    "            joint_dist_trans\n",
    "            .group_by(year)\n",
    "            .agg((pl.col('count').sum() / panel_size).alias('% of Total'))  # marginal distribution of purchasing of Alpha in the first & second year.\n",
    "            .with_columns(pl.lit(year).alias('Year'))\n",
    "            .rename({year: 'num_trans'})        \n",
    "        )\n",
    "        dfs.append(y_dist)\n",
    "    \n",
    "    marginal_dist = dfs[0].vstack(dfs[1])\n",
    "    \n",
    "    return marginal_dist\n",
    "\n",
    "# Step 2: Plot long-form data as grouped bar chart\n",
    "alt.Chart(marginal_dist()).mark_bar().encode(\n",
    "    x=alt.X(\"num_trans:O\", axis=alt.Axis(labelAngle=0, title='Number of Transactions')),\n",
    "    xOffset=\"Year:N\",\n",
    "    y=alt.Y(\"% of Total:Q\", axis=alt.Axis(format=\".0%\", title='% of Households')),\n",
    "    color='Year:N'\n",
    ").properties(\n",
    "    width=650,\n",
    "    height=250,\n",
    "    title={\"text\": 'Distribution of Alpha Transactions', \"subtitle\": 'Marginal Distribution'},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us made two immediate observations:\n",
    "- The **marginal distribution** of purchasing in the first year is reasonably similar to that for the second year. Some differences that stand out are the smaller percentage of households making zero purchases in year 2, which corresponds to the higher penetration, and the smaller percentage of households buying Alpha ten or more times in year 2.\n",
    "- When first seeing a table that shows the distribution of year 2 purchasing broken down by the level of year 1 purchasing, many expect there to be a strong diagonal in the table and are alarmed by the fact that this is rarely the case. It is important to realize that **buying behaviour is not deterministic**. From the perspective of the analyst, it can be viewed as-if *random*, bouncing around each *person’s underlying propensity to buy* the product. Someone who makes one purchase\n",
    "in year 1 is possibly a light buyer and so is the fact that 35.3% did not buy the product again in the second year that surprising? It does not mean that they are “lost”; most of them will buy Alpha again sometime the following year (year 3). Similarly, are those who did not buy the product in period 1 but did in period 2 new customers? Some probably are. But, for any established product category, most are probably people who have purchased the product in previous years and who, for whatever reason, did not buy it that year. The key to analysing such tables is to *compare them to a benchmark that assumes stable underlying purchasing patterns*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Temporal Variation in Customer-level Spend - by Category Spend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s explore how to create the joint distribution of category spend in years 1 and 2. The logic follows that of the binning of spend used to create the distribution of category spend in year 1 in the earlier section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizes Category Spend by year and panelist\n",
    "labels = [f\"{lb} - {lb + 5 if lb != 50 else '∞'}\" for lb in range(-5, 55, 5)]\n",
    "category_yearly_spend = (\n",
    "    grocery_lf\n",
    "    .with_columns(\n",
    "        (pl.col('week') / 52).ceil().cast(pl.UInt16).alias('year'),\n",
    "        (pl.col('units') * pl.col('price')).cast(pl.Float64).alias('spend')\n",
    "    )\n",
    "    .group_by(['panel_id', 'year'])\n",
    "    .agg(pl.col('spend').sum())\n",
    "    .with_columns(pl.col('spend').cut(breaks=np.arange(0, 55, 5), labels=labels))\n",
    "    .collect()\n",
    "    .pivot(\n",
    "        values='spend',\n",
    "        index='panel_id',\n",
    "        on='year'\n",
    "    )\n",
    "    .rename({'1': 'Year 1', '2': 'Year 2'})\n",
    "    .with_columns([\n",
    "        pl.when(pl.col('Year 1').is_null()).then(\"0 - 5\").otherwise(pl.col('Year 1')).alias('Year 1'),\n",
    "        pl.when(pl.col('Year 2').is_null()).then(\"0 - 5\").otherwise(pl.col('Year 2')).alias('Year 2')\n",
    "    ])\n",
    "    .fill_null(0)  \n",
    ")\n",
    "\n",
    "category_yearly_spend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizes Alpha transactions by year and panelist\n",
    "category_yearly_spend = (\n",
    "    grocery_lf\n",
    "    .with_columns(\n",
    "        (pl.col('week') / 52).ceil().cast(pl.UInt16).alias('year'),\n",
    "        (pl.col('units') * pl.col('price')).cast(pl.Float64).alias('spend')\n",
    "    )\n",
    "    .group_by(['panel_id', 'year'])\n",
    "    .agg(pl.col('spend').sum())\n",
    "    .collect()\n",
    "    .pivot(\n",
    "        values='num_trans',\n",
    "        index='panel_id',\n",
    "        on='year'\n",
    "    )\n",
    "    .rename({'1': 'Year 1', '2': 'Year 2'})\n",
    "    .fill_null(0)\n",
    ")\n",
    "\n",
    "# Create joint distribution\n",
    "joint_dist_trans = (\n",
    "    alpha_yearly_trans\n",
    "    .group_by(['Year 1', 'Year 2'])\n",
    "    .agg(pl.len().alias('count'))\n",
    ")\n",
    "\n",
    "# Add zero-purchase panellists\n",
    "zero_purchasers = panel_size - joint_dist_trans['count'].sum()\n",
    "joint_dist_trans = joint_dist_trans.vstack(\n",
    "    pl.DataFrame({'Year 1': [0], 'Year 2': [0], 'count': [zero_purchasers]}).cast(pl.UInt32)\n",
    ")\n",
    "\n",
    "# Right-censor transactions at 10+\n",
    "joint_dist_trans = joint_dist_trans.with_columns([\n",
    "    pl.when(pl.col('Year 1') > 9).then(10).otherwise(pl.col('Year 1')).alias('Year 1'),\n",
    "    pl.when(pl.col('Year 2') > 9).then(10).otherwise(pl.col('Year 2')).alias('Year 2')\n",
    "])\n",
    "\n",
    "# Create the year 1 x year 2 distribution matrix\n",
    "joint_dist_trans_pivot = (\n",
    "    joint_dist_trans\n",
    "    .sort('Year 2')\n",
    "    .pivot(\n",
    "        values='count',\n",
    "        index='Year 1',\n",
    "        on='Year 2',\n",
    "        aggregate_function='sum'\n",
    "    )\n",
    "    .rename({'Year 1': 'Year'})\n",
    "    .sort('Year')\n",
    "    .fill_null(0)\n",
    ")\n",
    "\n",
    "# Compute conditional distribution (row percentages)\n",
    "row_totals = joint_dist_trans_pivot.select(pl.col('*').exclude('Year')).sum_horizontal()\n",
    "pct_joint_dist = joint_dist_trans_pivot.with_columns(\n",
    "    (pl.col('*').exclude('Year') / row_totals)\n",
    ")\n",
    "\n",
    "# Display final results\n",
    "print(\"Alpha Yearly Transactions:\")\n",
    "display(alpha_yearly_trans)\n",
    "print(\"Joint Distribution:\")\n",
    "display(joint_dist_trans)\n",
    "print(\"Year 1 x Year 2 Distribution Matrix:\", )\n",
    "display(joint_dist_trans_pivot)\n",
    "print(\"Row Percentage Distribution:\")\n",
    "display(pct_joint_dist)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Gxxt_AWN9igL",
    "R7dq85pW9WSz"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "fin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
